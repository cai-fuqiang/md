-- Logs begin at Tue 2024-04-02 11:09:27 CST, end at Tue 2024-04-02 11:18:11 CST. --
Apr 02 11:09:27 node-9.domain.tld kernel: Linux version 4.18.0-147.5.1.es8_24.x86_64 (mockbuild@x86_64-kojid) (gcc version 8.4.1 20200928 (Red Hat 8.4.1-1) (GCC)) #1 SMP Tue May 10 10:29:13 CST 2022
Apr 02 11:09:27 node-9.domain.tld kernel: Command line: BOOT_IMAGE=(hd0,gpt3)/vmlinuz-4.18.0-147.5.1.es8_24.x86_64 root=/dev/mapper/os-root ro edd=off kvm.halt_poll_ns=400000 cgroup.memory=nokmem intel_iommu=on iommu=pt pci=realloc ixgbe.allow_unsupported_sfp=1 rootdelay=90 nomodeset intel_idle.max_cstate=0 processor.max_cstate=0 crashkernel=300M rd.lvm.lv=os/root biosdevname=0 net.ifnames=1 nopti
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x020: 'AVX-512 opmask'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x040: 'AVX-512 Hi256'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x080: 'AVX-512 ZMM_Hi256'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Supporting XSAVE feature 0x200: 'Protection Keys User registers'
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: xstate_offset[5]:  832, xstate_sizes[5]:   64
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: xstate_offset[6]:  896, xstate_sizes[6]:  512
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: xstate_offset[7]: 1408, xstate_sizes[7]: 1024
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: xstate_offset[9]: 2432, xstate_sizes[9]:    8
Apr 02 11:09:27 node-9.domain.tld kernel: x86/fpu: Enabled xstate features 0x2e7, context size is 2440 bytes, using 'compacted' format.
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-provided physical RAM map:
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x0000000000000000-0x000000000003dfff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x000000000003e000-0x000000000003efff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x000000000003f000-0x000000000009ffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000000a0000-0x00000000000fffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x0000000000100000-0x00000000a5c56fff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000a5c57000-0x00000000a7d56fff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000a7d57000-0x00000000a8656fff] ACPI data
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000a8657000-0x00000000a8b0efff] ACPI NVS
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000a8b0f000-0x00000000aaffefff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000aafff000-0x00000000af7fffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000af800000-0x00000000cfffffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000fd000000-0x00000000fe7fffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000fed20000-0x00000000fed44fff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x00000000ff000000-0x00000000ffffffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: BIOS-e820: [mem 0x0000000100000000-0x000001003fffffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: NX (Execute Disable) protection: active
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a855018-0x9a85d057] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a855018-0x9a85d057] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a365018-0x9a38c657] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a365018-0x9a38c657] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99c73018-0x99ce2c57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99c73018-0x99ce2c57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99c03018-0x99c72c57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99c03018-0x99c72c57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a814018-0x9a81c857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a814018-0x9a81c857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a7f0018-0x9a7f8857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a7f0018-0x9a7f8857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99bcc018-0x99c02a57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99bcc018-0x99c02a57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99b95018-0x99bcba57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99b95018-0x99bcba57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99b5d018-0x99b94c57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99b5d018-0x99b94c57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99b25018-0x99b5cc57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x99b25018-0x99b5cc57] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a6ff018-0x9a707857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a6ff018-0x9a707857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a455018-0x9a45d857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x9a455018-0x9a45d857] usable ==> usable
Apr 02 11:09:27 node-9.domain.tld kernel: extended physical RAM map:
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000000000000-0x000000000003dfff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000000003e000-0x000000000003efff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000000003f000-0x000000000009ffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000000a0000-0x00000000000fffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000000100000-0x0000000099b25017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099b25018-0x0000000099b5cc57] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099b5cc58-0x0000000099b5d017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099b5d018-0x0000000099b94c57] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099b94c58-0x0000000099b95017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099b95018-0x0000000099bcba57] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099bcba58-0x0000000099bcc017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099bcc018-0x0000000099c02a57] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099c02a58-0x0000000099c03017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099c03018-0x0000000099c72c57] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099c72c58-0x0000000099c73017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099c73018-0x0000000099ce2c57] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000099ce2c58-0x000000009a365017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a365018-0x000000009a38c657] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a38c658-0x000000009a455017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a455018-0x000000009a45d857] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a45d858-0x000000009a6ff017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a6ff018-0x000000009a707857] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a707858-0x000000009a7f0017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a7f0018-0x000000009a7f8857] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a7f8858-0x000000009a814017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a814018-0x000000009a81c857] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a81c858-0x000000009a855017] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a855018-0x000000009a85d057] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x000000009a85d058-0x00000000a5c56fff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000a5c57000-0x00000000a7d56fff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000a7d57000-0x00000000a8656fff] ACPI data
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000a8657000-0x00000000a8b0efff] ACPI NVS
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000a8b0f000-0x00000000aaffefff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000aafff000-0x00000000af7fffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000af800000-0x00000000cfffffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000fd000000-0x00000000fe7fffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000fed20000-0x00000000fed44fff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x00000000ff000000-0x00000000ffffffff] reserved
Apr 02 11:09:27 node-9.domain.tld kernel: reserve setup_data: [mem 0x0000000100000000-0x000001003fffffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: efi: EFI v2.80 by American Megatrends
Apr 02 11:09:27 node-9.domain.tld kernel: efi:  ACPI=0xa8656000  ACPI 2.0=0xa8656014  SMBIOS=0xaaa9b000  SMBIOS 3.0=0xaaa9a000  MEMATTR=0xa25f3018  ESRT=0xa1d43198 
Apr 02 11:09:27 node-9.domain.tld kernel: secureboot: Secure boot disabled
Apr 02 11:09:27 node-9.domain.tld kernel: SMBIOS 3.3.0 present.
Apr 02 11:09:27 node-9.domain.tld kernel: DMI: Lenovo ThinkSystem SR670 V2/7Z23CTOLWW, BIOS U8E124I-2.10 07/20/2023
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0x00000000-0x00000fff] usable ==> reserved
Apr 02 11:09:27 node-9.domain.tld kernel: e820: remove [mem 0x000a0000-0x000fffff] usable
Apr 02 11:09:27 node-9.domain.tld kernel: last_pfn = 0x10040000 max_arch_pfn = 0x10000000000
Apr 02 11:09:27 node-9.domain.tld kernel: MTRR default type: uncachable
Apr 02 11:09:27 node-9.domain.tld kernel: MTRR fixed ranges enabled:
Apr 02 11:09:27 node-9.domain.tld kernel:   00000-9FFFF write-back
Apr 02 11:09:27 node-9.domain.tld kernel:   A0000-FFFFF uncachable
Apr 02 11:09:27 node-9.domain.tld kernel: MTRR variable ranges enabled:
Apr 02 11:09:27 node-9.domain.tld kernel:   0 base 010000000000 mask 3F8000000000 write-back
Apr 02 11:09:27 node-9.domain.tld kernel:   1 base 000000000000 mask 3F0000000000 write-back
Apr 02 11:09:27 node-9.domain.tld kernel:   2 base 0000C0000000 mask 3FFFC0000000 uncachable
Apr 02 11:09:27 node-9.domain.tld kernel:   3 base 0000BF000000 mask 3FFFFF000000 uncachable
Apr 02 11:09:27 node-9.domain.tld kernel:   4 disabled
Apr 02 11:09:27 node-9.domain.tld kernel:   5 disabled
Apr 02 11:09:27 node-9.domain.tld kernel:   6 disabled
Apr 02 11:09:27 node-9.domain.tld kernel:   7 disabled
Apr 02 11:09:27 node-9.domain.tld kernel:   8 disabled
Apr 02 11:09:27 node-9.domain.tld kernel:   9 disabled
Apr 02 11:09:27 node-9.domain.tld kernel: x86/PAT: Configuration [0-7]: WB  WC  UC- UC  WB  WP  UC- WT  
Apr 02 11:09:27 node-9.domain.tld kernel: total RAM covered: 1571824M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 64K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 128K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 256K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 512K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 1M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 2M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64K         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 128K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 256K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 512K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 1M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 2M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128K         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 256K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 512K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 1M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 2M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256K         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 512K         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 1M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 2M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512K         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 1M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 2M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 2M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 4M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 4M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 8M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 8M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 16M         num_reg: 10          lose cover RAM: 1504G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 32M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 16M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1024G
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 32M         num_reg: 10          lose cover RAM: 1507344M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1048592M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1048592M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1048592M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1048592M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1048592M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 32M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1048592M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64M         chunk_size: 64M         num_reg: 10          lose cover RAM: 1441840M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1048624M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1048624M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1048624M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1048624M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 64M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1048624M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128M         chunk_size: 128M         num_reg: 10          lose cover RAM: 1310832M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1048688M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1048688M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1048688M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 128M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1048688M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256M         chunk_size: 256M         num_reg: 10          lose cover RAM: 1048816M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256M         chunk_size: 512M         num_reg: 10          lose cover RAM: 1048816M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1048816M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 256M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1048816M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512M         chunk_size: 512M         num_reg: 10          lose cover RAM: 524784M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512M         chunk_size: 1G         num_reg: 10          lose cover RAM: 1049072M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 512M         chunk_size: 2G         num_reg: 10          lose cover RAM: 1049072M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1G         chunk_size: 1G         num_reg: 10          lose cover RAM: 1008M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 1G         chunk_size: 2G         num_reg: 10          lose cover RAM: 1008M
Apr 02 11:09:27 node-9.domain.tld kernel:  gran_size: 2G         chunk_size: 2G         num_reg: 10          lose cover RAM: 1008M
Apr 02 11:09:27 node-9.domain.tld kernel: mtrr_cleanup: can not find optimal value
Apr 02 11:09:27 node-9.domain.tld kernel: please specify mtrr_gran_size/mtrr_chunk_size
Apr 02 11:09:27 node-9.domain.tld kernel: e820: update [mem 0xbf000000-0xffffffff] usable ==> reserved
Apr 02 11:09:27 node-9.domain.tld kernel: last_pfn = 0xaf800 max_arch_pfn = 0x10000000000
Apr 02 11:09:27 node-9.domain.tld kernel: esrt: Reserving ESRT space from 0x00000000a1d43198 to 0x00000000a1d431f8.
Apr 02 11:09:27 node-9.domain.tld kernel: Base memory trampoline at [(____ptrval____)] 96000 size 24576
Apr 02 11:09:27 node-9.domain.tld kernel: Using GB pages for direct mapping
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba02000, 0x59fba02fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba03000, 0x59fba03fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba04000, 0x59fba04fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba05000, 0x59fba05fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba06000, 0x59fba06fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba07000, 0x59fba07fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba08000, 0x59fba08fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba09000, 0x59fba09fff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba0a000, 0x59fba0afff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: BRK [0x59fba0b000, 0x59fba0bfff] PGTABLE
Apr 02 11:09:27 node-9.domain.tld kernel: RAMDISK: [mem 0x35b58000-0x3d4a1fff]
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Early table checksum verification disabled
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: RSDP 0x00000000A8656014 000024 (v02 LENOVO)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: XSDT 0x00000000A8655728 000114 (v01 LENOVO THINKSYS 00000100      01000013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: FACP 0x00000000A8654000 000114 (v06 LENOVO THINKSYS 00000100 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: DSDT 0x00000000A85F1000 0626FC (v02 LENOVO THINKSYS 00000100 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: FACS 0x00000000A8B0D000 000040
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: FIDT 0x00000000A85F0000 00009C (v01 LENOVO THINKSYS 00000100 AMI  00010013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: OEM9 0x00000000A85EF000 000030 (v01 LNV    LNVGYERR 00000003      00000000)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A85EE000 000748 (v02 INTEL  RAS_ACPI 00000001 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A85ED000 000745 (v02 INTEL  ADDRXLAT 00000001 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: ERST 0x00000000A85EC000 000230 (v01 LENOVO THINKSYS 00000001 INTL 00000001)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: BERT 0x00000000A85EB000 000030 (v01 LENOVO THINKSYS 00000001 INTL 00000001)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: MCFG 0x00000000A85EA000 00003C (v01 LENOVO THINKSYS 00000100 MSFT 00000097)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: BDAT 0x00000000A85E9000 000030 (v01 LENOVO THINKSYS 00000000 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: HMAT 0x00000000A85E8000 000180 (v01 LENOVO THINKSYS 00000001 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: HPET 0x00000000A85E7000 000038 (v01 LENOVO THINKSYS 00000001 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: MIGT 0x00000000A85E6000 000040 (v01 LENOVO THINKSYS 00000000 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: MSCT 0x00000000A85E5000 000090 (v01 LENOVO THINKSYS 00000001 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: WDDT 0x00000000A85E4000 000040 (v01 LENOVO THINKSYS 00000000 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: APIC 0x00000000A85E2000 00025E (v04 LENOVO THINKSYS 00000000 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SLIT 0x00000000A85E1000 000030 (v01 LENOVO THINKSYS 00000001      01000013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SRAT 0x00000000A85DA000 006430 (v03 LENOVO THINKSYS 00000002      01000013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: OEM4 0x00000000A8452000 187A61 (v02 INTEL  CPU  CST 00003000 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A83DB000 0764A5 (v02 INTEL  SSDT  PM 00004000 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: BDAT 0x00000000A83DA000 000030 (v01 LENOVO THINKSYS 00000000 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: HEST 0x00000000A83D9000 0004BC (v01 LENOVO THINKSYS 00000001 INTL 00000001)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: DMAR 0x00000000A83D8000 000308 (v01 LENOVO THINKSYS 00000001 INTL 20091013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: FPDT 0x00000000A83D7000 000044 (v01 LENOVO THINKSYS 00000100      01000013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SPCR 0x00000000A83D6000 000050 (v02 LENOVO THINKSYS 00000100      00050016)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A83CE000 0078BA (v02 INTEL  SpsNm    00000002 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SLIC 0x00000000A83CD000 000176 (v01 LENOVO THINKSYS 00000100 AMI  00010013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A83CC000 0003A7 (v02 LENOVO CDNTABLE 00001000 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A83CB000 000919 (v02 INTEL  xh_nccrb 00000000 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: WSMT 0x00000000A85E3000 000028 (v01 LENOVO THINKSYS 00000100 AMI  00010013)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SSDT 0x00000000A83CA000 0002FE (v02 LENOVO PCHSLOTS 00001000 INTL 20200925)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Local APIC address 0xfee00000
Apr 02 11:09:27 node-9.domain.tld kernel: system APIC only can use physical flat
Apr 02 11:09:27 node-9.domain.tld kernel: Setting APIC routing to physical flat.
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x00 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x01 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x02 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x03 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x04 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x05 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x06 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x07 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x08 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x09 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x0a -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x0b -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x0c -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x0d -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x0e -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x0f -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x10 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x11 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x12 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x13 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x14 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x15 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x16 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x17 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x18 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x19 -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x1a -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x1b -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x1c -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x1d -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x1e -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 0 -> APIC 0x1f -> Node 0
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x80 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x81 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x82 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x83 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x84 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x85 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x86 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x87 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x88 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x89 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x8a -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x8b -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x8c -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x8d -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x8e -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x8f -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x90 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x91 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x92 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x93 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x94 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x95 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x96 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x97 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x98 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x99 -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x9a -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x9b -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x9c -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x9d -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x9e -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: SRAT: PXM 1 -> APIC 0x9f -> Node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SRAT: Node 0 PXM 0 [mem 0x00000000-0xbfffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SRAT: Node 0 PXM 0 [mem 0x100000000-0x803fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SRAT: Node 1 PXM 1 [mem 0x8040000000-0x1003fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: NUMA: Initialized distance table, cnt=2
Apr 02 11:09:27 node-9.domain.tld kernel: NUMA: Node 0 [mem 0x00000000-0xbfffffff] + [mem 0x100000000-0x803fffffff] -> [mem 0x00000000-0x803fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: NODE_DATA(0) allocated [mem 0x803ffd6000-0x803fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: NODE_DATA(1) allocated [mem 0x1003ffd5000-0x1003fffefff]
Apr 02 11:09:27 node-9.domain.tld kernel: Reserving 300MB of memory at 1952MB for crashkernel (System RAM: 1048227MB)
Apr 02 11:09:27 node-9.domain.tld kernel: Zone ranges:
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA      [mem 0x0000000000001000-0x0000000000ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA32    [mem 0x0000000001000000-0x00000000ffffffff]
Apr 02 11:09:27 node-9.domain.tld kernel:   Normal   [mem 0x0000000100000000-0x000001003fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel:   Device   empty
Apr 02 11:09:27 node-9.domain.tld kernel: Movable zone start for each node
Apr 02 11:09:27 node-9.domain.tld kernel: Early memory node ranges
Apr 02 11:09:27 node-9.domain.tld kernel:   node   0: [mem 0x0000000000001000-0x000000000003dfff]
Apr 02 11:09:27 node-9.domain.tld kernel:   node   0: [mem 0x000000000003f000-0x000000000009ffff]
Apr 02 11:09:27 node-9.domain.tld kernel:   node   0: [mem 0x0000000000100000-0x00000000a5c56fff]
Apr 02 11:09:27 node-9.domain.tld kernel:   node   0: [mem 0x00000000aafff000-0x00000000af7fffff]
Apr 02 11:09:27 node-9.domain.tld kernel:   node   0: [mem 0x0000000100000000-0x000000803fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel:   node   1: [mem 0x0000008040000000-0x000001003fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: Zeroed struct page in unavailable ranges: 23562 pages
Apr 02 11:09:27 node-9.domain.tld kernel: Initmem setup node 0 [mem 0x0000000000001000-0x000000803fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: On node 0 totalpages: 134128630
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA zone: 64 pages used for memmap
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA zone: 25 pages reserved
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA zone: 3998 pages, LIFO batch:0
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA32 zone: 10834 pages used for memmap
Apr 02 11:09:27 node-9.domain.tld kernel:   DMA32 zone: 693336 pages, LIFO batch:31
Apr 02 11:09:27 node-9.domain.tld kernel:   Normal zone: 2084864 pages used for memmap
Apr 02 11:09:27 node-9.domain.tld kernel:   Normal zone: 133431296 pages, LIFO batch:31
Apr 02 11:09:27 node-9.domain.tld kernel: Initmem setup node 1 [mem 0x0000008040000000-0x000001003fffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: On node 1 totalpages: 134217728
Apr 02 11:09:27 node-9.domain.tld kernel:   Normal zone: 2097152 pages used for memmap
Apr 02 11:09:27 node-9.domain.tld kernel:   Normal zone: 134217728 pages, LIFO batch:31
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PM-Timer IO Port: 0x508
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Local APIC address 0xfee00000
Apr 02 11:09:27 node-9.domain.tld kernel: system APIC only can use physical flat
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: X2APIC_NMI (uid[0xffffffff] high edge lint[0x1])
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: LAPIC_NMI (acpi_id[0xff] high edge lint[0x1])
Apr 02 11:09:27 node-9.domain.tld kernel: IOAPIC[0]: apic_id 8, version 32, address 0xfec00000, GSI 0-119
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: IRQ0 used by override.
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: IRQ9 used by override.
Apr 02 11:09:27 node-9.domain.tld kernel: Using ACPI (MADT) for SMP configuration information
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: HPET id: 0x8086a701 base: 0xfed00000
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: SPCR: console: uart,io,0x2f8,115200
Apr 02 11:09:27 node-9.domain.tld kernel: smpboot: Allowing 64 CPUs, 0 hotplug CPUs
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x00000000-0x00000fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x0003e000-0x0003efff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x000a0000-0x000fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99b25000-0x99b25fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99b5c000-0x99b5cfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99b5d000-0x99b5dfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99b94000-0x99b94fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99b95000-0x99b95fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99bcb000-0x99bcbfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99bcc000-0x99bccfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99c02000-0x99c02fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99c03000-0x99c03fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99c72000-0x99c72fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99c73000-0x99c73fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x99ce2000-0x99ce2fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a365000-0x9a365fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a38c000-0x9a38cfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a455000-0x9a455fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a45d000-0x9a45dfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a6ff000-0x9a6fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a707000-0x9a707fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a7f0000-0x9a7f0fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a7f8000-0x9a7f8fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a814000-0x9a814fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a81c000-0x9a81cfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a855000-0x9a855fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0x9a85d000-0x9a85dfff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xa5c57000-0xa7d56fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xa7d57000-0xa8656fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xa8657000-0xa8b0efff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xa8b0f000-0xaaffefff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xaf800000-0xcfffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xd0000000-0xfcffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xfd000000-0xfe7fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xfe800000-0xfed1ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xfed20000-0xfed44fff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xfed45000-0xfeffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registered nosave memory: [mem 0xff000000-0xffffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: [mem 0xd0000000-0xfcffffff] available for PCI devices
Apr 02 11:09:27 node-9.domain.tld kernel: Booting paravirtualized kernel on bare hardware
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: refined-jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 1910969940391419 ns
Apr 02 11:09:27 node-9.domain.tld kernel: Detected CPU family 6 model 106 stepping 6
Apr 02 11:09:27 node-9.domain.tld kernel: Warning: Intel Processor - this hardware has not undergone testing by Red Hat and might not be certified. Please consult https://hardware.redhat.com for certified hardware.
Apr 02 11:09:27 node-9.domain.tld kernel: setup_percpu: NR_CPUS:8192 nr_cpumask_bits:64 nr_cpu_ids:64 nr_node_ids:2
Apr 02 11:09:27 node-9.domain.tld kernel: percpu: Embedded 47 pages/cpu s155648 r8192 d28672 u262144
Apr 02 11:09:27 node-9.domain.tld kernel: pcpu-alloc: s155648 r8192 d28672 u262144 alloc=1*2097152
Apr 02 11:09:27 node-9.domain.tld kernel: pcpu-alloc: [0] 00 01 02 03 04 05 06 07 [0] 08 09 10 11 12 13 14 15 
Apr 02 11:09:27 node-9.domain.tld kernel: pcpu-alloc: [0] 32 33 34 35 36 37 38 39 [0] 40 41 42 43 44 45 46 47 
Apr 02 11:09:27 node-9.domain.tld kernel: pcpu-alloc: [1] 16 17 18 19 20 21 22 23 [1] 24 25 26 27 28 29 30 31 
Apr 02 11:09:27 node-9.domain.tld kernel: pcpu-alloc: [1] 48 49 50 51 52 53 54 55 [1] 56 57 58 59 60 61 62 63 
Apr 02 11:09:27 node-9.domain.tld kernel: Built 2 zonelists, mobility grouping on.  Total pages: 264153419
Apr 02 11:09:27 node-9.domain.tld kernel: Policy zone: Normal
Apr 02 11:09:27 node-9.domain.tld kernel: Kernel command line: BOOT_IMAGE=(hd0,gpt3)/vmlinuz-4.18.0-147.5.1.es8_24.x86_64 root=/dev/mapper/os-root ro edd=off kvm.halt_poll_ns=400000 cgroup.memory=nokmem intel_iommu=on iommu=pt pci=realloc ixgbe.allow_unsupported_sfp=1 rootdelay=90 nomodeset intel_idle.max_cstate=0 processor.max_cstate=0 crashkernel=300M rd.lvm.lv=os/root biosdevname=0 net.ifnames=1 nopti
Apr 02 11:09:27 node-9.domain.tld kernel: Specific versions of hardware are certified with Red Hat Enterprise Linux 8. Please see the list of hardware certified with Red Hat Enterprise Linux 8 at https://access.redhat.com/ecosystem.
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: IOMMU enabled
Apr 02 11:09:27 node-9.domain.tld kernel: Memory: 2301200K/1073385432K available (12292K kernel code, 2117K rwdata, 3780K rodata, 2364K init, 6356K bss, 17572276K reserved, 0K cma-reserved)
Apr 02 11:09:27 node-9.domain.tld kernel: SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=64, Nodes=2
Apr 02 11:09:27 node-9.domain.tld kernel: ftrace: allocating 35707 entries in 140 pages
Apr 02 11:09:27 node-9.domain.tld kernel: rcu: Hierarchical RCU implementation.
Apr 02 11:09:27 node-9.domain.tld kernel: rcu:         RCU restricting CPUs from NR_CPUS=8192 to nr_cpu_ids=64.
Apr 02 11:09:27 node-9.domain.tld kernel: rcu: RCU calculated value of scheduler-enlistment delay is 100 jiffies.
Apr 02 11:09:27 node-9.domain.tld kernel: rcu: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=64
Apr 02 11:09:27 node-9.domain.tld kernel: NR_IRQS: 524544, nr_irqs: 2568, preallocated irqs: 16
Apr 02 11:09:27 node-9.domain.tld kernel: random: get_random_bytes called from start_kernel+0x36b/0x55b with crng_init=0
Apr 02 11:09:27 node-9.domain.tld kernel: Console: colour dummy device 80x25
Apr 02 11:09:27 node-9.domain.tld kernel: console [tty0] enabled
Apr 02 11:09:27 node-9.domain.tld kernel: mempolicy: Enabling automatic NUMA balancing. Configure with numa_balancing= or the kernel.numa_balancing sysctl
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Core revision 20180531
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: hpet: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 79635855245 ns
Apr 02 11:09:27 node-9.domain.tld kernel: hpet clockevent registered
Apr 02 11:09:27 node-9.domain.tld kernel: APIC: Switch to symmetric I/O mode setup
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Host address width 46
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000e7ffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar0: reg_base_addr e7ffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000ebffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar1: reg_base_addr ebffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000efffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar2: reg_base_addr efffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000f3ffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar3: reg_base_addr f3ffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000f77fc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar4: reg_base_addr f77fc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000d7ffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar5: reg_base_addr d7ffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000dbffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar6: reg_base_addr dbffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000dfffc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar7: reg_base_addr dfffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000e37fc000 flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar8: reg_base_addr e37fc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: DRHD base: 0x000000d3ffc000 flags: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar9: reg_base_addr d3ffc000 ver 4:0 cap 8ed008c40780466 ecap 60000f050df
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RMRR base: 0x000000aaac6000 end: 0x000000aaae9fff
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RMRR base: 0x000000aa7e2000 end: 0x000000aaa2bfff
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: ATSR flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: ATSR flags: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000d3ffc000 proximity domain: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000d7ffc000 proximity domain: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000dbffc000 proximity domain: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000dfffc000 proximity domain: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000e37fc000 proximity domain: 0x0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000e7ffc000 proximity domain: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000ebffc000 proximity domain: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000efffc000 proximity domain: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000f3ffc000 proximity domain: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: RHSA base: 0x000000f77fc000 proximity domain: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR-IR: IOAPIC id 8 under DRHD base  0xd3ffc000 IOMMU 9
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR-IR: HPET id 0 under DRHD base 0xd3ffc000
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR-IR: Queued invalidation will be enabled to support x2apic and Intr-remapping.
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR-IR: Enabled IRQ remapping in x2apic mode
Apr 02 11:09:27 node-9.domain.tld kernel: x2apic enabled
Apr 02 11:09:27 node-9.domain.tld kernel: System requires x2apic physical mode
Apr 02 11:09:27 node-9.domain.tld kernel: Switched APIC routing to physical x2apic.
Apr 02 11:09:27 node-9.domain.tld kernel: ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1
Apr 02 11:09:27 node-9.domain.tld kernel: tsc: Detected 3100.000 MHz processor
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: tsc-early: mask: 0xffffffffffffffff max_cycles: 0x2caf47d9d39, max_idle_ns: 440795292816 ns
Apr 02 11:09:27 node-9.domain.tld kernel: Calibrating delay loop (skipped), value calculated using timer frequency.. 6200.00 BogoMIPS (lpj=3100000)
Apr 02 11:09:27 node-9.domain.tld kernel: pid_max: default: 65536 minimum: 512
Apr 02 11:09:27 node-9.domain.tld kernel: Security Framework initialized
Apr 02 11:09:27 node-9.domain.tld kernel: Yama: becoming mindful.
Apr 02 11:09:27 node-9.domain.tld kernel: SELinux:  Initializing.
Apr 02 11:09:27 node-9.domain.tld kernel: Dentry cache hash table entries: 33554432 (order: 16, 268435456 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: Inode-cache hash table entries: 16777216 (order: 15, 134217728 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: Mount-cache hash table entries: 524288 (order: 10, 4194304 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: Mountpoint-cache hash table entries: 524288 (order: 10, 4194304 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: x86/tme: not enabled by BIOS
Apr 02 11:09:27 node-9.domain.tld kernel: ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
Apr 02 11:09:27 node-9.domain.tld kernel: ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)
Apr 02 11:09:27 node-9.domain.tld kernel: x86/cpu: User Mode Instruction Prevention (UMIP) activated
Apr 02 11:09:27 node-9.domain.tld kernel: CPU0: Thermal monitoring enabled (TM1)
Apr 02 11:09:27 node-9.domain.tld kernel: process: using mwait in idle threads
Apr 02 11:09:27 node-9.domain.tld kernel: Last level iTLB entries: 4KB 0, 2MB 0, 4MB 0
Apr 02 11:09:27 node-9.domain.tld kernel: Last level dTLB entries: 4KB 0, 2MB 0, 4MB 0, 1GB 0
Apr 02 11:09:27 node-9.domain.tld kernel: FEATURE SPEC_CTRL Present
Apr 02 11:09:27 node-9.domain.tld kernel: FEATURE IBPB_SUPPORT Present
Apr 02 11:09:27 node-9.domain.tld kernel: Spectre V1 : Mitigation: usercopy/swapgs barriers and __user pointer sanitization
Apr 02 11:09:27 node-9.domain.tld kernel: Spectre V2 : Mitigation: Enhanced IBRS
Apr 02 11:09:27 node-9.domain.tld kernel: Spectre V2 : Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch
Apr 02 11:09:27 node-9.domain.tld kernel: Spectre V2 : mitigation: Enabling conditional Indirect Branch Prediction Barrier
Apr 02 11:09:27 node-9.domain.tld kernel: Speculative Store Bypass: Mitigation: Speculative Store Bypass disabled via prctl and seccomp
Apr 02 11:09:27 node-9.domain.tld kernel: Freeing SMP alternatives memory: 32K
Apr 02 11:09:27 node-9.domain.tld kernel: TSC deadline timer enabled
Apr 02 11:09:27 node-9.domain.tld kernel: smpboot: CPU0: Intel(R) Xeon(R) Gold 6346 CPU @ 3.10GHz (family: 0x6, model: 0x6a, stepping: 0x6)
Apr 02 11:09:27 node-9.domain.tld kernel: Performance Events: no PEBS fmt4+, generic architected perfmon, 
Apr 02 11:09:27 node-9.domain.tld kernel: ------------[ cut here ]------------
Apr 02 11:09:27 node-9.domain.tld kernel: hw perf events fixed 4 > max(3), clipping!
Apr 02 11:09:27 node-9.domain.tld kernel: WARNING: CPU: 0 PID: 1 at arch/x86/events/intel/core.c:4695 intel_pmu_init+0x1245/0x144b
Apr 02 11:09:27 node-9.domain.tld kernel: Modules linked in:
Apr 02 11:09:27 node-9.domain.tld kernel: CPU: 0 PID: 1 Comm: swapper/0 Not tainted 4.18.0-147.5.1.es8_24.x86_64 #1
Apr 02 11:09:27 node-9.domain.tld kernel: Hardware name: Lenovo ThinkSystem SR670 V2/7Z23CTOLWW, BIOS U8E124I-2.10 07/20/2023
Apr 02 11:09:27 node-9.domain.tld kernel: RIP: 0010:intel_pmu_init+0x1245/0x144b
Apr 02 11:09:27 node-9.domain.tld kernel: Code: 00 8b 35 7e 0b c6 ff 48 d3 e0 48 ff c8 48 89 05 59 0c c6 ff 83 fe 03 7e 1d ba 03 00 00 00 48 c7 c7 c0 cc 46 ba e8 fd 18 91 fe <0f> 0b c7 05 4f 0b c6 ff 03 00 00 00 8b 0d 49 0b c6 ff b8 01 00 00
Apr 02 11:09:27 node-9.domain.tld kernel: RSP: 0000:ff6c878000067d50 EFLAGS: 00010282
Apr 02 11:09:27 node-9.domain.tld kernel: RAX: 0000000000000000 RBX: 0000000000000000 RCX: ffffffffba65a348
Apr 02 11:09:27 node-9.domain.tld kernel: RDX: 0000000000000001 RSI: 0000000000000082 RDI: 0000000000000247
Apr 02 11:09:27 node-9.domain.tld kernel: RBP: ff6c878000067dc0 R08: 000000000000023b R09: ffffffffbae37f22
Apr 02 11:09:27 node-9.domain.tld kernel: R10: 0024d50c489a7637 R11: 000000000000002a R12: 0000000000000000
Apr 02 11:09:27 node-9.domain.tld kernel: R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
Apr 02 11:09:27 node-9.domain.tld kernel: FS:  0000000000000000(0000) GS:ff1cea7e3f800000(0000) knlGS:0000000000000000
Apr 02 11:09:27 node-9.domain.tld kernel: CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
Apr 02 11:09:27 node-9.domain.tld kernel: CR2: ff1cea59fba02000 CR3: 00000059fac0a001 CR4: 0000000000761ef0
Apr 02 11:09:27 node-9.domain.tld kernel: DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
Apr 02 11:09:27 node-9.domain.tld kernel: DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
Apr 02 11:09:27 node-9.domain.tld kernel: PKRU: 00000000
Apr 02 11:09:27 node-9.domain.tld kernel: Call Trace:
Apr 02 11:09:27 node-9.domain.tld kernel:  ? printk+0x58/0x6f
Apr 02 11:09:27 node-9.domain.tld kernel:  ? merge_attr+0xac/0xac
Apr 02 11:09:27 node-9.domain.tld kernel:  init_hw_perf_events+0x46/0x5f4
Apr 02 11:09:27 node-9.domain.tld kernel:  ? merge_attr+0xac/0xac
Apr 02 11:09:27 node-9.domain.tld kernel:  do_one_initcall+0x46/0x1c3
Apr 02 11:09:27 node-9.domain.tld kernel:  ? proc_register+0xc5/0x120
Apr 02 11:09:27 node-9.domain.tld kernel:  kernel_init_freeable+0xf6/0x258
Apr 02 11:09:27 node-9.domain.tld kernel:  ? rest_init+0xaa/0xaa
Apr 02 11:09:27 node-9.domain.tld kernel:  kernel_init+0xa/0xfc
Apr 02 11:09:27 node-9.domain.tld kernel:  ret_from_fork+0x1f/0x40
Apr 02 11:09:27 node-9.domain.tld kernel: ---[ end trace fd10bf108d2e1b6f ]---
Apr 02 11:09:27 node-9.domain.tld kernel: full-width counters, Intel PMU driver.
Apr 02 11:09:27 node-9.domain.tld kernel: ... version:                5
Apr 02 11:09:27 node-9.domain.tld kernel: ... bit width:              48
Apr 02 11:09:27 node-9.domain.tld kernel: ... generic registers:      8
Apr 02 11:09:27 node-9.domain.tld kernel: ... value mask:             0000ffffffffffff
Apr 02 11:09:27 node-9.domain.tld kernel: ... max period:             00007fffffffffff
Apr 02 11:09:27 node-9.domain.tld kernel: ... fixed-purpose events:   3
Apr 02 11:09:27 node-9.domain.tld kernel: ... event mask:             00000007000000ff
Apr 02 11:09:27 node-9.domain.tld kernel: rcu: Hierarchical SRCU implementation.
Apr 02 11:09:27 node-9.domain.tld kernel: NMI watchdog: Enabled. Permanently consumes one hw-PMU counter.
Apr 02 11:09:27 node-9.domain.tld kernel: smp: Bringing up secondary CPUs ...
Apr 02 11:09:27 node-9.domain.tld kernel: x86: Booting SMP configuration:
Apr 02 11:09:27 node-9.domain.tld kernel: .... node  #0, CPUs:        #1  #2  #3  #4  #5  #6  #7  #8  #9 #10 #11 #12 #13 #14 #15
Apr 02 11:09:27 node-9.domain.tld kernel: .... node  #1, CPUs:   #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31
Apr 02 11:09:27 node-9.domain.tld kernel: .... node  #0, CPUs:   #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47
Apr 02 11:09:27 node-9.domain.tld kernel: .... node  #1, CPUs:   #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63
Apr 02 11:09:27 node-9.domain.tld kernel: smp: Brought up 2 nodes, 64 CPUs
Apr 02 11:09:27 node-9.domain.tld kernel: smpboot: Max logical packages: 2
Apr 02 11:09:27 node-9.domain.tld kernel: smpboot: Total of 64 processors activated (397005.95 BogoMIPS)
Apr 02 11:09:27 node-9.domain.tld kernel: node 0 initialised, 131259729 pages in 3594ms
Apr 02 11:09:27 node-9.domain.tld kernel: node 1 initialised, 132052724 pages in 3686ms
Apr 02 11:09:27 node-9.domain.tld kernel: devtmpfs: initialized
Apr 02 11:09:27 node-9.domain.tld kernel: x86/mm: Memory block size: 1024MB
Apr 02 11:09:27 node-9.domain.tld kernel: PM: Registering ACPI NVS region [mem 0xa8657000-0xa8b0efff] (4947968 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 1911260446275000 ns
Apr 02 11:09:27 node-9.domain.tld kernel: futex hash table entries: 16384 (order: 8, 1048576 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: pinctrl core: initialized pinctrl subsystem
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 16
Apr 02 11:09:27 node-9.domain.tld kernel: audit: initializing netlink subsys (disabled)
Apr 02 11:09:27 node-9.domain.tld kernel: audit: type=2000 audit(1712027363.341:1): state=initialized audit_enabled=0 res=1
Apr 02 11:09:27 node-9.domain.tld kernel: cpuidle: using governor menu
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI FADT declares the system doesn't support PCIe ASPM, so disable it
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: bus type PCI registered
Apr 02 11:09:27 node-9.domain.tld kernel: acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: MMCONFIG for domain 0000 [bus 00-ff] at [mem 0xc0000000-0xcfffffff] (base 0xc0000000)
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: MMCONFIG at [mem 0xc0000000-0xcfffffff] reserved in E820
Apr 02 11:09:27 node-9.domain.tld kernel: pmd_set_huge: Cannot satisfy [mem 0xc0000000-0xc0200000] with a huge-page mapping due to MTRR override.
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: Using configuration type 1 for base access
Apr 02 11:09:27 node-9.domain.tld kernel: HugeTLB registered 1.00 GiB page size, pre-allocated 0 pages
Apr 02 11:09:27 node-9.domain.tld kernel: HugeTLB registered 2.00 MiB page size, pre-allocated 0 pages
Apr 02 11:09:27 node-9.domain.tld kernel: cryptd: max_cpu_qlen set to 1000
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Added _OSI(Module Device)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Added _OSI(Processor Device)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Added _OSI(3.0 _SCP Extensions)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Added _OSI(Processor Aggregator Device)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Added _OSI(Linux-Dell-Video)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: 8 ACPI AML tables successfully acquired and loaded
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Interpreter enabled
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: (supports S0 S5)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Using IOAPIC for interrupt routing
Apr 02 11:09:27 node-9.domain.tld kernel: HEST: Enabling Firmware First mode for corrected errors.
Apr 02 11:09:27 node-9.domain.tld kernel: HEST: Table parsing has been initialized.
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: Using host bridge windows from ACPI; if necessary, use "pci=nocrs" and report a bug
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Enabled 5 GPEs in block 00 to 7F
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC00] (domain 0000 [bus 00-15])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:00: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:00: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:00: _OSC: platform does not support [SHPCHotplug LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:00: _OSC: OS now controls [PCIeHotplug PME PCIeCapability]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:00: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:00
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7 window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [io  0x1000-0x3fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [mem 0x000c8000-0x000cffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [mem 0xfe010000-0xfe010fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [mem 0xd0000000-0xd3ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [mem 0x20000000000-0x2ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: root bus resource [bus 00-15]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.0: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.0: reg 0x10: [mem 0x2fffff50000-0x2fffff53fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.1: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.1: reg 0x10: [mem 0x2fffff4c000-0x2fffff4ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.2: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.2: reg 0x10: [mem 0x2fffff48000-0x2fffff4bfff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.3: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.3: reg 0x10: [mem 0x2fffff44000-0x2fffff47fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.4: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.4: reg 0x10: [mem 0x2fffff40000-0x2fffff43fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.5: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.5: reg 0x10: [mem 0x2fffff3c000-0x2fffff3ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.6: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.6: reg 0x10: [mem 0x2fffff38000-0x2fffff3bfff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.7: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:01.7: reg 0x10: [mem 0x2fffff34000-0x2fffff37fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.0: [8086:09a6] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.0: reg 0x10: [mem 0xd3f88000-0xd3f89fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.1: [8086:09a7] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.1: reg 0x10: [mem 0xd3f00000-0xd3f7ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.1: reg 0x14: [mem 0xd3e80000-0xd3efffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.4: [8086:3456] type 00 class 0x130000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.4: reg 0x10: [mem 0x2ffffe00000-0x2ffffefffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.4: reg 0x18: [mem 0x2fffff30000-0x2fffff33fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:02.4: reg 0x20: [mem 0x2fffff00000-0x2fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.0: [8086:a1ec] type 00 class 0xff0000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: [8086:a1d2] type 00 class 0x010601
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: reg 0x10: [mem 0xd3f86000-0xd3f87fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: reg 0x14: [mem 0xd3f8b000-0xd3f8b0ff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: reg 0x18: [io  0x3070-0x3077]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: reg 0x1c: [io  0x3060-0x3063]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: reg 0x20: [io  0x3020-0x303f]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: reg 0x24: [mem 0xd3d80000-0xd3dfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:11.5: PME# supported from D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:14.0: [8086:a1af] type 00 class 0x0c0330
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:14.0: reg 0x10: [mem 0x2fffff20000-0x2fffff2ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:14.0: PME# supported from D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:14.2: [8086:a1b1] type 00 class 0x118000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:14.2: reg 0x10: [mem 0x2fffff57000-0x2fffff57fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.0: [8086:a1ba] type 00 class 0x078000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.0: reg 0x10: [mem 0x2fffff56000-0x2fffff56fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.0: PME# supported from D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.1: [8086:a1bb] type 00 class 0x078000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.1: reg 0x10: [mem 0x2fffff55000-0x2fffff55fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.1: PME# supported from D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.4: [8086:a1be] type 00 class 0x078000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.4: reg 0x10: [mem 0x2fffff54000-0x2fffff54fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:16.4: PME# supported from D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: [8086:a182] type 00 class 0x010601
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: reg 0x10: [mem 0xd3f84000-0xd3f85fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: reg 0x14: [mem 0xd3f8a000-0xd3f8a0ff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: reg 0x18: [io  0x3050-0x3057]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: reg 0x1c: [io  0x3040-0x3043]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: reg 0x20: [io  0x3000-0x301f]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: reg 0x24: [mem 0xd3d00000-0xd3d7ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:17.0: PME# supported from D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.0: [8086:a190] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6: [8086:a196] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.0: [8086:a1cb] type 00 class 0x060100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.2: [8086:a1a1] type 00 class 0x058000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.2: reg 0x10: [mem 0xd3f80000-0xd3f83fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.4: [8086:a1a3] type 00 class 0x0c0500
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.4: reg 0x10: [mem 0x00000000-0x000000ff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.4: reg 0x20: [io  0x0780-0x079f]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.5: [8086:a1a4] type 00 class 0x0c8000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.5: reg 0x10: [mem 0xfe010000-0xfe010fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0: [19a2:0120] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0: reg 0x10: [mem 0xd3b00000-0xd3b00fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.0: PCI bridge to [bus 01-02]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.0:   bridge window [mem 0xd2000000-0xd3bfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:02: extended config space not accessible
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: [102b:0522] type 00 class 0x030000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: reg 0x10: [mem 0xd2000000-0xd2ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: reg 0x14: [mem 0xd3a10000-0xd3a13fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: reg 0x18: [mem 0xd3000000-0xd37fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: reg 0x30: [mem 0xd3a00000-0xd3a0ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: BAR 0: assigned to efifb
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0: PCI bridge to [bus 02]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0:   bridge window [mem 0xd2000000-0xd3afffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0:   bridge window [mem 0x00000000-0x000fffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: [1b4b:9230] type 00 class 0x010601
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x10: [io  0x2050-0x2057]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x14: [io  0x2040-0x2043]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x18: [io  0x2030-0x2037]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x1c: [io  0x2020-0x2023]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x20: [io  0x2000-0x201f]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x24: [mem 0xd3c40000-0xd3c407ff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: reg 0x30: [mem 0xd3c00000-0xd3c3ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: Enabling fixed DMA alias to 00.1
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:03:00.0: PME# supported from D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6: PCI bridge to [bus 03]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6:   bridge window [io  0x2000-0x2fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6:   bridge window [mem 0xd3c00000-0xd3cfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC01] (domain 0000 [bus 16-2f])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:01: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:01: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:01: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:01: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:01: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:16
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: root bus resource [io  0x4000-0x5fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: root bus resource [mem 0xd4000000-0xd7ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: root bus resource [mem 0x30000000000-0x3ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: root bus resource [bus 16-2f]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: reg 0x10: [mem 0x3fffff00000-0x3fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: PCI bridge to [bus 17-1b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0:   bridge window [mem 0xd4000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0:   bridge window [mem 0x3a000000000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:1f.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:1f.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0: PCI bridge to [bus 18-1b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0:   bridge window [mem 0xd4000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0:   bridge window [mem 0x3a000000000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: [10de:20f1] type 00 class 0x030200
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: reg 0x10: [mem 0xd6000000-0xd6ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: reg 0x14: [mem 0x3d000000000-0x3dfffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: reg 0x1c: [mem 0x3f020000000-0x3f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: PME# supported from D0 D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: reg 0xbf0: [mem 0xd7000000-0xd703ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: VF(n) BAR0 space: [mem 0xd7000000-0xd73fffff] (contains BAR0 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: reg 0xbf4: [mem 0x3e000000000-0x3e0ffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: VF(n) BAR1 space: [mem 0x3e000000000-0x3efffffffff 64bit pref] (contains BAR1 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: reg 0xbfc: [mem 0x3f000000000-0x3f001ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:19:00.0: VF(n) BAR3 space: [mem 0x3f000000000-0x3f01fffffff 64bit pref] (contains BAR3 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0: PCI bridge to [bus 19]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0:   bridge window [mem 0xd6000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0:   bridge window [mem 0x3d000000000-0x3f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: [10de:20f1] type 00 class 0x030200
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: reg 0x10: [mem 0xd4000000-0xd4ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: reg 0x14: [mem 0x3a000000000-0x3afffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: reg 0x1c: [mem 0x3c020000000-0x3c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: PME# supported from D0 D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: reg 0xbf0: [mem 0xd5000000-0xd503ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: VF(n) BAR0 space: [mem 0xd5000000-0xd53fffff] (contains BAR0 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: reg 0xbf4: [mem 0x3b000000000-0x3b0ffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: VF(n) BAR1 space: [mem 0x3b000000000-0x3bfffffffff 64bit pref] (contains BAR1 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: reg 0xbfc: [mem 0x3c000000000-0x3c001ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1a:00.0: VF(n) BAR3 space: [mem 0x3c000000000-0x3c01fffffff 64bit pref] (contains BAR3 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0: PCI bridge to [bus 1a]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0:   bridge window [mem 0xd4000000-0xd53fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0:   bridge window [mem 0x3a000000000-0x3c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1b:00.0: [1000:00b2] type 00 class 0x010700
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1b:00.0: reg 0x10: [mem 0x3f022100000-0x3f022103fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:1b:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:1f.0: PCI bridge to [bus 1b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:1f.0:   bridge window [mem 0x3f022100000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC02] (domain 0000 [bus 30-49])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:02: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:02: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:02: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:02: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:02: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:30
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: root bus resource [io  0x6000-0x7fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: root bus resource [mem 0xd8000000-0xdbffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: root bus resource [mem 0x40000000000-0x4ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: root bus resource [bus 30-49]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: reg 0x10: [mem 0x4fffff00000-0x4fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: PCI bridge to [bus 31-34]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0:   bridge window [mem 0xd8000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0:   bridge window [mem 0x4a000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0: PCI bridge to [bus 32-34]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0:   bridge window [mem 0xd8000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0:   bridge window [mem 0x4a000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: [10de:20f1] type 00 class 0x030200
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: reg 0x10: [mem 0xda000000-0xdaffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: reg 0x14: [mem 0x4d000000000-0x4dfffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: reg 0x1c: [mem 0x4f020000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: PME# supported from D0 D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: reg 0xbf0: [mem 0xdb000000-0xdb03ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: VF(n) BAR0 space: [mem 0xdb000000-0xdb3fffff] (contains BAR0 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: reg 0xbf4: [mem 0x4e000000000-0x4e0ffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: VF(n) BAR1 space: [mem 0x4e000000000-0x4efffffffff 64bit pref] (contains BAR1 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: reg 0xbfc: [mem 0x4f000000000-0x4f001ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:33:00.0: VF(n) BAR3 space: [mem 0x4f000000000-0x4f01fffffff 64bit pref] (contains BAR3 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0: PCI bridge to [bus 33]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0:   bridge window [mem 0xda000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0:   bridge window [mem 0x4d000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: [10de:20f1] type 00 class 0x030200
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: reg 0x10: [mem 0xd8000000-0xd8ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: reg 0x14: [mem 0x4a000000000-0x4afffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: reg 0x1c: [mem 0x4c020000000-0x4c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: PME# supported from D0 D3hot
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: reg 0xbf0: [mem 0xd9000000-0xd903ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: VF(n) BAR0 space: [mem 0xd9000000-0xd93fffff] (contains BAR0 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: reg 0xbf4: [mem 0x4b000000000-0x4b0ffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: VF(n) BAR1 space: [mem 0x4b000000000-0x4bfffffffff 64bit pref] (contains BAR1 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: reg 0xbfc: [mem 0x4c000000000-0x4c001ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:34:00.0: VF(n) BAR3 space: [mem 0x4c000000000-0x4c01fffffff 64bit pref] (contains BAR3 for 16 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0: PCI bridge to [bus 34]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0:   bridge window [mem 0xd8000000-0xd93fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0:   bridge window [mem 0x4a000000000-0x4c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC04] (domain 0000 [bus 4a-63])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:04: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:04: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:04: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:04: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:04: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:4a
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: root bus resource [io  0x8000-0x9fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: root bus resource [mem 0xdc000000-0xdfffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: root bus resource [mem 0x50000000000-0x5ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: root bus resource [bus 4a-63]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0: reg 0x10: [mem 0x5fffff00000-0x5fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.0: [15b3:101b] type 00 class 0x020700
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.0: reg 0x10: [mem 0x5fff6000000-0x5fff7ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.0: reg 0x30: [mem 0xdfe00000-0xdfefffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.0: PME# supported from D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.0: reg 0x1a4: [mem 0x5fffbc00000-0x5fffbdfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.0: VF(n) BAR0 space: [mem 0x5fffbc00000-0x5ffff7fffff 64bit pref] (contains BAR0 for 30 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.1: [15b3:101b] type 00 class 0x020700
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.1: reg 0x10: [mem 0x5fff4000000-0x5fff5ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.1: reg 0x30: [mem 0xdfd00000-0xdfdfffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.1: PME# supported from D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.1: reg 0x1a4: [mem 0x5fff8000000-0x5fff81fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4b:00.1: VF(n) BAR0 space: [mem 0x5fff8000000-0x5fffbbfffff 64bit pref] (contains BAR0 for 30 VFs)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0: PCI bridge to [bus 4b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0:   bridge window [mem 0xdfd00000-0xdfefffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0:   bridge window [mem 0x5fff4000000-0x5ffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC05] (domain 0000 [bus 64-7d])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:05: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:05: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:05: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:05: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:05: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:64
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: root bus resource [io  0xa000-0xafff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: root bus resource [mem 0xe0000000-0xe37fffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: root bus resource [mem 0x60000000000-0x6ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: root bus resource [bus 64-7d]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: reg 0x10: [mem 0x6fffff40000-0x6fffff5ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: [8086:347b] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: reg 0x10: [mem 0x6fffff20000-0x6fffff3ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0: [8086:347c] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0: reg 0x10: [mem 0x6fffff00000-0x6fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:65:00.0: [8086:0a54] type 00 class 0x010802
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:65:00.0: reg 0x10: [mem 0xe3410000-0xe3413fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:65:00.0: reg 0x30: [mem 0xe3400000-0xe340ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: PCI bridge to [bus 65]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [mem 0xe3400000-0xe35fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [mem 0x6ffffa00000-0x6ffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:66:00.0: [8086:0a54] type 00 class 0x010802
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:66:00.0: reg 0x10: [mem 0xe3210000-0xe3213fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:66:00.0: reg 0x30: [mem 0xe3200000-0xe320ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: PCI bridge to [bus 66]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0:   bridge window [mem 0xe3200000-0xe33fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0:   bridge window [mem 0x6ffff800000-0x6ffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.0: [14e4:16d7] type 00 class 0x020000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.0: reg 0x10: [mem 0x6ffffe10000-0x6ffffe1ffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.0: reg 0x18: [mem 0x6ffffd00000-0x6ffffdfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.0: reg 0x20: [mem 0x6ffffe22000-0x6ffffe23fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.0: reg 0x30: [mem 0xe3640000-0xe367ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.1: [14e4:16d7] type 00 class 0x020000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.1: reg 0x10: [mem 0x6ffffe00000-0x6ffffe0ffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.1: reg 0x18: [mem 0x6ffffc00000-0x6ffffcfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.1: reg 0x20: [mem 0x6ffffe20000-0x6ffffe21fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.1: reg 0x30: [mem 0xe3600000-0xe363ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:67:00.1: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0: PCI bridge to [bus 67]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0:   bridge window [mem 0xe3600000-0xe36fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0:   bridge window [mem 0x6ffffc00000-0x6ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [UC06] (domain 0000 [bus 7e])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:06: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:06: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:06: _OSC: platform does not support [SHPCHotplug LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:06: _OSC: OS now controls [PCIeHotplug PME PCIeCapability]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:06: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:7e
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:7e: root bus resource [bus 7e]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:00.0: [8086:3450] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:00.1: [8086:3451] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:00.2: [8086:3452] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:00.3: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:00.5: [8086:3455] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:02.0: [8086:3440] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:02.1: [8086:3441] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:02.2: [8086:3442] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:03.0: [8086:3440] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:03.1: [8086:3441] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:03.2: [8086:3442] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:04.0: [8086:3440] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:04.1: [8086:3441] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:04.2: [8086:3442] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:04.3: [8086:3443] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:05.0: [8086:3445] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:05.1: [8086:3446] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:05.2: [8086:3447] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:06.0: [8086:3445] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:06.1: [8086:3446] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:06.2: [8086:3447] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:07.0: [8086:3445] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:07.1: [8086:3446] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:07.2: [8086:3447] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0b.0: [8086:3448] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0b.1: [8086:3448] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0b.2: [8086:344b] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0c.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0d.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0e.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:0f.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:1a.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:1b.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:1c.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7e:1d.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:7e: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [UC07] (domain 0000 [bus 7f])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:07: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:07: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:07: _OSC: platform does not support [SHPCHotplug LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:07: _OSC: OS now controls [PCIeHotplug PME PCIeCapability]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:07: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:7f
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:7f: root bus resource [bus 7f]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.0: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.1: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.2: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.3: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.4: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.5: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.6: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:00.7: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.0: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.1: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.2: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.3: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.4: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.5: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.6: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:01.7: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.0: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.1: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.2: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.3: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.4: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.5: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.6: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:02.7: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.0: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.1: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.2: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.3: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.4: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.5: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.6: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0a.7: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.0: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.1: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.2: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.3: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.4: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.5: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.6: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0b.7: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.0: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.1: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.2: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.3: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.4: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.5: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.6: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:0c.7: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1d.0: [8086:344f] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1d.1: [8086:3457] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.0: [8086:3458] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.1: [8086:3459] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.2: [8086:345a] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.3: [8086:345b] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.4: [8086:345c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.5: [8086:345d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.6: [8086:345e] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:7f:1e.7: [8086:345f] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:7f: on NUMA node 0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC06] (domain 0000 [bus 80-96])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:08: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:08: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:08: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:08: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:08: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:80
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: root bus resource [io  0xb000-0xbfff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: root bus resource [mem 0xe4000000-0xe7ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: root bus resource [mem 0x70000000000-0x7ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: root bus resource [bus 80-96]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.0: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.0: reg 0x10: [mem 0x7fffff40000-0x7fffff43fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.1: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.1: reg 0x10: [mem 0x7fffff3c000-0x7fffff3ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.2: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.2: reg 0x10: [mem 0x7fffff38000-0x7fffff3bfff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.3: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.3: reg 0x10: [mem 0x7fffff34000-0x7fffff37fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.4: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.4: reg 0x10: [mem 0x7fffff30000-0x7fffff33fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.5: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.5: reg 0x10: [mem 0x7fffff2c000-0x7fffff2ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.6: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.6: reg 0x10: [mem 0x7fffff28000-0x7fffff2bfff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.7: [8086:0b00] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:01.7: reg 0x10: [mem 0x7fffff24000-0x7fffff27fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.0: [8086:09a6] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.0: reg 0x10: [mem 0xe7f80000-0xe7f81fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.1: [8086:09a7] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.1: reg 0x10: [mem 0xe7f00000-0xe7f7ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.1: reg 0x14: [mem 0xe7e80000-0xe7efffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.4: [8086:3456] type 00 class 0x130000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.4: reg 0x10: [mem 0x7ffffe00000-0x7ffffefffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.4: reg 0x18: [mem 0x7fffff20000-0x7fffff23fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:80:02.4: reg 0x20: [mem 0x7fffff00000-0x7fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC07] (domain 0000 [bus 97-af])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:09: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:09: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:09: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:09: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:09: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:97
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: root bus resource [io  0xc000-0xcfff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: root bus resource [mem 0xe8000000-0xebffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: root bus resource [mem 0x80000000000-0x8ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: root bus resource [bus 97-af]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0: reg 0x10: [mem 0x8fffff00000-0x8fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.0: [8086:1572] type 00 class 0x020000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.0: reg 0x10: [mem 0x8ffff000000-0x8ffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.0: reg 0x1c: [mem 0x8ffff808000-0x8ffff80ffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.0: reg 0x30: [mem 0xebe80000-0xebefffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.1: [8086:1572] type 00 class 0x020000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.1: reg 0x10: [mem 0x8fffe800000-0x8fffeffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.1: reg 0x1c: [mem 0x8ffff800000-0x8ffff807fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.1: reg 0x30: [mem 0xebe00000-0xebe7ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:98:00.1: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0: PCI bridge to [bus 98]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0:   bridge window [mem 0xebe00000-0xebefffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0:   bridge window [mem 0x8fffe800000-0x8ffff8fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC08] (domain 0000 [bus b0-c8])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0a: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0a: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0a: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0a: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0a: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:b0
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: root bus resource [io  0xd000-0xdfff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: root bus resource [mem 0xec000000-0xefffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: root bus resource [mem 0x90000000000-0x9ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: root bus resource [bus b0-c8]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: reg 0x10: [mem 0x9fffff00000-0x9fffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PCI bridge to [bus b1-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PCI bridge to [bus b2-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: [1000:02b2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: reg 0x10: [mem 0x00000000-0x01ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: reg 0x14: [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: [Firmware Bug]: reg 0x1c: invalid BAR (can't size)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: PCI bridge to [bus b3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: [1000:02b2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: reg 0x10: [mem 0x00000000-0x01ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: reg 0x14: [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: [Firmware Bug]: reg 0x1c: invalid BAR (can't size)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: PCI bridge to [bus b4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b5:00.0: [1000:00b2] type 00 class 0x010700
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b5:00.0: reg 0x10: [mem 0x9ffffe00000-0x9ffffe03fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b5:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PCI bridge to [bus b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC10] (domain 0000 [bus c9-e1])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0c: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0c: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0c: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0c: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0c: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:c9
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: root bus resource [io  0xe000-0xefff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: root bus resource [mem 0xf0000000-0xf3ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: root bus resource [mem 0xa0000000000-0xaffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: root bus resource [bus c9-e1]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: reg 0x10: [mem 0xafffff00000-0xafffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: PCI bridge to [bus ca-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: [1000:c012] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: PCI bridge to [bus cb-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: [1000:02b2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: reg 0x10: [mem 0x00000000-0x01ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: reg 0x14: [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: [Firmware Bug]: reg 0x1c: invalid BAR (can't size)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: PCI bridge to [bus cc]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: [1000:02b2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: reg 0x10: [mem 0x00000000-0x01ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: reg 0x14: [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: [Firmware Bug]: reg 0x1c: invalid BAR (can't size)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: PCI bridge to [bus cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [PC11] (domain 0000 [bus e2-fa])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0d: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0d: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0d: _OSC: platform does not support [SHPCHotplug]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0d: _OSC: OS now controls [PCIeHotplug PME PCIeCapability LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0d: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:e2
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: root bus resource [io  0xf000-0xffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: root bus resource [mem 0xf4000000-0xf77fffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: root bus resource [mem 0xb0000000000-0xbffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: root bus resource [bus e2-fa]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:00.0: [8086:09a2] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:00.1: [8086:09a4] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:00.2: [8086:09a3] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:00.4: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: [8086:347a] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: reg 0x10: [mem 0xbfffff60000-0xbfffff7ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: [8086:347b] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: reg 0x10: [mem 0xbfffff40000-0xbfffff5ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: [8086:347c] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: reg 0x10: [mem 0xbfffff20000-0xbfffff3ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: [8086:347d] type 01 class 0x060400
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: reg 0x10: [mem 0xbfffff00000-0xbfffff1ffff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: enabling Extended Tags
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: PME# supported from D0 D3hot D3cold
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e3:00.0: [8086:0a54] type 00 class 0x010802
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e3:00.0: reg 0x10: [mem 0xf7410000-0xf7413fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e3:00.0: reg 0x30: [mem 0xf7400000-0xf740ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: PCI bridge to [bus e3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [mem 0xf7400000-0xf75fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [mem 0xbffffc00000-0xbffffdfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e4:00.0: [8086:0a54] type 00 class 0x010802
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e4:00.0: reg 0x10: [mem 0xf7210000-0xf7213fff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e4:00.0: reg 0x30: [mem 0xf7200000-0xf720ffff pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: PCI bridge to [bus e4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0:   bridge window [mem 0xf7200000-0xf73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0:   bridge window [mem 0xbffffa00000-0xbffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: PCI bridge to [bus e5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0:   bridge window [mem 0xf7000000-0xf71fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0:   bridge window [mem 0xbffff800000-0xbffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: PCI bridge to [bus e6]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0:   bridge window [mem 0xf6e00000-0xf6ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0:   bridge window [mem 0xbffff600000-0xbffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [UC16] (domain 0000 [bus fe])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0e: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0e: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0e: _OSC: platform does not support [SHPCHotplug LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0e: _OSC: OS now controls [PCIeHotplug PME PCIeCapability]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0e: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:fe
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:fe: root bus resource [bus fe]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:00.0: [8086:3450] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:00.1: [8086:3451] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:00.2: [8086:3452] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:00.3: [8086:0998] type 00 class 0x060000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:00.5: [8086:3455] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:02.0: [8086:3440] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:02.1: [8086:3441] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:02.2: [8086:3442] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:03.0: [8086:3440] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:03.1: [8086:3441] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:03.2: [8086:3442] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:04.0: [8086:3440] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:04.1: [8086:3441] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:04.2: [8086:3442] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:04.3: [8086:3443] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:05.0: [8086:3445] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:05.1: [8086:3446] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:05.2: [8086:3447] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:06.0: [8086:3445] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:06.1: [8086:3446] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:06.2: [8086:3447] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:07.0: [8086:3445] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:07.1: [8086:3446] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:07.2: [8086:3447] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0b.0: [8086:3448] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0b.1: [8086:3448] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0b.2: [8086:344b] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0c.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0d.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0e.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:0f.0: [8086:344a] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:1a.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:1b.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:1c.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:fe:1d.0: [8086:2880] type 00 class 0x110100
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:fe: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Root Bridge [UC17] (domain 0000 [bus ff])
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0f: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0f: PCIe AER handled by firmware
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0f: _OSC: platform does not support [SHPCHotplug LTR]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0f: _OSC: OS now controls [PCIeHotplug PME PCIeCapability]
Apr 02 11:09:27 node-9.domain.tld kernel: acpi PNP0A08:0f: FADT indicates ASPM is unsupported, using BIOS configuration
Apr 02 11:09:27 node-9.domain.tld kernel: PCI host bridge to bus 0000:ff
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:ff: root bus resource [bus ff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.0: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.1: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.2: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.3: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.4: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.5: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.6: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:00.7: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.0: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.1: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.2: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.3: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.4: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.5: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.6: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:01.7: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.0: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.1: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.2: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.3: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.4: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.5: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.6: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:02.7: [8086:344c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.0: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.1: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.2: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.3: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.4: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.5: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.6: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0a.7: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.0: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.1: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.2: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.3: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.4: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.5: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.6: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0b.7: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.0: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.1: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.2: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.3: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.4: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.5: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.6: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:0c.7: [8086:344d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1d.0: [8086:344f] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1d.1: [8086:3457] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.0: [8086:3458] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.1: [8086:3459] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.2: [8086:345a] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.3: [8086:345b] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.4: [8086:345c] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.5: [8086:345d] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.6: [8086:345e] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ff:1e.7: [8086:345f] type 00 class 0x088000
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:ff: on NUMA node 1
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKA] (IRQs 3 4 5 6 7 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKB] (IRQs 3 4 5 6 7 *10 11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKC] (IRQs 3 4 5 6 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKD] (IRQs 3 4 5 6 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKE] (IRQs 3 4 5 6 7 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKF] (IRQs 3 4 5 6 7 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKG] (IRQs 3 4 5 6 7 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: PCI Interrupt Link [LNKH] (IRQs 3 4 5 6 7 10 *11 12 14 15)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: vgaarb: VGA device added: decodes=io+mem,owns=none,locks=none
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: vgaarb: bridge control possible
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:02:00.0: vgaarb: setting as boot device
Apr 02 11:09:27 node-9.domain.tld kernel: vgaarb: loaded
Apr 02 11:09:27 node-9.domain.tld kernel: SCSI subsystem initialized
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: bus type USB registered
Apr 02 11:09:27 node-9.domain.tld kernel: usbcore: registered new interface driver usbfs
Apr 02 11:09:27 node-9.domain.tld kernel: usbcore: registered new interface driver hub
Apr 02 11:09:27 node-9.domain.tld kernel: usbcore: registered new device driver usb
Apr 02 11:09:27 node-9.domain.tld kernel: pps_core: LinuxPPS API ver. 1 registered
Apr 02 11:09:27 node-9.domain.tld kernel: pps_core: Software ver. 5.3.6 - Copyright 2005-2007 Rodolfo Giometti <giometti@linux.it>
Apr 02 11:09:27 node-9.domain.tld kernel: PTP clock support registered
Apr 02 11:09:27 node-9.domain.tld kernel: EDAC MC: Ver: 3.0.0
Apr 02 11:09:27 node-9.domain.tld kernel: Registered efivars operations
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: Using ACPI for IRQ routing
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: pci_cache_line_size set to 64 bytes
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x0003e000-0x0003ffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x99b25018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x99b5d018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x99b95018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x99bcc018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x99c03018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x99c73018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x9a365018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x9a455018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x9a6ff018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x9a7f0018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x9a814018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0x9a855018-0x9bffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0xa5c57000-0xa7ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: e820: reserve RAM buffer [mem 0xaf800000-0xafffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: NetLabel: Initializing
Apr 02 11:09:27 node-9.domain.tld kernel: NetLabel:  domain hash size = 128
Apr 02 11:09:27 node-9.domain.tld kernel: NetLabel:  protocols = UNLABELED CIPSOv4 CALIPSO
Apr 02 11:09:27 node-9.domain.tld kernel: NetLabel:  unlabeled traffic allowed by default
Apr 02 11:09:27 node-9.domain.tld kernel: hpet0: at MMIO 0xfed00000, IRQs 2, 8, 0, 0, 0, 0, 0, 0
Apr 02 11:09:27 node-9.domain.tld kernel: hpet0: 8 comparators, 64-bit 24.000000 MHz counter
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: Switched to clocksource tsc-early
Apr 02 11:09:27 node-9.domain.tld kernel: VFS: Disk quotas dquot_6.6.0
Apr 02 11:09:27 node-9.domain.tld kernel: VFS: Dquot-cache hash table entries: 512 (order 0, 4096 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: pnp: PnP ACPI init
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:00: Plug and Play ACPI device, IDs PNP0b00 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:01: disabling [mem 0xff000000-0xffffffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:01: disabling [mem 0xff000000-0xffffffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:01: disabling [mem 0xff000000-0xffffffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:01: disabling [mem 0xff000000-0xffffffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:01: [io  0x0500-0x05fe] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:01: [io  0x0400-0x041f] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:01: Plug and Play ACPI device, IDs PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:02: [io  0x0c80-0x0c8f] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:02: [io  0x0ca0-0x0cbf] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:02: [io  0x0c30-0x0c3f] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:02: Plug and Play ACPI device, IDs PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:03: [dma 0 disabled]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:03: Plug and Play ACPI device, IDs PNP0501 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:04: [dma 0 disabled]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:04: Plug and Play ACPI device, IDs PNP0501 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:05: [io  0x0cc0] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:05: [io  0x0cc1] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:05: Plug and Play ACPI device, IDs IPI0001 PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:06: [io  0x0c00-0x0c0f] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:06: Plug and Play ACPI device, IDs PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:07: [io  0x0cc2] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:07: [io  0x0cc3] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:07: Plug and Play ACPI device, IDs PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfd000000-0xfdabffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdad0000-0xfdadffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdb00000-0xfdffffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe000000-0xfe00ffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe011000-0xfe01ffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe036000-0xfe03bfff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe03d000-0xfe3fffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe410000-0xfe7fffff] because it overlaps 0000:b3:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfd000000-0xfdabffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdad0000-0xfdadffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdb00000-0xfdffffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe000000-0xfe00ffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe011000-0xfe01ffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe036000-0xfe03bfff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe03d000-0xfe3fffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe410000-0xfe7fffff disabled] because it overlaps 0000:b4:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfd000000-0xfdabffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdad0000-0xfdadffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdb00000-0xfdffffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe000000-0xfe00ffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe011000-0xfe01ffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe036000-0xfe03bfff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe03d000-0xfe3fffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe410000-0xfe7fffff disabled] because it overlaps 0000:cc:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfd000000-0xfdabffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdad0000-0xfdadffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfdb00000-0xfdffffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe000000-0xfe00ffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe011000-0xfe01ffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe036000-0xfe03bfff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe03d000-0xfe3fffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pnp 00:08: disabling [mem 0xfe410000-0xfe7fffff disabled] because it overlaps 0000:cd:00.0 BAR 1 [mem 0x00000000-0x3fffffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:08: Plug and Play ACPI device, IDs PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:09: [io  0x0f00-0x0ffe] has been reserved
Apr 02 11:09:27 node-9.domain.tld kernel: system 00:09: Plug and Play ACPI device, IDs PNP0c02 (active)
Apr 02 11:09:27 node-9.domain.tld kernel: pnp: PnP ACPI: found 10 devices
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: acpi_pm: mask: 0xffffff max_cycles: 0xffffff, max_idle_ns: 2085701024 ns
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: max bus depth: 2 pci_try_num: 3
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1f.4: BAR 0: assigned [mem 0x20000000000-0x200000000ff 64bit]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0: PCI bridge to [bus 02]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:01:00.0:   bridge window [mem 0xd2000000-0xd3afffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.0: PCI bridge to [bus 01-02]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.0:   bridge window [mem 0xd2000000-0xd3bfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6: PCI bridge to [bus 03]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6:   bridge window [io  0x2000-0x2fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:00:1c.6:   bridge window [mem 0xd3c00000-0xd3cfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 4 [io  0x0000-0x0cf7 window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 5 [io  0x1000-0x3fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 6 [mem 0x000a0000-0x000bffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 7 [mem 0x000c8000-0x000cffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 8 [mem 0xfe010000-0xfe010fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 9 [mem 0xd0000000-0xd3ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:00: resource 10 [mem 0x20000000000-0x2ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:01: resource 1 [mem 0xd2000000-0xd3bfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:02: resource 1 [mem 0xd2000000-0xd3afffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:03: resource 0 [io  0x2000-0x2fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:03: resource 1 [mem 0xd3c00000-0xd3cfffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: max bus depth: 3 pci_try_num: 4
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: BAR 13: assigned [io  0x4000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0: BAR 13: assigned [io  0x4000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0: BAR 13: assigned [io  0x4000-0x4fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0: BAR 13: assigned [io  0x5000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0: PCI bridge to [bus 19]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0:   bridge window [io  0x4000-0x4fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0:   bridge window [mem 0xd6000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:00.0:   bridge window [mem 0x3d000000000-0x3f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0: PCI bridge to [bus 1a]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0:   bridge window [io  0x5000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0:   bridge window [mem 0xd4000000-0xd53fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:01.0:   bridge window [mem 0x3a000000000-0x3c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:1f.0: PCI bridge to [bus 1b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:18:1f.0:   bridge window [mem 0x3f022100000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0: PCI bridge to [bus 18-1b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0:   bridge window [io  0x4000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0:   bridge window [mem 0xd4000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:17:00.0:   bridge window [mem 0x3a000000000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0: PCI bridge to [bus 17-1b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0:   bridge window [io  0x4000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0:   bridge window [mem 0xd4000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:16:02.0:   bridge window [mem 0x3a000000000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: resource 4 [io  0x4000-0x5fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: resource 5 [mem 0xd4000000-0xd7ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:16: resource 6 [mem 0x30000000000-0x3ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:17: resource 0 [io  0x4000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:17: resource 1 [mem 0xd4000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:17: resource 2 [mem 0x3a000000000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:18: resource 0 [io  0x4000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:18: resource 1 [mem 0xd4000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:18: resource 2 [mem 0x3a000000000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:19: resource 0 [io  0x4000-0x4fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:19: resource 1 [mem 0xd6000000-0xd73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:19: resource 2 [mem 0x3d000000000-0x3f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:1a: resource 0 [io  0x5000-0x5fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:1a: resource 1 [mem 0xd4000000-0xd53fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:1a: resource 2 [mem 0x3a000000000-0x3c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:1b: resource 2 [mem 0x3f022100000-0x3f0221fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: max bus depth: 3 pci_try_num: 4
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: BAR 13: assigned [io  0x6000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0: BAR 13: assigned [io  0x6000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0: BAR 13: assigned [io  0x6000-0x6fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0: BAR 13: assigned [io  0x7000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0: PCI bridge to [bus 33]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0:   bridge window [io  0x6000-0x6fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0:   bridge window [mem 0xda000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:00.0:   bridge window [mem 0x4d000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0: PCI bridge to [bus 34]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0:   bridge window [io  0x7000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0:   bridge window [mem 0xd8000000-0xd93fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:32:01.0:   bridge window [mem 0x4a000000000-0x4c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0: PCI bridge to [bus 32-34]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0:   bridge window [io  0x6000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0:   bridge window [mem 0xd8000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:31:00.0:   bridge window [mem 0x4a000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0: PCI bridge to [bus 31-34]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0:   bridge window [io  0x6000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0:   bridge window [mem 0xd8000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:30:02.0:   bridge window [mem 0x4a000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: resource 4 [io  0x6000-0x7fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: resource 5 [mem 0xd8000000-0xdbffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:30: resource 6 [mem 0x40000000000-0x4ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:31: resource 0 [io  0x6000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:31: resource 1 [mem 0xd8000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:31: resource 2 [mem 0x4a000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:32: resource 0 [io  0x6000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:32: resource 1 [mem 0xd8000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:32: resource 2 [mem 0x4a000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:33: resource 0 [io  0x6000-0x6fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:33: resource 1 [mem 0xda000000-0xdb3fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:33: resource 2 [mem 0x4d000000000-0x4f021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:34: resource 0 [io  0x7000-0x7fff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:34: resource 1 [mem 0xd8000000-0xd93fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:34: resource 2 [mem 0x4a000000000-0x4c021ffffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: max bus depth: 1 pci_try_num: 2
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0: PCI bridge to [bus 4b]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0:   bridge window [mem 0xdfd00000-0xdfefffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:4a:02.0:   bridge window [mem 0x5fff4000000-0x5ffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: resource 4 [io  0x8000-0x9fff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: resource 5 [mem 0xdc000000-0xdfffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4a: resource 6 [mem 0x50000000000-0x5ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4b: resource 1 [mem 0xdfd00000-0xdfefffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:4b: resource 2 [mem 0x5fff4000000-0x5ffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: max bus depth: 1 pci_try_num: 2
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: BAR 13: assigned [io  0xa000-0xafff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: PCI bridge to [bus 65]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [io  0xa000-0xafff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [mem 0xe3400000-0xe35fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [mem 0x6ffffa00000-0x6ffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: PCI bridge to [bus 66]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0:   bridge window [mem 0xe3200000-0xe33fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0:   bridge window [mem 0x6ffff800000-0x6ffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0: PCI bridge to [bus 67]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0:   bridge window [mem 0xe3600000-0xe36fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0:   bridge window [mem 0x6ffffc00000-0x6ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: No. 2 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0: PCI bridge to [bus 65]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [io  0xa000-0xafff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [mem 0xe3400000-0xe35fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:02.0:   bridge window [mem 0x6ffffa00000-0x6ffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0: PCI bridge to [bus 66]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0:   bridge window [mem 0xe3200000-0xe33fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:03.0:   bridge window [mem 0x6ffff800000-0x6ffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0: PCI bridge to [bus 67]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0:   bridge window [mem 0xe3600000-0xe36fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:64:04.0:   bridge window [mem 0x6ffffc00000-0x6ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: resource 4 [io  0xa000-0xafff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: resource 5 [mem 0xe0000000-0xe37fffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:64: resource 6 [mem 0x60000000000-0x6ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:65: resource 0 [io  0xa000-0xafff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:65: resource 1 [mem 0xe3400000-0xe35fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:65: resource 2 [mem 0x6ffffa00000-0x6ffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:66: resource 1 [mem 0xe3200000-0xe33fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:66: resource 2 [mem 0x6ffff800000-0x6ffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:67: resource 1 [mem 0xe3600000-0xe36fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:67: resource 2 [mem 0x6ffffc00000-0x6ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:7e: max bus depth: 0 pci_try_num: 1
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:7f: max bus depth: 0 pci_try_num: 1
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: max bus depth: 0 pci_try_num: 1
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: resource 4 [io  0xb000-0xbfff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: resource 5 [mem 0xe4000000-0xe7ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:80: resource 6 [mem 0x70000000000-0x7ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: max bus depth: 1 pci_try_num: 2
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0: PCI bridge to [bus 98]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0:   bridge window [mem 0xebe00000-0xebefffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:97:02.0:   bridge window [mem 0x8fffe800000-0x8ffff8fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: resource 4 [io  0xc000-0xcfff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: resource 5 [mem 0xe8000000-0xebffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:97: resource 6 [mem 0x80000000000-0x8ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:98: resource 1 [mem 0xebe00000-0xebefffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:98: resource 2 [mem 0x8fffe800000-0x8ffff8fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: max bus depth: 3 pci_try_num: 4
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: disabling BAR 1: [mem 0x00000000-0x3fffffffff 64bit pref] (bad alignment 0x4000000000)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: disabling BAR 1: [mem 0x00000000-0x3fffffffff 64bit pref] (bad alignment 0x4000000000)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: no space for [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: failed to assign [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: no space for [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: failed to assign [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: no space for [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: failed to assign [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: no space for [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: failed to assign [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: PCI bridge to [bus b3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: PCI bridge to [bus b4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PCI bridge to [bus b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PCI bridge to [bus b2-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PCI bridge to [bus b1-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: No. 2 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: no space for [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: failed to assign [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: no space for [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: failed to assign [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: no space for [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: failed to assign [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: no space for [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: failed to assign [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: PCI bridge to [bus b3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: PCI bridge to [bus b4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PCI bridge to [bus b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PCI bridge to [bus b2-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PCI bridge to [bus b1-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: No. 3 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: release child resource [mem 0x9ffffe00000-0x9ffffe03fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: resource 15 [mem 0x9ffffe00000-0x9ffffefffff 64bit pref] released
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PCI bridge to [bus b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: resource 15 [mem 0x9ffffe00000-0x9ffffefffff 64bit pref] released
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PCI bridge to [bus b2-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: no space for [mem size 0x08000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: failed to assign [mem size 0x08000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: no space for [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: failed to assign [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 15: no space for [mem size 0x00500000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 15: failed to assign [mem size 0x00500000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: no space for [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: failed to assign [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: no space for [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: failed to assign [mem size 0x00200000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: BAR 15: no space for [mem size 0x00100000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: BAR 15: failed to assign [mem size 0x00100000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: PCI bridge to [bus b3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: PCI bridge to [bus b4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b5:00.0: BAR 0: no space for [mem size 0x00004000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b5:00.0: BAR 0: failed to assign [mem size 0x00004000 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PCI bridge to [bus b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PCI bridge to [bus b2-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PCI bridge to [bus b1-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0:   bridge window [mem 0x9ffffe00000-0x9ffffefffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: No. 4 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: resource 15 [mem 0x9ffffe00000-0x9ffffefffff 64bit pref] released
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PCI bridge to [bus b1-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: no space for [mem size 0x08000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 14: failed to assign [mem size 0x08000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 15: assigned [mem 0x90000000000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: no space for [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 14: failed to assign [mem size 0x06000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 15: assigned [mem 0x90000000000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 15: assigned [mem 0x90000000000-0x900001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 15: assigned [mem 0x90000200000-0x900003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: BAR 15: assigned [mem 0x90000400000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b3:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0: PCI bridge to [bus b3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:00.0:   bridge window [mem 0x90000000000-0x900001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b4:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0: PCI bridge to [bus b4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:01.0:   bridge window [mem 0x90000200000-0x900003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b5:00.0: BAR 0: assigned [mem 0x90000400000-0x90000403fff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0: PCI bridge to [bus b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b2:1f.0:   bridge window [mem 0x90000400000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0: PCI bridge to [bus b2-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b1:00.0:   bridge window [mem 0x90000000000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0: PCI bridge to [bus b1-b5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:b0:02.0:   bridge window [mem 0x90000000000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: resource 4 [io  0xd000-0xdfff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: resource 5 [mem 0xec000000-0xefffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b0: resource 6 [mem 0x90000000000-0x9ffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b1: resource 2 [mem 0x90000000000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b2: resource 2 [mem 0x90000000000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b3: resource 2 [mem 0x90000000000-0x900001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b4: resource 2 [mem 0x90000200000-0x900003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:b5: resource 2 [mem 0x90000400000-0x900004fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: max bus depth: 3 pci_try_num: 4
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: disabling BAR 1: [mem 0x00000000-0x3fffffffff 64bit pref] (bad alignment 0x4000000000)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: disabling BAR 1: [mem 0x00000000-0x3fffffffff 64bit pref] (bad alignment 0x4000000000)
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 15: assigned [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 15: assigned [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 15: assigned [mem 0xa0000000000-0xa00001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 15: assigned [mem 0xa0000200000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: PCI bridge to [bus cc]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0:   bridge window [mem 0xa0000000000-0xa00001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: PCI bridge to [bus cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0:   bridge window [mem 0xa0000200000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: PCI bridge to [bus cb-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: PCI bridge to [bus ca-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: No. 2 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: PCI bridge to [bus cc]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0:   bridge window [mem 0xa0000000000-0xa00001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: PCI bridge to [bus cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0:   bridge window [mem 0xa0000200000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: PCI bridge to [bus cb-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: PCI bridge to [bus ca-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: No. 3 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: PCI bridge to [bus cc]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0:   bridge window [mem 0xa0000000000-0xa00001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: PCI bridge to [bus cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0:   bridge window [mem 0xa0000200000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: PCI bridge to [bus cb-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: PCI bridge to [bus ca-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: No. 4 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: no space for [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 14: failed to assign [mem size 0x04000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: no space for [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: BAR 13: failed to assign [io  size 0x2000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 14: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cc:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0: PCI bridge to [bus cc]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:00.0:   bridge window [mem 0xa0000000000-0xa00001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: no space for [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cd:00.0: BAR 0: failed to assign [mem size 0x02000000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0: PCI bridge to [bus cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:cb:01.0:   bridge window [mem 0xa0000200000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0: PCI bridge to [bus cb-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:ca:00.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0: PCI bridge to [bus ca-cd]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:c9:02.0:   bridge window [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: resource 4 [io  0xe000-0xefff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: resource 5 [mem 0xf0000000-0xf3ffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:c9: resource 6 [mem 0xa0000000000-0xaffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:ca: resource 2 [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:cb: resource 2 [mem 0xa0000000000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:cc: resource 2 [mem 0xa0000000000-0xa00001fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:cd: resource 2 [mem 0xa0000200000-0xa00003fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: max bus depth: 1 pci_try_num: 2
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: BAR 13: assigned [io  0xf000-0xffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: PCI bridge to [bus e3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [io  0xf000-0xffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [mem 0xf7400000-0xf75fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [mem 0xbffffc00000-0xbffffdfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: PCI bridge to [bus e4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0:   bridge window [mem 0xf7200000-0xf73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0:   bridge window [mem 0xbffffa00000-0xbffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: PCI bridge to [bus e5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0:   bridge window [mem 0xf7000000-0xf71fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0:   bridge window [mem 0xbffff800000-0xbffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: PCI bridge to [bus e6]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0:   bridge window [mem 0xf6e00000-0xf6ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0:   bridge window [mem 0xbffff600000-0xbffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: No. 2 try to assign unassigned res
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: BAR 13: no space for [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: BAR 13: failed to assign [io  size 0x1000]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0: PCI bridge to [bus e3]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [io  0xf000-0xffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [mem 0xf7400000-0xf75fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:02.0:   bridge window [mem 0xbffffc00000-0xbffffdfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0: PCI bridge to [bus e4]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0:   bridge window [mem 0xf7200000-0xf73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:03.0:   bridge window [mem 0xbffffa00000-0xbffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0: PCI bridge to [bus e5]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0:   bridge window [mem 0xf7000000-0xf71fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:04.0:   bridge window [mem 0xbffff800000-0xbffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0: PCI bridge to [bus e6]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0:   bridge window [mem 0xf6e00000-0xf6ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci 0000:e2:05.0:   bridge window [mem 0xbffff600000-0xbffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: resource 4 [io  0xf000-0xffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: resource 5 [mem 0xf4000000-0xf77fffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e2: resource 6 [mem 0xb0000000000-0xbffffffffff window]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e3: resource 0 [io  0xf000-0xffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e3: resource 1 [mem 0xf7400000-0xf75fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e3: resource 2 [mem 0xbffffc00000-0xbffffdfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e4: resource 1 [mem 0xf7200000-0xf73fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e4: resource 2 [mem 0xbffffa00000-0xbffffbfffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e5: resource 1 [mem 0xf7000000-0xf71fffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e5: resource 2 [mem 0xbffff800000-0xbffff9fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e6: resource 1 [mem 0xf6e00000-0xf6ffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:e6: resource 2 [mem 0xbffff600000-0xbffff7fffff 64bit pref]
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:fe: max bus depth: 0 pci_try_num: 1
Apr 02 11:09:27 node-9.domain.tld kernel: pci_bus 0000:ff: max bus depth: 0 pci_try_num: 1
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 2
Apr 02 11:09:27 node-9.domain.tld kernel: tcp_listen_portaddr_hash hash table entries: 65536 (order: 8, 1048576 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: TCP established hash table entries: 524288 (order: 10, 4194304 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: TCP bind hash table entries: 65536 (order: 8, 1048576 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: TCP: Hash tables configured (established 524288 bind 65536)
Apr 02 11:09:27 node-9.domain.tld kernel: UDP hash table entries: 65536 (order: 9, 2097152 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: UDP-Lite hash table entries: 65536 (order: 9, 2097152 bytes)
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 1
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 44
Apr 02 11:09:27 node-9.domain.tld kernel: PCI: CLS mismatch (64 != 32), using 64 bytes
Apr 02 11:09:27 node-9.domain.tld kernel: Unpacking initramfs...
Apr 02 11:09:27 node-9.domain.tld kernel: Freeing initrd memory: 124200K
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar8: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar7: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar6: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar5: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar4: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar3: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar2: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar1: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar0: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: dmar9: Using Queued invalidation
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:01.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:02.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:02.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:11.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:11.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:14.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:14.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:16.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:16.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:16.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:17.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:1c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:1c.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:1f.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:1f.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:1f.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:00:1f.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:03:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:16:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:16:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:16:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:16:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:16:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:17:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:18:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:18:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:18:1f.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:19:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:1a:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:1b:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:30:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:30:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:30:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:30:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:30:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:31:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:32:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:32:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:33:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:34:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4a:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4a:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4a:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4a:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4a:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4b:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:4b:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:03.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:64:04.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:65:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:66:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:67:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:67:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:00.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:00.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:02.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:02.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:03.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:03.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:03.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:04.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:04.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:04.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:04.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:05.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:05.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:05.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:06.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:06.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:06.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:07.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:07.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:07.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0b.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0b.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0b.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0d.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0e.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:0f.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:1a.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:1b.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:1c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7e:1d.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:00.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:01.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:02.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0a.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0b.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:0c.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1d.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1d.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:7f:1e.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:01.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:02.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:80:02.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:97:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:97:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:97:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:97:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:97:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:98:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:98:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b0:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b0:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b0:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b0:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b0:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b1:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b2:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b2:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b2:1f.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b3:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b4:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:b5:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:c9:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:c9:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:c9:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:c9:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:c9:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ca:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:cb:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:cb:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:cc:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:cd:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:03.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:04.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e2:05.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e3:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:e4:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:00.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:00.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:02.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:02.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:03.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:03.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:03.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:04.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:04.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:04.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:04.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:05.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:05.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:05.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:06.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:06.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:06.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:07.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:07.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:07.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0b.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0b.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0b.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0d.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0e.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:0f.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:1a.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:1b.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:1c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:fe:1d.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:00.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:01.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:02.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0a.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0b.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:0c.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1d.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1d.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.0
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.1
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.2
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.3
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.4
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.5
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.6
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Hardware identity mapping for device 0000:ff:1e.7
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Setting RMRR:
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Ignoring identity map for HW passthrough device 0000:00:14.0 [0xaa7e2000 - 0xaaa2bfff]
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Ignoring identity map for HW passthrough device 0000:00:14.0 [0xaaac6000 - 0xaaae9fff]
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Prepare 0-16MiB unity mapping for LPC
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Ignoring identity map for HW passthrough device 0000:00:1f.0 [0x0 - 0xffffff]
Apr 02 11:09:27 node-9.domain.tld kernel: DMAR: Intel(R) Virtualization Technology for Directed I/O
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:00.0 to group 0
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:00.1 to group 0
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:00.2 to group 0
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:00.4 to group 0
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.0 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.1 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.2 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.3 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.4 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.5 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.6 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:01.7 to group 1
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:02.0 to group 2
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:02.1 to group 2
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:02.4 to group 2
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:11.0 to group 3
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:11.5 to group 3
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:14.0 to group 4
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:14.2 to group 4
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:16.0 to group 5
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:16.1 to group 5
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:16.4 to group 5
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:17.0 to group 6
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:1c.0 to group 7
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:1c.6 to group 8
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:1f.0 to group 9
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:1f.2 to group 9
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:1f.4 to group 9
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:00:1f.5 to group 9
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:01:00.0 to group 10
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:02:00.0 to group 10
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:03:00.0 to group 11
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:16:00.0 to group 12
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:16:00.1 to group 12
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:16:00.2 to group 12
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:16:00.4 to group 12
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:16:02.0 to group 13
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:17:00.0 to group 14
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:18:00.0 to group 15
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:18:01.0 to group 16
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:18:1f.0 to group 17
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:19:00.0 to group 18
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:1a:00.0 to group 19
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:1b:00.0 to group 20
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:30:00.0 to group 21
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:30:00.1 to group 21
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:30:00.2 to group 21
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:30:00.4 to group 21
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:30:02.0 to group 22
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:31:00.0 to group 23
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:32:00.0 to group 24
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:32:01.0 to group 25
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:33:00.0 to group 26
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:34:00.0 to group 27
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4a:00.0 to group 28
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4a:00.1 to group 28
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4a:00.2 to group 28
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4a:00.4 to group 28
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4a:02.0 to group 29
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4b:00.0 to group 30
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:4b:00.1 to group 31
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:00.0 to group 32
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:00.1 to group 32
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:00.2 to group 32
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:00.4 to group 32
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:02.0 to group 33
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:03.0 to group 34
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:64:04.0 to group 35
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:65:00.0 to group 36
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:66:00.0 to group 37
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:67:00.0 to group 38
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:67:00.1 to group 38
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:00.0 to group 39
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:00.1 to group 39
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:00.2 to group 39
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:00.3 to group 39
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:00.5 to group 39
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:02.0 to group 40
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:02.1 to group 40
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:02.2 to group 40
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:03.0 to group 41
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:03.1 to group 41
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:03.2 to group 41
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:04.0 to group 42
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:04.1 to group 42
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:04.2 to group 42
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:04.3 to group 42
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:05.0 to group 43
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:05.1 to group 43
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:05.2 to group 43
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:06.0 to group 44
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:06.1 to group 44
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:06.2 to group 44
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:07.0 to group 45
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:07.1 to group 45
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:07.2 to group 45
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0b.0 to group 46
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0b.1 to group 46
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0b.2 to group 46
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0c.0 to group 47
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0d.0 to group 48
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0e.0 to group 49
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:0f.0 to group 50
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:1a.0 to group 51
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:1b.0 to group 52
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:1c.0 to group 53
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7e:1d.0 to group 54
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.0 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.1 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.2 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.3 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.4 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.5 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.6 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:00.7 to group 55
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.0 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.1 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.2 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.3 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.4 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.5 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.6 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:01.7 to group 56
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.0 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.1 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.2 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.3 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.4 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.5 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.6 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:02.7 to group 57
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.0 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.1 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.2 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.3 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.4 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.5 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.6 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0a.7 to group 58
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.0 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.1 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.2 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.3 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.4 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.5 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.6 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0b.7 to group 59
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.0 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.1 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.2 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.3 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.4 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.5 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.6 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:0c.7 to group 60
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1d.0 to group 61
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1d.1 to group 61
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.0 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.1 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.2 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.3 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.4 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.5 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.6 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:7f:1e.7 to group 62
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:00.0 to group 63
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:00.1 to group 63
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:00.2 to group 63
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:00.4 to group 63
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.0 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.1 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.2 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.3 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.4 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.5 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.6 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:01.7 to group 64
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:02.0 to group 65
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:02.1 to group 65
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:80:02.4 to group 65
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:97:00.0 to group 66
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:97:00.1 to group 66
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:97:00.2 to group 66
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:97:00.4 to group 66
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:97:02.0 to group 67
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:98:00.0 to group 68
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:98:00.1 to group 69
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b0:00.0 to group 70
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b0:00.1 to group 70
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b0:00.2 to group 70
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b0:00.4 to group 70
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b0:02.0 to group 71
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b1:00.0 to group 72
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b2:00.0 to group 73
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b2:01.0 to group 74
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b2:1f.0 to group 75
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b3:00.0 to group 76
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b4:00.0 to group 77
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:b5:00.0 to group 78
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:c9:00.0 to group 79
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:c9:00.1 to group 79
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:c9:00.2 to group 79
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:c9:00.4 to group 79
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:c9:02.0 to group 80
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ca:00.0 to group 81
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:cb:00.0 to group 82
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:cb:01.0 to group 83
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:cc:00.0 to group 84
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:cd:00.0 to group 85
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:00.0 to group 86
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:00.1 to group 86
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:00.2 to group 86
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:00.4 to group 86
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:02.0 to group 87
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:03.0 to group 88
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:04.0 to group 89
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e2:05.0 to group 90
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e3:00.0 to group 91
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:e4:00.0 to group 92
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:00.0 to group 93
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:00.1 to group 93
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:00.2 to group 93
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:00.3 to group 93
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:00.5 to group 93
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:02.0 to group 94
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:02.1 to group 94
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:02.2 to group 94
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:03.0 to group 95
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:03.1 to group 95
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:03.2 to group 95
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:04.0 to group 96
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:04.1 to group 96
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:04.2 to group 96
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:04.3 to group 96
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:05.0 to group 97
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:05.1 to group 97
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:05.2 to group 97
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:06.0 to group 98
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:06.1 to group 98
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:06.2 to group 98
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:07.0 to group 99
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:07.1 to group 99
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:07.2 to group 99
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0b.0 to group 100
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0b.1 to group 100
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0b.2 to group 100
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0c.0 to group 101
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0d.0 to group 102
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0e.0 to group 103
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:0f.0 to group 104
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:1a.0 to group 105
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:1b.0 to group 106
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:1c.0 to group 107
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:fe:1d.0 to group 108
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.0 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.1 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.2 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.3 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.4 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.5 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.6 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:00.7 to group 109
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.0 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.1 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.2 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.3 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.4 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.5 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.6 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:01.7 to group 110
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.0 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.1 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.2 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.3 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.4 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.5 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.6 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:02.7 to group 111
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.0 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.1 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.2 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.3 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.4 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.5 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.6 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0a.7 to group 112
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.0 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.1 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.2 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.3 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.4 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.5 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.6 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0b.7 to group 113
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.0 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.1 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.2 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.3 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.4 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.5 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.6 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:0c.7 to group 114
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1d.0 to group 115
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1d.1 to group 115
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.0 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.1 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.2 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.3 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.4 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.5 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.6 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: iommu: Adding device 0000:ff:1e.7 to group 116
Apr 02 11:09:27 node-9.domain.tld kernel: Initialise system trusted keyrings
Apr 02 11:09:27 node-9.domain.tld kernel: Key type blacklist registered
Apr 02 11:09:27 node-9.domain.tld kernel: workingset: timestamp_bits=36 max_order=28 bucket_order=0
Apr 02 11:09:27 node-9.domain.tld kernel: zbud: loaded
Apr 02 11:09:27 node-9.domain.tld kernel: pstore: using deflate compression
Apr 02 11:09:27 node-9.domain.tld kernel: Platform Keyring initialized
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 38
Apr 02 11:09:27 node-9.domain.tld kernel: Key type asymmetric registered
Apr 02 11:09:27 node-9.domain.tld kernel: Asymmetric key parser 'x509' registered
Apr 02 11:09:27 node-9.domain.tld kernel: Block layer SCSI generic (bsg) driver version 0.4 loaded (major 245)
Apr 02 11:09:27 node-9.domain.tld kernel: io scheduler mq-deadline registered
Apr 02 11:09:27 node-9.domain.tld kernel: io scheduler kyber registered
Apr 02 11:09:27 node-9.domain.tld kernel: io scheduler bfq registered
Apr 02 11:09:27 node-9.domain.tld kernel: atomic64_test: passed for x86-64 platform with CX8 and with SSE
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:00:1c.0: Signaling PME with IRQ 130
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:00:1c.6: Signaling PME with IRQ 131
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:16:02.0: Signaling PME with IRQ 132
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:18:00.0:pcie204: Slot #5 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:18:01.0:pcie204: Slot #6 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:30:02.0: Signaling PME with IRQ 135
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:32:00.0:pcie204: Slot #4 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:32:01.0:pcie204: Slot #3 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:4a:02.0: Signaling PME with IRQ 138
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:64:02.0: Signaling PME with IRQ 139
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:64:02.0:pcie004: Slot #65 AttnBtn- PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl- LLActRep+ (with Cmd Compl erratum)
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:64:03.0: Signaling PME with IRQ 140
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:64:03.0:pcie004: Slot #64 AttnBtn- PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl- LLActRep+ (with Cmd Compl erratum)
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:64:04.0: Signaling PME with IRQ 141
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:97:02.0: Signaling PME with IRQ 142
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:b0:02.0: Signaling PME with IRQ 143
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:b2:00.0:pcie204: Slot #9 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:b2:01.0:pcie204: Slot #10 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:c9:02.0: Signaling PME with IRQ 146
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:cb:00.0:pcie204: Slot #8 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:cb:01.0:pcie204: Slot #7 AttnBtn+ PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl+ LLActRep+
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:e2:02.0: Signaling PME with IRQ 149
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:e2:02.0:pcie004: Slot #66 AttnBtn- PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl- LLActRep+ (with Cmd Compl erratum)
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:e2:03.0: Signaling PME with IRQ 150
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:e2:03.0:pcie004: Slot #67 AttnBtn- PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl- LLActRep+ (with Cmd Compl erratum)
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:e2:04.0: Signaling PME with IRQ 151
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:e2:04.0:pcie004: Slot #68 AttnBtn- PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl- LLActRep+ (with Cmd Compl erratum)
Apr 02 11:09:27 node-9.domain.tld kernel: pcieport 0000:e2:05.0: Signaling PME with IRQ 152
Apr 02 11:09:27 node-9.domain.tld kernel: pciehp 0000:e2:05.0:pcie004: Slot #69 AttnBtn- PwrCtrl- MRL- AttnInd+ PwrInd+ HotPlug+ Surprise+ Interlock- NoCompl- LLActRep+ (with Cmd Compl erratum)
Apr 02 11:09:27 node-9.domain.tld kernel: shpchp: Standard Hot Plug PCI Controller Driver version: 0.4
Apr 02 11:09:27 node-9.domain.tld kernel: efifb: probing for efifb
Apr 02 11:09:27 node-9.domain.tld kernel: efifb: framebuffer at 0xd2000000, using 3072k, total 3072k
Apr 02 11:09:27 node-9.domain.tld kernel: efifb: mode is 1024x768x32, linelength=4096, pages=1
Apr 02 11:09:27 node-9.domain.tld kernel: efifb: scrolling: redraw
Apr 02 11:09:27 node-9.domain.tld kernel: efifb: Truecolor: size=8:8:8:8, shift=24:16:8:0
Apr 02 11:09:27 node-9.domain.tld kernel: Console: switching to colour frame buffer device 128x48
Apr 02 11:09:27 node-9.domain.tld kernel: fb0: EFI VGA frame buffer device
Apr 02 11:09:27 node-9.domain.tld kernel: intel_idle: disabled
Apr 02 11:09:27 node-9.domain.tld kernel: input: Power Button as /devices/LNXSYSTM:00/LNXPWRBN:00/input/input0
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: Power Button [PWRF]
Apr 02 11:09:27 node-9.domain.tld kernel: ACPI: ACPI: processor limited to max C-state 0
Apr 02 11:09:27 node-9.domain.tld kernel: ERST: Error Record Serialization Table (ERST) support is initialized.
Apr 02 11:09:27 node-9.domain.tld kernel: pstore: Registered erst as persistent store backend
Apr 02 11:09:27 node-9.domain.tld kernel: GHES: APEI firmware first mode is enabled by APEI bit and WHEA _OSC.
Apr 02 11:09:27 node-9.domain.tld kernel: Serial: 8250/16550 driver, 4 ports, IRQ sharing enabled
Apr 02 11:09:27 node-9.domain.tld kernel: 00:03: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A
Apr 02 11:09:27 node-9.domain.tld kernel: 00:04: ttyS1 at I/O 0x2f8 (irq = 3, base_baud = 115200) is a 16550A
Apr 02 11:09:27 node-9.domain.tld kernel: Non-volatile memory driver v1.3
Apr 02 11:09:27 node-9.domain.tld kernel: rdac: device handler registered
Apr 02 11:09:27 node-9.domain.tld kernel: hp_sw: device handler registered
Apr 02 11:09:27 node-9.domain.tld kernel: emc: device handler registered
Apr 02 11:09:27 node-9.domain.tld kernel: alua: device handler registered
Apr 02 11:09:27 node-9.domain.tld kernel: libphy: Fixed MDIO Bus: probed
Apr 02 11:09:27 node-9.domain.tld kernel: ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver
Apr 02 11:09:27 node-9.domain.tld kernel: ehci-pci: EHCI PCI platform driver
Apr 02 11:09:27 node-9.domain.tld kernel: ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver
Apr 02 11:09:27 node-9.domain.tld kernel: ohci-pci: OHCI PCI platform driver
Apr 02 11:09:27 node-9.domain.tld kernel: uhci_hcd: USB Universal Host Controller Interface driver
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: xHCI Host Controller
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: new USB bus registered, assigned bus number 1
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: hcc params 0x200077c1 hci version 0x100 quirks 0x0000000000009810
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: cache line size of 64 is not supported
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb1: New USB device found, idVendor=1d6b, idProduct=0002, bcdDevice= 4.18
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb1: New USB device strings: Mfr=3, Product=2, SerialNumber=1
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb1: Product: xHCI Host Controller
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb1: Manufacturer: Linux 4.18.0-147.5.1.es8_24.x86_64 xhci-hcd
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb1: SerialNumber: 0000:00:14.0
Apr 02 11:09:27 node-9.domain.tld kernel: hub 1-0:1.0: USB hub found
Apr 02 11:09:27 node-9.domain.tld kernel: hub 1-0:1.0: 16 ports detected
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: xHCI Host Controller
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: new USB bus registered, assigned bus number 2
Apr 02 11:09:27 node-9.domain.tld kernel: xhci_hcd 0000:00:14.0: Host supports USB 3.0  SuperSpeed
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb2: New USB device found, idVendor=1d6b, idProduct=0003, bcdDevice= 4.18
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb2: New USB device strings: Mfr=3, Product=2, SerialNumber=1
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb2: Product: xHCI Host Controller
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb2: Manufacturer: Linux 4.18.0-147.5.1.es8_24.x86_64 xhci-hcd
Apr 02 11:09:27 node-9.domain.tld kernel: usb usb2: SerialNumber: 0000:00:14.0
Apr 02 11:09:27 node-9.domain.tld kernel: hub 2-0:1.0: USB hub found
Apr 02 11:09:27 node-9.domain.tld kernel: hub 2-0:1.0: 10 ports detected
Apr 02 11:09:27 node-9.domain.tld kernel: usb: port power management may be unreliable
Apr 02 11:09:27 node-9.domain.tld kernel: usbcore: registered new interface driver usbserial_generic
Apr 02 11:09:27 node-9.domain.tld kernel: usbserial: USB Serial support registered for generic
Apr 02 11:09:27 node-9.domain.tld kernel: i8042: PNP: No PS/2 controller found.
Apr 02 11:09:27 node-9.domain.tld kernel: mousedev: PS/2 mouse device common for all mice
Apr 02 11:09:27 node-9.domain.tld kernel: rtc_cmos 00:00: RTC can wake from S4
Apr 02 11:09:27 node-9.domain.tld kernel: rtc_cmos 00:00: registered as rtc0
Apr 02 11:09:27 node-9.domain.tld kernel: rtc_cmos 00:00: alarms up to one month, y3k, 114 bytes nvram, hpet irqs
Apr 02 11:09:27 node-9.domain.tld kernel: hidraw: raw HID events driver (C) Jiri Kosina
Apr 02 11:09:27 node-9.domain.tld kernel: usbcore: registered new interface driver usbhid
Apr 02 11:09:27 node-9.domain.tld kernel: usbhid: USB HID core driver
Apr 02 11:09:27 node-9.domain.tld kernel: drop_monitor: Initializing network drop monitor service
Apr 02 11:09:27 node-9.domain.tld kernel: Initializing XFRM netlink socket
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 10
Apr 02 11:09:27 node-9.domain.tld kernel: Segment Routing with IPv6
Apr 02 11:09:27 node-9.domain.tld kernel: NET: Registered protocol family 17
Apr 02 11:09:27 node-9.domain.tld kernel: mpls_gso: MPLS GSO support
Apr 02 11:09:27 node-9.domain.tld kernel: core: Using 28 MCE banks
Apr 02 11:09:27 node-9.domain.tld kernel: microcode: sig=0x606a6, pf=0x1, revision=0xd0003a5
Apr 02 11:09:27 node-9.domain.tld kernel: microcode: Microcode Update Driver: v2.2.
Apr 02 11:09:27 node-9.domain.tld kernel: resctrl: L3 allocation detected
Apr 02 11:09:27 node-9.domain.tld kernel: resctrl: MB allocation detected
Apr 02 11:09:27 node-9.domain.tld kernel: resctrl: L3 monitoring detected
Apr 02 11:09:27 node-9.domain.tld kernel: AVX2 version of gcm_enc/dec engaged.
Apr 02 11:09:27 node-9.domain.tld kernel: AES CTR mode by8 optimization enabled
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1: new high-speed USB device number 2 using xhci_hcd
Apr 02 11:09:27 node-9.domain.tld kernel: sched_clock: Marking stable (6152459750, 0)->(6863441651, -710981901)
Apr 02 11:09:27 node-9.domain.tld kernel: registered taskstats version 1
Apr 02 11:09:27 node-9.domain.tld kernel: Loading compiled-in X.509 certificates
Apr 02 11:09:27 node-9.domain.tld kernel: Loaded X.509 cert 'Red Hat Enterprise Linux kernel signing key: 28ef44921b3b488857f585030baae2f974a8aa01'
Apr 02 11:09:27 node-9.domain.tld kernel: Loaded X.509 cert 'Red Hat Enterprise Linux Driver Update Program (key 3): bf57f3e87362bc7229d9f465321773dfd1f77a80'
Apr 02 11:09:27 node-9.domain.tld kernel: Loaded X.509 cert 'Red Hat Enterprise Linux kpatch signing key: 4d38fd864ebe18c5f0b72e3852e2014c3a676fc8'
Apr 02 11:09:27 node-9.domain.tld kernel: zswap: loaded using pool lzo/zbud
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1: New USB device found, idVendor=2a4b, idProduct=0400, bcdDevice= 1.00
Apr 02 11:09:27 node-9.domain.tld kernel: Key type big_key registered
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1: New USB device strings: Mfr=1, Product=2, SerialNumber=3
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1: Product: Emulex Pilot4 HighSpeed HUB
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1: Manufacturer: Emulex Communications
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1: SerialNumber: 0xBABEFACE
Apr 02 11:09:27 node-9.domain.tld kernel: hub 1-1:1.0: USB hub found
Apr 02 11:09:27 node-9.domain.tld kernel: Key type encrypted registered
Apr 02 11:09:27 node-9.domain.tld kernel: hub 1-1:1.0: 7 ports detected
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'Nutanix Secure Boot Key: 91c4039b914cc481fc369b3ec07f9549a2cdaa4a'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'Intel Corporation: Intel(R) Power Thermal Utility Option ROM: fc49402b5fdcdea10ae183113f4ccd61844f1487'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'VMware, Inc.: VMware Secure Boot Signing: 04597f3e1ffb240bba0ff0f05d5eb05f3e15f6d7'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'VMware, Inc.: 4ad8ba0472073d28127706ddc6ccb9050441bbc7'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'SUSE Linux Enterprise Secure Boot Signkey: 3fb077b6cebc6ff2522e1c148c57c777c788e3e7'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'Lenovo Supplier Executable CA 2017: 6c63e1131bfe251c0af6db325d2a4daed41df5da'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'Lenovo UEFI DB: 565d7cfa86b3d0456858da2f497a0305add1447f'
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'Microsoft Corporation UEFI CA 2011: 13adbf4309bd82709c8cd54f316ed522988a1bd4'
Apr 02 11:09:27 node-9.domain.tld kernel: tsc: Refined TSC clocksource calibration: 3092.626 MHz
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:db
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'Microsoft Windows Production PCA 2011: a92902398e16c49778cd90f99e4f9ae17c55af53'
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x2c9412453fe, max_idle_ns: 440795364979 ns
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loading X.509 certificate: UEFI:MokListRT
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1.6: new high-speed USB device number 3 using xhci_hcd
Apr 02 11:09:27 node-9.domain.tld kernel: clocksource: Switched to clocksource tsc
Apr 02 11:09:27 node-9.domain.tld kernel: integrity: Loaded X.509 cert 'CentOS Secure Boot CA 2: 70007f99209c126be14774eaec7b6d9631f34dca'
Apr 02 11:09:27 node-9.domain.tld kernel: ima: No TPM chip found, activating TPM-bypass!
Apr 02 11:09:27 node-9.domain.tld kernel: ima: Allocated hash algorithm: sha1
Apr 02 11:09:27 node-9.domain.tld kernel: evm: Initialising EVM extended attributes:
Apr 02 11:09:27 node-9.domain.tld kernel: evm: security.selinux
Apr 02 11:09:27 node-9.domain.tld kernel: evm: security.ima
Apr 02 11:09:27 node-9.domain.tld kernel: evm: security.capability
Apr 02 11:09:27 node-9.domain.tld kernel: evm: HMAC attrs: 0x1
Apr 02 11:09:27 node-9.domain.tld kernel: rtc_cmos 00:00: setting system clock to 2024-04-02 03:09:26 UTC (1712027366)
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1.6: New USB device found, idVendor=04b3, idProduct=4010, bcdDevice= 4.14
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1.6: New USB device strings: Mfr=1, Product=2, SerialNumber=0
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1.6: Product: XClarity Controller
Apr 02 11:09:27 node-9.domain.tld kernel: usb 1-1.6: Manufacturer: IBM
Apr 02 11:09:27 node-9.domain.tld kernel: Freeing unused decrypted memory: 2040K
Apr 02 11:09:27 node-9.domain.tld kernel: Freeing unused kernel memory: 2364K
Apr 02 11:09:27 node-9.domain.tld kernel: Write protecting the kernel read-only data: 18432k
Apr 02 11:09:27 node-9.domain.tld kernel: Freeing unused kernel memory: 2020K
Apr 02 11:09:27 node-9.domain.tld kernel: Freeing unused kernel memory: 316K
Apr 02 11:09:27 node-9.domain.tld systemd[1]: systemd 239 (239-45.es8_5) running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=legacy)
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Detected architecture x86-64.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Running in initial RAM disk.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Set hostname to <node-9.domain.tld>.
Apr 02 11:09:27 node-9.domain.tld kernel: random: systemd: uninitialized urandom read (16 bytes read)
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Listening on Journal Socket.
Apr 02 11:09:27 node-9.domain.tld kernel: random: systemd: uninitialized urandom read (16 bytes read)
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting Create list of required static device nodes for the current kernel...
Apr 02 11:09:27 node-9.domain.tld kernel: random: systemd: uninitialized urandom read (16 bytes read)
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting Setup Virtual Console...
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Listening on udev Kernel Socket.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Reached target Timers.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Reached target Slices.
Apr 02 11:09:27 node-9.domain.tld kernel: IPMI message handler: version 39.2
Apr 02 11:09:27 node-9.domain.tld kernel: ipmi device interface
Apr 02 11:09:27 node-9.domain.tld kernel: IPMI Watchdog: driver initialized
Apr 02 11:09:27 node-9.domain.tld kernel: bridge: filtering via arp/ip/ip6tables is no longer available by default. Update your scripts to load br_netfilter if you need this.
Apr 02 11:09:27 node-9.domain.tld kernel: Bridge firewalling registered
Apr 02 11:09:27 node-9.domain.tld systemd-journald[668]: Journal started
-- Subject: The journal has been started
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The system journal process has started up, opened the journal
-- files for writing and is now ready to process requests.
Apr 02 11:09:27 node-9.domain.tld systemd-journald[668]: Runtime journal (/run/log/journal/22da9d3cc0234fd4ba2ea46a77ca71eb) is 8.0M, max 4.0G, 3.9G free.
-- Subject: Disk space used by the journal
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Runtime journal (/run/log/journal/22da9d3cc0234fd4ba2ea46a77ca71eb) is currently using 8.0M.
-- Maximum allowed usage is set to 4.0G.
-- Leaving at least 4.0G free (of currently available 503.6G of disk space).
-- Enforced usage limit is thus 4.0G, of which 3.9G are still available.
-- 
-- The limits controlling how much disk space is used by the journal may
-- be configured with SystemMaxUse=, SystemKeepFree=, SystemMaxFileSize=,
-- RuntimeMaxUse=, RuntimeKeepFree=, RuntimeMaxFileSize= settings in
-- /etc/systemd/journald.conf. See journald.conf(5) for details.
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Failed to find module 'drbd'
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Failed to find module 'drbd_transport_tcp'
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Inserted module 'escache'
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Module 'msr' is builtin
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: sh: /sbin/sysctl: No such file or directory
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Inserted module 'ipmi_watchdog'
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Inserted module 'br_netfilter'
Apr 02 11:09:27 node-9.domain.tld haveged[666]: haveged: command socket is listening at fd 3
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Error running install command for nf_conntrack
Apr 02 11:09:27 node-9.domain.tld dracut-cmdline[672]: dracut-8.4 (AltArch) dracut-049-201.git20220131.es8
Apr 02 11:09:27 node-9.domain.tld dracut-cmdline[672]: Using kernel command line parameters: BOOT_IMAGE=(hd0,gpt3)/vmlinuz-4.18.0-147.5.1.es8_24.x86_64 root=/dev/mapper/os-root ro edd=off kvm.halt_poll_ns=400000 cgroup.memory=nokmem intel_iommu=on iommu=pt pci=realloc ixgbe.allow_unsupported_sfp=1 rootdelay=90 nomodeset intel_idle.max_cstate=0 processor.max_cstate=0 crashkernel=300M rd.lvm.lv=os/root biosdevname=0 net.ifnames=1 nopti
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Failed to insert 'nf_conntrack_ipv4': Key has expired
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Inserted module 'nf_conntrack_ipv6'
Apr 02 11:09:27 node-9.domain.tld kernel: Key type dns_resolver registered
Apr 02 11:09:27 node-9.domain.tld kernel: Key type ceph registered
Apr 02 11:09:27 node-9.domain.tld kernel: libceph: loaded (mon/osd proto 15/24)
Apr 02 11:09:27 node-9.domain.tld systemd-modules-load[656]: Inserted module 'rbd'
Apr 02 11:09:27 node-9.domain.tld systemd[1]: systemd-modules-load.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:09:27 node-9.domain.tld kernel: rbd: loaded (major 252)
Apr 02 11:09:27 node-9.domain.tld systemd[1]: systemd-modules-load.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-modules-load.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Failed to start Load Kernel Modules.
-- Subject: Unit systemd-modules-load.service has failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-modules-load.service has failed.
-- 
-- The result is failed.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting Apply Kernel Variables...
-- Subject: Unit systemd-sysctl.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-sysctl.service has begun starting up.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started Apply Kernel Variables.
-- Subject: Unit systemd-sysctl.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-sysctl.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started dracut cmdline hook.
-- Subject: Unit dracut-cmdline.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-cmdline.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting dracut pre-udev hook...
-- Subject: Unit dracut-pre-udev.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-pre-udev.service has begun starting up.
Apr 02 11:09:27 node-9.domain.tld kernel: device-mapper: uevent: version 1.0.3
Apr 02 11:09:27 node-9.domain.tld kernel: device-mapper: ioctl: 4.39.0-ioctl (2018-04-03) initialised: dm-devel@redhat.com
Apr 02 11:09:27 node-9.domain.tld haveged[666]: haveged: ver: 1.9.14; arch: x86; vend: GenuineIntel; build: (gcc 8.4.1 ITV); collect: 128K
Apr 02 11:09:27 node-9.domain.tld haveged[666]: haveged: cpu: (L4 VC); data: 48K (L4 V); inst: 32K (L4 V); idx: 24/40; sz: 31434/52909
Apr 02 11:09:27 node-9.domain.tld haveged[666]: haveged: tot tests(BA8): A:1/1 B:1/1 continuous tests(B):  last entropy estimate 7.99937
Apr 02 11:09:27 node-9.domain.tld haveged[666]: haveged: fills: 0, generated: 0
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started dracut pre-udev hook.
-- Subject: Unit dracut-pre-udev.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-pre-udev.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld kernel: random: crng init done
Apr 02 11:09:27 node-9.domain.tld kernel: random: 7 urandom warning(s) missed due to ratelimiting
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting udev Kernel Device Manager...
-- Subject: Unit systemd-udevd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd.service has begun starting up.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started udev Kernel Device Manager.
-- Subject: Unit systemd-udevd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting udev Coldplug all Devices...
-- Subject: Unit systemd-udev-trigger.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udev-trigger.service has begun starting up.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Mounting Kernel Configuration File System...
-- Subject: Unit sys-kernel-config.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sys-kernel-config.mount has begun starting up.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Mounted Kernel Configuration File System.
-- Subject: Unit sys-kernel-config.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sys-kernel-config.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started udev Coldplug all Devices.
-- Subject: Unit systemd-udev-trigger.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udev-trigger.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Reached target System Initialization.
-- Subject: Unit sysinit.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysinit.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting Show Plymouth Boot Screen...
-- Subject: Unit plymouth-start.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-start.service has begun starting up.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Starting dracut initqueue hook...
-- Subject: Unit dracut-initqueue.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-initqueue.service has begun starting up.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Received SIGRTMIN+20 from PID 852 (plymouthd).
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started Show Plymouth Boot Screen.
-- Subject: Unit plymouth-start.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-start.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Started Forward Password Requests to Plymouth Directory Watch.
-- Subject: Unit systemd-ask-password-plymouth.path has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-ask-password-plymouth.path has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Reached target Paths.
-- Subject: Unit paths.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit paths.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld systemd[1]: Reached target Basic System.
-- Subject: Unit basic.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit basic.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:27 node-9.domain.tld kernel: mlx_compat: loading out-of-tree module taints kernel.
Apr 02 11:09:27 node-9.domain.tld kernel: mlx_compat: module verification failed: signature and/or required key missing - tainting kernel
Apr 02 11:09:27 node-9.domain.tld kernel: Compat-mlnx-ofed backport release: 9886a54
Apr 02 11:09:27 node-9.domain.tld kernel: Backport based on mlnx_ofed/mlnx-ofa_kernel-4.0.git 9886a54
Apr 02 11:09:27 node-9.domain.tld kernel: compat.git: mlnx_ofed/mlnx-ofa_kernel-4.0.git
Apr 02 11:09:28 node-9.domain.tld systemd-udevd[932]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:28 node-9.domain.tld kernel: i40e: Intel(R) Ethernet Connection XL710 Network Driver - version 2.8.20-k
Apr 02 11:09:28 node-9.domain.tld kernel: i40e: Copyright (c) 2013 - 2014 Intel Corporation.
Apr 02 11:09:28 node-9.domain.tld kernel: Broadcom NetXtreme-C/E driver bnxt_en v1.10.0
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.0: fw 8.13.63341 api 1.12 nvm 8.15 0x80009e52 1.2890.0 [8086:1572] [8086:000a]
Apr 02 11:09:28 node-9.domain.tld kernel: bnxt_en 0000:67:00.0 eth0: Broadcom BCM57414 NetXtreme-E 10Gb/25Gb Ethernet found at mem 6ffffe10000, node addr bc:97:e1:b2:29:cc
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.0: The driver for the device detected a newer version of the NVM image v1.12 than expected v1.8. Please install the most recent version of the network driver.
Apr 02 11:09:28 node-9.domain.tld kernel: bnxt_en 0000:67:00.0: 63.008 Gb/s available PCIe bandwidth (8 GT/s x8 link)
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme0: pci function 0000:65:00.0
Apr 02 11:09:28 node-9.domain.tld kernel: libata version 3.00 loaded.
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme1: pci function 0000:66:00.0
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme1: Shutdown timeout set to 15 seconds
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme0: Shutdown timeout set to 15 seconds
Apr 02 11:09:28 node-9.domain.tld kernel: bnxt_en 0000:67:00.1 eth1: Broadcom BCM57414 NetXtreme-E 10Gb/25Gb Ethernet found at mem 6ffffe00000, node addr bc:97:e1:b2:29:cd
Apr 02 11:09:28 node-9.domain.tld kernel: bnxt_en 0000:67:00.1: 63.008 Gb/s available PCIe bandwidth (8 GT/s x8 link)
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme0: 64/0/0 default/read/poll queues
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme1: 64/0/0 default/read/poll queues
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.0: MAC address: 68:05:ca:d5:12:3c
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.0 eth2: NIC Link is Up, 10 Gbps Full Duplex, Flow Control: None
Apr 02 11:09:28 node-9.domain.tld kernel: TECH PREVIEW: kTLS may not be fully supported.
                                          Please review provided documentation for limitations.
Apr 02 11:09:28 node-9.domain.tld systemd-udevd[881]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:28 node-9.domain.tld systemd-udevd[881]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:28 node-9.domain.tld systemd-udevd[844]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:28 node-9.domain.tld kernel: bnxt_en 0000:67:00.1 ens27f1np1: renamed from eth1
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.0: PCI-Express: Speed 8.0GT/s Width x8
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas version 28.100.00.00 loaded
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.0: Features: PF-id[0] VFs: 64 VSIs: 2 QP: 64 RSS FD_ATR FD_SB NTUPLE DCB VxLAN Geneve PTP VEPA
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas_cm0: 63 BIT PCI BUS DMA ADDRESSING SUPPORTED, total mem (1056261160 kB)
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme2: pci function 0000:e3:00.0
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme3: pci function 0000:e4:00.0
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:00:11.5: version 3.0
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme2: Shutdown timeout set to 15 seconds
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme3: Shutdown timeout set to 15 seconds
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:00:11.5: AHCI 0001.0301 32 slots 4 ports 6 Gbps 0x3c impl SATA mode
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme2: 64/0/0 default/read/poll queues
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.1: fw 8.13.63341 api 1.12 nvm 8.15 0x80009e52 1.2890.0 [8086:1572] [8086:0000]
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:00:11.5: flags: 64bit ncq sntf pm led clo only pio slum part ems deso sadm sds apst 
Apr 02 11:09:28 node-9.domain.tld kernel: nvme nvme3: 64/0/0 default/read/poll queues
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.1: The driver for the device detected a newer version of the NVM image v1.12 than expected v1.8. Please install the most recent version of the network driver.
Apr 02 11:09:28 node-9.domain.tld kernel: mlx5_core 0000:4b:00.0: firmware version: 20.36.1010
Apr 02 11:09:28 node-9.domain.tld kernel: mlx5_core 0000:4b:00.0: 252.048 Gb/s available PCIe bandwidth (16 GT/s x16 link)
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia: module license 'NVIDIA' taints kernel.
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia: module license 'NVIDIA' taints kernel.
Apr 02 11:09:28 node-9.domain.tld kernel: Disabling lock debugging due to kernel taint
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia: module license 'NVIDIA' taints kernel.
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host1: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host2: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host3: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host4: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host5: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host6: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: ata1: DUMMY
Apr 02 11:09:28 node-9.domain.tld kernel: ata2: DUMMY
Apr 02 11:09:28 node-9.domain.tld kernel: ata3: SATA max UDMA/133 abar m524288@0xd3d80000 port 0xd3d80200 irq 440
Apr 02 11:09:28 node-9.domain.tld kernel: ata4: SATA max UDMA/133 abar m524288@0xd3d80000 port 0xd3d80280 irq 440
Apr 02 11:09:28 node-9.domain.tld kernel: ata5: SATA max UDMA/133 abar m524288@0xd3d80000 port 0xd3d80300 irq 440
Apr 02 11:09:28 node-9.domain.tld kernel: ata6: SATA max UDMA/133 abar m524288@0xd3d80000 port 0xd3d80380 irq 440
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:00:17.0: AHCI 0001.0301 32 slots 8 ports 6 Gbps 0xff impl SATA mode
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:00:17.0: flags: 64bit ncq sntf pm led clo only pio slum part ems deso sadm sds apst 
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia-nvlink: Nvlink Core is being initialized, major device number 236
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia 0000:19:00.0: enabling device (0140 -> 0142)
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host7: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host8: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host9: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host10: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host11: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host12: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.1: MAC address: 68:05:ca:d5:12:3d
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas_cm0: MSI-X vectors supported: 1, no of cores: 64, max_msix_vectors: -1
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas0-msix0: PCI-MSI-X enabled: IRQ 580
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas_cm0: iomem(0x000003f022100000), mapped(0x000000002ce1f3c1), size(16384)
Apr 02 11:09:28 node-9.domain.tld kernel: mpt3sas_cm0: ioport(0x0000000000000000), size(0)
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia 0000:1a:00.0: enabling device (0140 -> 0142)
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host13: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host14: ahci
Apr 02 11:09:28 node-9.domain.tld kernel: ata7: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00100 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: ata8: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00180 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: ata9: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00200 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: ata10: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00280 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: ata11: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00300 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: ata12: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00380 irq 505
Apr 02 11:09:28 node-9.domain.tld systemd-udevd[844]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:28 node-9.domain.tld kernel: ata13: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00400 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: bnxt_en 0000:67:00.0 ens27f0np0: renamed from eth0
Apr 02 11:09:28 node-9.domain.tld kernel: ata14: SATA max UDMA/133 abar m524288@0xd3d00000 port 0xd3d00480 irq 505
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.1 eth1: NIC Link is Up, 10 Gbps Full Duplex, Flow Control: None
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.1: PCI-Express: Speed 8.0GT/s Width x8
Apr 02 11:09:28 node-9.domain.tld kernel: ata4: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:03:00.0: controller can do FBS, turning on CAP_FBS
Apr 02 11:09:28 node-9.domain.tld kernel: nvidia 0000:33:00.0: enabling device (0140 -> 0142)
Apr 02 11:09:28 node-9.domain.tld kernel: i40e 0000:98:00.1: Features: PF-id[1] VFs: 64 VSIs: 2 QP: 64 RSS FD_ATR FD_SB NTUPLE DCB VxLAN Geneve PTP VEPA
Apr 02 11:09:28 node-9.domain.tld kernel: ata6: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:03:00.0: AHCI 0001.0200 32 slots 3 ports 6 Gbps 0x7 impl SATA mode
Apr 02 11:09:28 node-9.domain.tld kernel: ata3: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:28 node-9.domain.tld kernel: ahci 0000:03:00.0: flags: 64bit ncq fbs pio 
Apr 02 11:09:28 node-9.domain.tld kernel: scsi host15: ahci
Apr 02 11:09:29 node-9.domain.tld kernel: ata5: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi host16: ahci
Apr 02 11:09:29 node-9.domain.tld kernel: scsi host17: ahci
Apr 02 11:09:29 node-9.domain.tld systemd-udevd[881]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:29 node-9.domain.tld systemd-udevd[841]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:29 node-9.domain.tld kernel: ata15: SATA max UDMA/133 abar m2048@0xd3c40000 port 0xd3c40100 irq 581
Apr 02 11:09:29 node-9.domain.tld kernel: i40e 0000:98:00.0 ens2f0: renamed from eth2
Apr 02 11:09:29 node-9.domain.tld kernel: ata16: SATA max UDMA/133 abar m2048@0xd3c40000 port 0xd3c40180 irq 581
Apr 02 11:09:29 node-9.domain.tld kernel: ata17: SATA max UDMA/133 abar m2048@0xd3c40000 port 0xd3c40200 irq 581
Apr 02 11:09:29 node-9.domain.tld kernel: nvidia 0000:34:00.0: enabling device (0140 -> 0142)
Apr 02 11:09:29 node-9.domain.tld systemd-udevd[841]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:29 node-9.domain.tld systemd-udevd[923]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:29 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: renamed from eth1
Apr 02 11:09:29 node-9.domain.tld kernel: mlx5_core 0000:4b:00.0: Port module event: module 0, Cable plugged
Apr 02 11:09:29 node-9.domain.tld systemd-udevd[835]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k
Apr 02 11:09:29 node-9.domain.tld kernel: NVRM: loading NVIDIA UNIX x86_64 Kernel Module  535.104.05  Sat Aug 19 01:15:15 UTC 2023
Apr 02 11:09:29 node-9.domain.tld kernel: mlx5_core 0000:4b:00.0: mlx5_fw_tracer_start:821:(pid 5): FWTracer: Ownership granted and active
Apr 02 11:09:29 node-9.domain.tld kernel: DMAR: 32bit 0000:1b:00.0 uses non-identity mapping
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: Allocated physical memory: size(16 kB)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: Current Controller Queue Depth(3),Max Controller Queue Depth(16)
Apr 02 11:09:29 node-9.domain.tld kernel: nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  535.104.05  Sat Aug 19 00:59:57 UTC 2023
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: Scatter Gather Elements per IO(128)
Apr 02 11:09:29 node-9.domain.tld kernel: mlx5_core 0000:4b:00.1: firmware version: 20.36.1010
Apr 02 11:09:29 node-9.domain.tld kernel: mlx5_core 0000:4b:00.1: 252.048 Gb/s available PCIe bandwidth (16 GT/s x16 link)
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] [nvidia-drm] [GPU ID 0x00001900] Loading driver
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:19:00.0 on minor 0
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] [nvidia-drm] [GPU ID 0x00001a00] Loading driver
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:1a:00.0 on minor 1
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] [nvidia-drm] [GPU ID 0x00003300] Loading driver
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:33:00.0 on minor 2
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: _base_display_fwpkg_version: complete
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] [nvidia-drm] [GPU ID 0x00003400] Loading driver
Apr 02 11:09:29 node-9.domain.tld kernel: mf:
                                                  
Apr 02 11:09:29 node-9.domain.tld kernel: 12050001 
Apr 02 11:09:29 node-9.domain.tld kernel: ata14: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: ata9: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: ata12: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: ata7: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: ata13: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:34:00.0 on minor 3
Apr 02 11:09:29 node-9.domain.tld kernel: ata8: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00010000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:29 node-9.domain.tld kernel: ata10: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:29 node-9.domain.tld kernel: ata11: SATA link down (SStatus 0 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: ata15: SATA link up 6.0 Gbps (SStatus 133 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: ata16: SATA link down (SStatus 0 SControl 310)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: ata17: SATA link up 1.5 Gbps (SStatus 113 SControl 300)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: ata15.00: ATA-8: ThinkSystem M.2 VD, MV.R00-0, max UDMA7
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: "PEX88096"      : FWVersion(00.02.01.00), ChipRevision(0xb0), BiosVersion(00.00.00.00)
Apr 02 11:09:29 node-9.domain.tld kernel: ata15.00: 1875253888 sectors, multi 0: LBA48 NCQ (depth 32)
Apr 02 11:09:29 node-9.domain.tld kernel: ata17.00: ATAPI: MARVELL VIRTUAL, 1.09, max UDMA/66
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: Protocol=(
Apr 02 11:09:29 node-9.domain.tld kernel: ata15.00: configured for UDMA/133
Apr 02 11:09:29 node-9.domain.tld kernel: Initiator
Apr 02 11:09:29 node-9.domain.tld kernel: ata17.00: configured for UDMA/66
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 15:0:0:0: Direct-Access     ATA      ThinkSystem M.2  00-0 PQ: 0 ANSI: 5
Apr 02 11:09:29 node-9.domain.tld kernel: ), Capabilities=(
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 17:0:0:0: Processor         Marvell  Console          1.01 PQ: 0 ANSI: 5
Apr 02 11:09:29 node-9.domain.tld kernel: Diag Trace Buffer,Task Set Full,NCQ)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi host0: Fusion MPT SAS Host
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 15:0:0:0: Attached scsi generic sg0 type 0
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: sending port enable !!
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: 63 BIT PCI BUS DMA ADDRESSING SUPPORTED, total mem (1056261160 kB)
Apr 02 11:09:29 node-9.domain.tld kernel: mlx5_core 0000:4b:00.1: Port module event: module 1, Cable plugged
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 17:0:0:0: Attached scsi generic sg1 type 3
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: host_add: handle(0x0001), sas_addr(0x500605b0000272b0), phys(1)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 0:0:0:0: Enclosure         BROADCOM PEX88096         0201 PQ: 0 ANSI: 5
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 0:0:0:0: set ignore_delay_remove for handle(0x0002)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 0:0:0:0: SES: handle(0x0002), sas_addr(0x500605b0000272b0), phy(0), device_name(0x500605b0000272b0)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 0:0:0:0: enclosure logical id (0x500605b0000272b8), slot(0) 
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: CurrentHostPageSize is 0: Setting default host page size to 4k
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 0:0:0:0: Power-on or device reset occurred
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: MSI-X vectors supported: 1, no of cores: 64, max_msix_vectors: -1
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm0: port enable: SUCCESS
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas1-msix0: PCI-MSI-X enabled: IRQ 710
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 0:0:0:0: Attached scsi generic sg2 type 13
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: iomem(0x0000090000400000), mapped(0x00000000f6784b12), size(16384)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: ioport(0x0000000000000000), size(0)
Apr 02 11:09:29 node-9.domain.tld kernel: sd 15:0:0:0: [sda] 1875253888 512-byte logical blocks: (960 GB/894 GiB)
Apr 02 11:09:29 node-9.domain.tld kernel: sd 15:0:0:0: [sda] 4096-byte physical blocks
Apr 02 11:09:29 node-9.domain.tld kernel: sd 15:0:0:0: [sda] Write Protect is off
Apr 02 11:09:29 node-9.domain.tld kernel: sd 15:0:0:0: [sda] Mode Sense: 00 3a 00 00
Apr 02 11:09:29 node-9.domain.tld kernel: sd 15:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: CurrentHostPageSize is 0: Setting default host page size to 4k
Apr 02 11:09:29 node-9.domain.tld kernel:  sda: sda1 sda2 sda3 sda4 sda5
Apr 02 11:09:29 node-9.domain.tld kernel: sd 15:0:0:0: [sda] Attached SCSI disk
Apr 02 11:09:29 node-9.domain.tld kernel: DMAR: 32bit 0000:b5:00.0 uses non-identity mapping
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: Allocated physical memory: size(16 kB)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: Current Controller Queue Depth(3),Max Controller Queue Depth(16)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: Scatter Gather Elements per IO(128)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: _base_display_fwpkg_version: complete
Apr 02 11:09:29 node-9.domain.tld kernel: mf:
                                                  
Apr 02 11:09:29 node-9.domain.tld kernel: 12050001 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00010000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 00000000 
Apr 02 11:09:29 node-9.domain.tld kernel: 
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030102): originator(IOP), code(0x03), sub_code(0x0102)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: "PEX88096"      : FWVersion(00.02.01.00), ChipRevision(0xb0), BiosVersion(00.00.00.00)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: Protocol=(Initiator), Capabilities=(Diag Trace Buffer,Task Set Full,NCQ)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030100): originator(IOP), code(0x03), sub_code(0x0100)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi host18: Fusion MPT SAS Host
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: sending port enable !!
Apr 02 11:09:29 node-9.domain.tld kernel: mpt3sas_cm1: host_add: handle(0x0001), sas_addr(0x500605b0000272b8), phys(1)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 18:0:0:0: Enclosure         BROADCOM PEX88096         0201 PQ: 0 ANSI: 5
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 18:0:0:0: set ignore_delay_remove for handle(0x0002)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 18:0:0:0: SES: handle(0x0002), sas_addr(0x500605b0000272b8), phy(0), device_name(0x500605b0000272b8)
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 18:0:0:0: enclosure logical id (0x500605b0000272b8), slot(0) 
Apr 02 11:09:29 node-9.domain.tld kernel: scsi 18:0:0:0: Power-on or device reset occurred
Apr 02 11:09:30 node-9.domain.tld kernel: mpt3sas_cm1: port enable: SUCCESS
Apr 02 11:09:30 node-9.domain.tld kernel: scsi 18:0:0:0: Attached scsi generic sg3 type 13
Apr 02 11:09:30 node-9.domain.tld dracut-initqueue[859]: Scanning devices sda4 sda5  for LVM logical volumes os/root
Apr 02 11:09:30 node-9.domain.tld dracut-initqueue[859]: inactive '/dev/os/root' [531.67 GiB] inherit
Apr 02 11:09:30 node-9.domain.tld dracut-initqueue[859]: inactive '/dev/docker/containerd_root' [<357.18 GiB] inherit
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Found device /dev/mapper/os-root.
-- Subject: Unit dev-mapper-os\x2droot.device has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dev-mapper-os\x2droot.device has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Initrd Root Device.
-- Subject: Unit initrd-root-device.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-root-device.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started dracut initqueue hook.
-- Subject: Unit dracut-initqueue.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-initqueue.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Remote File Systems (Pre).
-- Subject: Unit remote-fs-pre.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit remote-fs-pre.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Remote File Systems.
-- Subject: Unit remote-fs.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit remote-fs.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting File System Check on /dev/mapper/os-root...
-- Subject: Unit systemd-fsck-root.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-fsck-root.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd-fsck[1181]: /usr/sbin/fsck.xfs: XFS file system.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started File System Check on /dev/mapper/os-root.
-- Subject: Unit systemd-fsck-root.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-fsck-root.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Mounting /sysroot...
-- Subject: Unit sysroot.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysroot.mount has begun starting up.
Apr 02 11:09:30 node-9.domain.tld kernel: SGI XFS with ACLs, security attributes, no debug enabled
Apr 02 11:09:30 node-9.domain.tld kernel: XFS (dm-0): Mounting V5 Filesystem
Apr 02 11:09:30 node-9.domain.tld kernel: XFS (dm-0): Ending clean mount
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Mounted /sysroot.
-- Subject: Unit sysroot.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysroot.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Initrd Root File System.
-- Subject: Unit initrd-root-fs.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-root-fs.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Reload Configuration from the Real Root...
-- Subject: Unit initrd-parse-etc.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-parse-etc.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reloading.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: initrd-parse-etc.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit initrd-parse-etc.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started Reload Configuration from the Real Root.
-- Subject: Unit initrd-parse-etc.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-parse-etc.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Initrd File Systems.
-- Subject: Unit initrd-fs.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-fs.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Initrd Default Target.
-- Subject: Unit initrd.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting dracut pre-pivot and cleanup hook...
-- Subject: Unit dracut-pre-pivot.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-pre-pivot.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started dracut pre-pivot and cleanup hook.
-- Subject: Unit dracut-pre-pivot.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-pre-pivot.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Cleaning Up and Shutting Down Daemons...
-- Subject: Unit initrd-cleanup.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-cleanup.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Tell haveged about new root...
-- Subject: Unit haveged-switch-root.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit haveged-switch-root.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Timers.
-- Subject: Unit timers.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit timers.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: dracut-pre-pivot.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit dracut-pre-pivot.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped dracut pre-pivot and cleanup hook.
-- Subject: Unit dracut-pre-pivot.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-pre-pivot.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Initrd Default Target.
-- Subject: Unit initrd.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Initrd Root Device.
-- Subject: Unit initrd-root-device.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-root-device.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Basic System.
-- Subject: Unit basic.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit basic.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Sockets.
-- Subject: Unit sockets.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sockets.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target System Initialization.
-- Subject: Unit sysinit.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysinit.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Swap.
-- Subject: Unit swap.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit swap.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-sysctl.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-sysctl.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped Apply Kernel Variables.
-- Subject: Unit systemd-sysctl.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-sysctl.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-tmpfiles-setup.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-tmpfiles-setup.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped Create Volatile Files and Directories.
-- Subject: Unit systemd-tmpfiles-setup.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-setup.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Local File Systems.
-- Subject: Unit local-fs.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit local-fs.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Paths.
-- Subject: Unit paths.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit paths.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Plymouth switch root service...
-- Subject: Unit plymouth-switch-root.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-switch-root.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Remote File Systems.
-- Subject: Unit remote-fs.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit remote-fs.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Remote File Systems (Pre).
-- Subject: Unit remote-fs-pre.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit remote-fs-pre.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: dracut-initqueue.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit dracut-initqueue.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped dracut initqueue hook.
-- Subject: Unit dracut-initqueue.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-initqueue.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-udev-trigger.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-udev-trigger.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped udev Coldplug all Devices.
-- Subject: Unit systemd-udev-trigger.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udev-trigger.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopping udev Kernel Device Manager...
-- Subject: Unit systemd-udevd.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd.service has begun shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Setup Virtual Console...
-- Subject: Unit systemd-vconsole-setup.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-vconsole-setup.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped target Slices.
-- Subject: Unit slices.target has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit slices.target has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: haveged-switch-root.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit haveged-switch-root.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started Tell haveged about new root.
-- Subject: Unit haveged-switch-root.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit haveged-switch-root.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-udevd.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-udevd.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped udev Kernel Device Manager.
-- Subject: Unit systemd-udevd.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: initrd-cleanup.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit initrd-cleanup.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started Cleaning Up and Shutting Down Daemons.
-- Subject: Unit initrd-cleanup.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-cleanup.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-tmpfiles-setup-dev.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-tmpfiles-setup-dev.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped Create Static Device Nodes in /dev.
-- Subject: Unit systemd-tmpfiles-setup-dev.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-setup-dev.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: kmod-static-nodes.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit kmod-static-nodes.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped Create list of required static device nodes for the current kernel.
-- Subject: Unit kmod-static-nodes.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit kmod-static-nodes.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: dracut-pre-udev.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit dracut-pre-udev.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped dracut pre-udev hook.
-- Subject: Unit dracut-pre-udev.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-pre-udev.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: dracut-cmdline.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit dracut-cmdline.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Stopped dracut cmdline hook.
-- Subject: Unit dracut-cmdline.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-cmdline.service has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-udevd-kernel.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-udevd-kernel.socket has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Closed udev Kernel Socket.
-- Subject: Unit systemd-udevd-kernel.socket has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd-kernel.socket has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-udevd-control.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-udevd-control.socket has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Closed udev Control Socket.
-- Subject: Unit systemd-udevd-control.socket has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd-control.socket has finished shutting down.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Cleanup udevd DB...
-- Subject: Unit initrd-udevadm-cleanup-db.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-udevadm-cleanup-db.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started Plymouth switch root service.
-- Subject: Unit plymouth-switch-root.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-switch-root.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: initrd-udevadm-cleanup-db.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit initrd-udevadm-cleanup-db.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started Cleanup udevd DB.
-- Subject: Unit initrd-udevadm-cleanup-db.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-udevadm-cleanup-db.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: systemd-vconsole-setup.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-vconsole-setup.service has successfully entered the 'dead' state.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Started Setup Virtual Console.
-- Subject: Unit systemd-vconsole-setup.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-vconsole-setup.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Reached target Switch Root.
-- Subject: Unit initrd-switch-root.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-switch-root.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Starting Switch Root...
-- Subject: Unit initrd-switch-root.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit initrd-switch-root.service has begun starting up.
Apr 02 11:09:30 node-9.domain.tld systemd[1]: Switching root.
Apr 02 11:09:30 node-9.domain.tld systemd-journald[668]: Journal stopped
-- Subject: The journal has been stopped
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The system journal process has shut down and closed all currently
-- active journal files.
Apr 02 11:09:32 node-9.domain.tld kernel: systemd: 17 output lines suppressed due to ratelimiting
Apr 02 11:09:32 node-9.domain.tld kernel: SELinux:  Disabled at runtime.
Apr 02 11:09:32 node-9.domain.tld kernel: audit: type=1404 audit(1712027371.148:2): enforcing=0 old_enforcing=0 auid=4294967295 ses=4294967295 enabled=0 old-enabled=1 lsm=selinux res=1
Apr 02 11:09:32 node-9.domain.tld systemd[1]: systemd 239 (239-45.es8_5) running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=legacy)
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Detected architecture x86-64.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Set hostname to <node-9.domain.tld>.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: systemd-journald.service: Succeeded.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: initrd-switch-root.service: Succeeded.
Apr 02 11:09:32 node-9.domain.tld kernel: drbd: initialized. Version: 9.0.21-14 (api:2/proto:86-116)
Apr 02 11:09:32 node-9.domain.tld kernel: drbd: GIT-hash: 81d0d0d252b79cc3d612c1d02af2565f66e5858e build by mockbuild@, 2022-05-18 18:42:27
Apr 02 11:09:32 node-9.domain.tld kernel: drbd: registered as block device major 147
Apr 02 11:09:32 node-9.domain.tld systemd-journald[1405]: Journal started
-- Subject: The journal has been started
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The system journal process has started up, opened the journal
-- files for writing and is now ready to process requests.
Apr 02 11:09:32 node-9.domain.tld systemd-journald[1405]: Runtime journal (/run/log/journal/22da9d3cc0234fd4ba2ea46a77ca71eb) is 8.0M, max 4.0G, 3.9G free.
-- Subject: Disk space used by the journal
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Runtime journal (/run/log/journal/22da9d3cc0234fd4ba2ea46a77ca71eb) is currently using 8.0M.
-- Maximum allowed usage is set to 4.0G.
-- Leaving at least 4.0G free (of currently available 503.6G of disk space).
-- Enforced usage limit is thus 4.0G, of which 3.9G are still available.
-- 
-- The limits controlling how much disk space is used by the journal may
-- be configured with SystemMaxUse=, SystemKeepFree=, SystemMaxFileSize=,
-- RuntimeMaxUse=, RuntimeKeepFree=, RuntimeMaxFileSize= settings in
-- /etc/systemd/journald.conf. See journald.conf(5) for details.
Apr 02 11:09:31 node-9.domain.tld systemd-modules-load[1374]: Inserted module 'drbd'
Apr 02 11:09:31 node-9.domain.tld systemd-modules-load[1374]: Inserted module 'drbd_transport_tcp'
Apr 02 11:09:31 node-9.domain.tld systemd-modules-load[1374]: Module 'msr' is builtin
Apr 02 11:09:31 node-9.domain.tld systemd-modules-load[1374]: Inserted module 'nf_conntrack_ipv4'
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Started Create Static Device Nodes in /dev.
-- Subject: Unit systemd-tmpfiles-setup-dev.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-setup-dev.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Starting udev Kernel Device Manager...
-- Subject: Unit systemd-udevd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd.service has begun starting up.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Starting Flush Journal to Persistent Storage...
-- Subject: Unit systemd-journal-flush.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-journal-flush.service has begun starting up.
Apr 02 11:09:32 node-9.domain.tld systemd-journald[1405]: Runtime journal (/run/log/journal/22da9d3cc0234fd4ba2ea46a77ca71eb) is 8.0M, max 4.0G, 3.9G free.
-- Subject: Disk space used by the journal
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Runtime journal (/run/log/journal/22da9d3cc0234fd4ba2ea46a77ca71eb) is currently using 8.0M.
-- Maximum allowed usage is set to 4.0G.
-- Leaving at least 4.0G free (of currently available 503.6G of disk space).
-- Enforced usage limit is thus 4.0G, of which 3.9G are still available.
-- 
-- The limits controlling how much disk space is used by the journal may
-- be configured with SystemMaxUse=, SystemKeepFree=, SystemMaxFileSize=,
-- RuntimeMaxUse=, RuntimeKeepFree=, RuntimeMaxFileSize= settings in
-- /etc/systemd/journald.conf. See journald.conf(5) for details.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Started Flush Journal to Persistent Storage.
-- Subject: Unit systemd-journal-flush.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-journal-flush.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Started udev Kernel Device Manager.
-- Subject: Unit systemd-udevd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udevd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Started udev Coldplug all Devices.
-- Subject: Unit systemd-udev-trigger.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-udev-trigger.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_si: IPMI System Interface driver
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_si dmi-ipmi-si.0: probing via SMBIOS
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_platform: ipmi_si: SMBIOS: io 0xcc0 regsize 1 spacing 1 irq 0
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_si: Adding SMBIOS-specified kcs state machine
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_si: Trying SMBIOS-specified kcs state machine at i/o address 0xcc0, slave address 0x20, irq 0
Apr 02 11:09:32 node-9.domain.tld kernel: power_meter ACPI000D:00: Found ACPI power meter.
Apr 02 11:09:32 node-9.domain.tld kernel: power_meter ACPI000D:00: Ignoring unsafe software power cap!
Apr 02 11:09:32 node-9.domain.tld kernel: power_meter ACPI000D:00: hwmon_device_register() is deprecated. Please convert the driver to use hwmon_device_register_with_info().
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_si dmi-ipmi-si.0: Found new BMC (man_id: 0x004a66, prod_id: 0x04c4, dev_id: 0x20)
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_si dmi-ipmi-si.0: IPMI kcs interface initialized
Apr 02 11:09:32 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:25: Unknown lvalue 'ProtectHostname' in section 'Service'
Apr 02 11:09:32 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:26: Unknown lvalue 'ProtectKernelLogs' in section 'Service'
Apr 02 11:09:32 node-9.domain.tld kernel: ipmi_ssif: IPMI SSIF Interface driver
Apr 02 11:09:32 node-9.domain.tld kernel: i801_smbus 0000:00:1f.4: enabling device (0001 -> 0003)
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Starting RDMA Node Description Daemon...
-- Subject: Unit rdma-ndd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rdma-ndd.service has begun starting up.
Apr 02 11:09:32 node-9.domain.tld kernel: i801_smbus 0000:00:1f.4: SPD Write Disable is set
Apr 02 11:09:32 node-9.domain.tld kernel: i801_smbus 0000:00:1f.4: SMBus using PCI interrupt
Apr 02 11:09:32 node-9.domain.tld kernel: input: PC Speaker as /devices/platform/pcspkr/input/input1
Apr 02 11:09:32 node-9.domain.tld systemd-udevd[1426]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:32 node-9.domain.tld systemd[1]: Started RDMA Node Description Daemon.
-- Subject: Unit rdma-ndd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rdma-ndd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:32 node-9.domain.tld systemd-udevd[1426]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:32 node-9.domain.tld kernel: cdc_ether 1-1.6:1.0 usb0: register 'cdc_ether' at usb-0000:00:14.0-1.6, CDC Ethernet Device, 7e:8a:e1:d6:b0:c5
Apr 02 11:09:32 node-9.domain.tld kernel: usbcore: registered new interface driver cdc_ether
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1570]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1570]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:33 node-9.domain.tld kernel: ses 0:0:0:0: Attached Enclosure device
Apr 02 11:09:33 node-9.domain.tld kernel: ses 18:0:0:0: Attached Enclosure device
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1492]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1492]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1418]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1418]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1644]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started Entropy Daemon based on the HAVEGE algorithm.
-- Subject: Unit haveged.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit haveged.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1469]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1469]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:33 node-9.domain.tld kernel: cdc_ether 1-1.6:1.0 enp0s20f0u1u6: renamed from usb0
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1572]: Process 'esstorage-udev-disk-hotplug add /dev/nvme1n1 pci-0000:66:00.0-nvme-1' failed with exit code 1.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1571]: Process 'esstorage-udev-disk-hotplug add /dev/nvme3n1 pci-0000:e4:00.0-nvme-1' failed with exit code 1.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1576]: Process 'esstorage-udev-disk-hotplug add /dev/nvme2n1 pci-0000:e3:00.0-nvme-1' failed with exit code 1.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1547]: Process 'esstorage-udev-disk-hotplug add /dev/nvme0n1 pci-0000:65:00.0-nvme-1' failed with exit code 1.
Apr 02 11:09:33 node-9.domain.tld systemd-udevd[1561]: Process 'esstorage-udev-disk-hotplug add /dev/sda pci-0000:03:00.0-ata-1' failed with exit code 1.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Condition check resulted in ThinkSystem_M.2 primary being skipped.
-- Subject: Unit dev-block-8:4.device has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dev-block-8:4.device has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Condition check resulted in ThinkSystem_M.2 primary being skipped.
-- Subject: Unit dev-block-8:5.device has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dev-block-8:5.device has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Created slice system-lvm2\x2dpvscan.slice.
-- Subject: Unit system-lvm2\x2dpvscan.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit system-lvm2\x2dpvscan.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting LVM event activation on device 8:5...
-- Subject: Unit lvm2-pvscan@8:5.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit lvm2-pvscan@8:5.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting LVM event activation on device 8:4...
-- Subject: Unit lvm2-pvscan@8:4.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit lvm2-pvscan@8:4.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld haveged[2074]: haveged: command socket is listening at fd 3
Apr 02 11:09:33 node-9.domain.tld lvm[2211]:   pvscan[2211] PV /dev/sda5 online, VG os is complete.
Apr 02 11:09:33 node-9.domain.tld lvm[2211]:   pvscan[2211] VG os run autoactivation.
Apr 02 11:09:33 node-9.domain.tld lvm[2211]:   1 logical volume(s) in volume group "os" now active
Apr 02 11:09:33 node-9.domain.tld lvm[2213]:   pvscan[2213] PV /dev/sda4 online, VG docker is complete.
Apr 02 11:09:33 node-9.domain.tld lvm[2213]:   pvscan[2213] VG docker run autoactivation.
Apr 02 11:09:33 node-9.domain.tld lvm[1393]:   1 logical volume(s) in volume group "os" monitored
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started LVM event activation on device 8:5.
-- Subject: Unit lvm2-pvscan@8:5.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit lvm2-pvscan@8:5.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld lvm[2213]:   1 logical volume(s) in volume group "docker" now active
Apr 02 11:09:33 node-9.domain.tld lvm[1393]:   1 logical volume(s) in volume group "docker" monitored
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Found device /dev/docker/containerd_root.
-- Subject: Unit dev-docker-containerd_root.device has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dev-docker-containerd_root.device has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started LVM event activation on device 8:4.
-- Subject: Unit lvm2-pvscan@8:4.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit lvm2-pvscan@8:4.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started Monitoring of LVM2 mirrors, snapshots etc. using dmeventd or progress polling.
-- Subject: Unit lvm2-monitor.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit lvm2-monitor.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Reached target Local File Systems (Pre).
-- Subject: Unit local-fs-pre.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit local-fs-pre.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounting /var/lib/containerd...
-- Subject: Unit var-lib-containerd.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit var-lib-containerd.mount has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting File System Check on /dev/disk/by-uuid/3D8A-D8DC...
-- Subject: Unit systemd-fsck@dev-disk-by\x2duuid-3D8A\x2dD8DC.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-fsck@dev-disk-by\x2duuid-3D8A\x2dD8DC.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld kernel: XFS (dm-1): Mounting V5 Filesystem
Apr 02 11:09:33 node-9.domain.tld haveged[2074]: haveged: ver: 1.9.14; arch: x86; vend: GenuineIntel; build: (gcc 8.4.1 ITV); collect: 128K
Apr 02 11:09:33 node-9.domain.tld haveged[2074]: haveged: cpu: (L4 VC); data: 48K (L4 V); inst: 32K (L4 V); idx: 24/40; sz: 31434/52909
Apr 02 11:09:33 node-9.domain.tld haveged[2074]: haveged: tot tests(BA8): A:1/1 B:1/1 continuous tests(B):  last entropy estimate 7.99961
Apr 02 11:09:33 node-9.domain.tld haveged[2074]: haveged: fills: 0, generated: 0
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounting /boot...
-- Subject: Unit boot.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit boot.mount has begun starting up.
Apr 02 11:09:33 node-9.domain.tld kernel: XFS (sda3): Mounting V5 Filesystem
Apr 02 11:09:33 node-9.domain.tld kernel: XFS (dm-1): Ending clean mount
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounted /var/lib/containerd.
-- Subject: Unit var-lib-containerd.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit var-lib-containerd.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd-fsck[2276]: fsck.fat 4.1 (2017-01-24)
Apr 02 11:09:33 node-9.domain.tld systemd-fsck[2276]: /dev/sda2: 14 files, 1850/51145 clusters
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started File System Check on /dev/disk/by-uuid/3D8A-D8DC.
-- Subject: Unit systemd-fsck@dev-disk-by\x2duuid-3D8A\x2dD8DC.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-fsck@dev-disk-by\x2duuid-3D8A\x2dD8DC.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld kernel: XFS (sda3): Ending clean mount
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounted /boot.
-- Subject: Unit boot.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit boot.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounting /boot/efi...
-- Subject: Unit boot-efi.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit boot-efi.mount has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounted /boot/efi.
-- Subject: Unit boot-efi.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit boot-efi.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Reached target Local File Systems.
-- Subject: Unit local-fs.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit local-fs.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting Tell Plymouth To Write Out Runtime Data...
-- Subject: Unit plymouth-read-write.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-read-write.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting openibd - configure Mellanox devices...
-- Subject: Unit openibd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit openibd.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + OPENIBD_CONFIG=/etc/infiniband/openib.conf
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + CONFIG=/etc/infiniband/openib.conf
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + export LANG=C
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + LANG=C
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + '[' '!' -f /etc/infiniband/openib.conf ']'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + . /etc/infiniband/openib.conf
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ ONBOOT=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ ALLOW_STOP=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ FORCE_MODE=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: +++ hostname -s
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ NODE_DESC=node-9
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ NODE_DESC_TIME_BEFORE_UPDATE=20
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ NODE_DESC_UPDATE_TIMEOUT=120
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ POST_START_DELAY=0
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RUN_AFFINITY_TUNER=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RUN_MLNX_TUNE=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ LOAD_EIPOIB=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RENICE_IB_MAD=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RUN_SYSCTL=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ ENABLE_FW_TRACER=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ UMAD_LOAD=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ UVERBS_LOAD=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RDMA_CM_LOAD=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RDMA_UCM_LOAD=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ MLX5_LOAD=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ IPOIB_LOAD=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ SET_IPOIB_CM=auto
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ RUN_FW_UPDATER_ONBOOT=no
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting Create Volatile Files and Directories...
-- Subject: Unit systemd-tmpfiles-setup.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-setup.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ pwd
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + CWD=/
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + cd /etc/infiniband
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ pwd
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + WD=/etc/infiniband
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/usr/bin:/lib/udev
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + '[' -e /etc/profile.d/ofed.sh ']'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + ALLOW_STOP=yes
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + FORCE_MODE=no
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + OPENIBD_PRE_START=/etc/infiniband/pre-start-hook.sh
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + OPENIBD_POST_START=/etc/infiniband/post-start-hook.sh
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + OPENIBD_PRE_STOP=/etc/infiniband/pre-stop-hook.sh
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + OPENIBD_POST_STOP=/etc/infiniband/post-stop-hook.sh
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + systemd_auto=0
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + bootID=30f0605b3efd4ca9bec17272900ad44c
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + '[' X30f0605b3efd4ca9bec17272900ad44c '!=' X ']'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ cat /var/run/openibd.bootid
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting Restore /run/initramfs on shutdown...
-- Subject: Unit dracut-shutdown.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-shutdown.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + last_bootID=
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + echo 30f0605b3efd4ca9bec17272900ad44c
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + '[' X == Xmanual ']'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + [[ X == \X ]]
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + systemd_auto=1
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ date +%s
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: ++ tr -d '[:space:]'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + start_time=1712027373
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + base=openibd
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + link=openibd
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + [[ openibd == openibd ]]
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + [[ /etc/init.d/openibd != \/\e\t\c\/\r\c\.\d\/\i\n\i\t\.\d\/\o\p\e\n\i\b\d ]]
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + [[ 1 -eq 0 ]]
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + RUNMODE=auto
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + log_msg 'running in auto mode'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + logger -i 'openibd: running in auto mode'
Apr 02 11:09:33 node-9.domain.tld root[2334]: openibd: running in auto mode
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + echo 1712027373
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + modprobe=/sbin/modprobe
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started Tell Plymouth To Write Out Runtime Data.
-- Subject: Unit plymouth-read-write.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-read-write.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + grep -q '^allow_unsupported_modules  *0'
Apr 02 11:09:33 node-9.domain.tld openibd[2319]: + /sbin/modprobe -c
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started Create Volatile Files and Directories.
-- Subject: Unit systemd-tmpfiles-setup.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-setup.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Started Restore /run/initramfs on shutdown.
-- Subject: Unit dracut-shutdown.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dracut-shutdown.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Received SIGRTMIN+20 from PID 852 (plymouthd).
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting Security Auditing Service...
-- Subject: Unit auditd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit auditd.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting Network Name Resolution...
-- Subject: Unit systemd-resolved.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-resolved.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Starting RPC Bind...
-- Subject: Unit rpcbind.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rpcbind.service has begun starting up.
Apr 02 11:09:33 node-9.domain.tld systemd[1]: Mounting RPC Pipe File System...
-- Subject: Unit var-lib-nfs-rpc_pipefs.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit var-lib-nfs-rpc_pipefs.mount has begun starting up.
Apr 02 11:09:34 node-9.domain.tld auditd[2348]: No plugins found, not dispatching events
Apr 02 11:09:34 node-9.domain.tld auditd[2348]: Init complete, auditd 3.0 listening for events (startup state enable)
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: /sbin/augenrules: No change
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started RPC Bind.
-- Subject: Unit rpcbind.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rpcbind.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: No rules
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: enabled 1
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: failure 1
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: pid 2348
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: rate_limit 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog_limit 8192
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: lost 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog_wait_time 60000
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: enabled 1
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: failure 1
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: pid 2348
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: rate_limit 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog_limit 8192
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: lost 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog_wait_time 60000
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: enabled 1
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: failure 1
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: pid 2348
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: rate_limit 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog_limit 8192
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: lost 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog 0
Apr 02 11:09:34 node-9.domain.tld augenrules[2351]: backlog_wait_time 60000
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -e /sbin/ip ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + ip=/sbin/ip
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + ACTION=start
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + shift
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + ORIG_ACTION=start
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + max_ports_num_in_hca=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module_RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + FORCE=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + XE=/opt/xensource/bin/xe
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + INTERFACE_RENAME=/etc/sysconfig/network-scripts/interface-rename.py
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + INTERFACE_RECONFIGURE=/opt/xensource/libexec/interface-reconfigure
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes '!=' Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep -i 'SuSE Linux' /etc/issue /etc/os-release
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -z '' ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + COLUMNS=80
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -z '' ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -f /etc/sysconfig/init ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + BOOTUP=color
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RES_COL=60
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + MOVE_TO_COL='echo -en \033[60G'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + SETCOLOR_SUCCESS='echo -en \033[1;32m'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + SETCOLOR_FAILURE='echo -en \033[1;31m'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + SETCOLOR_WARNING='echo -en \033[1;33m'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + SETCOLOR_NORMAL='echo -en \033[0;39m'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + LOGLEVEL=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + is_serial
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '' = serial ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case `tty` in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ tty
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + return 1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' color '!=' verbose ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + INITLOG_ARGS=-q
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -f /etc/redhat-release ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + DISTRIB=RedHat
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + NETWORK_CONF_DIR=/etc/sysconfig/network-scripts
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xauto == Xmanual ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ uname -r
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ cut -c -3
Apr 02 11:09:34 node-9.domain.tld kernel: RPC: Registered named UNIX socket transport module.
Apr 02 11:09:34 node-9.domain.tld kernel: RPC: Registered udp transport module.
Apr 02 11:09:34 node-9.domain.tld kernel: RPC: Registered tcp transport module.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ tr -d .
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ tr -d '[:space:]'
Apr 02 11:09:34 node-9.domain.tld kernel: RPC: Registered tcp NFSv4.1 backchannel transport module.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + KPREFIX=41
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + POST_LOAD_MODULES=
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RUN_SYSCTL=no
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UMAD_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UVERBS_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xno == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + IPOIB=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + IPOIB=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + POST_LOAD_MODULES=' rdma_cm'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + POST_LOAD_MODULES=' rdma_cm rdma_ucm'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + GEN1_UNLOAD_MODULES='ib_srp_target scsi_target ib_srp kdapltest_module ib_kdapl ib_sdp eth_ipoib ib_useraccess ib_useraccess_cm ib_cm ib_dapl_srv ib_ip2pr ib_ipoib ib_mlnx_bx ib_tavor mod_thh mod_rhh ib_dm_client ib_sa_client ib_client_query ib_poll ib_mad ib_core ib_services'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4 ib_qib'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4 ib_qib eth_ipoib ib_ipoib mlx4_vnic ib_madeye ib_rds hns_roce'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4 ib_qib eth_ipoib ib_ipoib mlx4_vnic ib_madeye ib_rds hns_roce rds_rdma rds_tcp rds ib_ucm kdapl ib_srp_target scsi_target ib_srp ib_iser ib_sdp'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4 ib_qib eth_ipoib ib_ipoib mlx4_vnic ib_madeye ib_rds hns_roce rds_rdma rds_tcp rds ib_ucm kdapl ib_srp_target scsi_target ib_srp ib_iser ib_sdp rdma_ucm rdma_cm iw_cm ib_cm ib_local_sa findex'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4 ib_qib eth_ipoib ib_ipoib mlx4_vnic ib_madeye ib_rds hns_roce rds_rdma rds_tcp rds ib_ucm kdapl ib_srp_target scsi_target ib_srp ib_iser ib_sdp rdma_ucm rdma_cm iw_cm ib_cm ib_local_sa findex auxiliary mlxdevm mlx5_vdpa'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_MODULES='ib_mthca mlx5_fpga_tools mlx5_ib mlx5_core mlx4_ib ib_ipath ipath_core ib_ehca iw_nes cxgb3i iw_cxgb3 cxgb3 iw_cxgb4 cxgb4i cxgb4 ib_qib eth_ipoib ib_ipoib mlx4_vnic ib_madeye ib_rds hns_roce rds_rdma rds_tcp rds ib_ucm kdapl ib_srp_target scsi_target ib_srp ib_iser ib_sdp rdma_ucm rdma_cm iw_cm ib_cm ib_local_sa findex auxiliary mlxdevm mlx5_vdpa ib_sa ib_uverbs ib_umad ib_mad ib_core ib_addr ib_netlink rdma_rxe mlxfw vfio_mdev'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + STATUS_MODULES='rdma_ucm ib_srp qlgc_vnic ib_sdp rdma_cm ib_local_sa findex ib_ipoib mlx4_core mlx4_ib mlx4_en mlx4_vnic mlx5_core mlx5_ib ib_uverbs ib_umad ib_cm ib_core eth_ipoib mlxfw'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + modinfo scsi_transport_srp
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep depends:
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep -q compat
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Mounted RPC Pipe File System.
-- Subject: Unit var-lib-nfs-rpc_pipefs.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit var-lib-nfs-rpc_pipefs.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Security Auditing Service.
-- Subject: Unit auditd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit auditd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + lsmod
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep scsi_transport_srp
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep -q compat
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Update UTMP about System Boot/Shutdown...
-- Subject: Unit systemd-update-utmp.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-update-utmp.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target rpc_pipefs.target.
-- Subject: Unit rpc_pipefs.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rpc_pipefs.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + modinfo cls_flower
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep depends:
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep -q compat
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Update UTMP about System Boot/Shutdown.
-- Subject: Unit systemd-update-utmp.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-update-utmp.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target System Initialization.
-- Subject: Unit sysinit.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysinit.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + lsmod
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep cls_flower
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep -q compat
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Run system activity accounting tool every 10 minutes.
-- Subject: Unit sysstat-collect.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysstat-collect.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Listening on D-Bus System Message Bus Socket.
-- Subject: Unit dbus.socket has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dbus.socket has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + ipoib_ha_pidfile=/var/run/ipoib_ha.pid
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + srp_daemon_pidfile=/var/run/srp_daemon.pid
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + _truescale=/etc/infiniband/truescale.cmds
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target Sockets.
-- Subject: Unit sockets.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sockets.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Daily Cleanup of Temporary Directories.
-- Subject: Unit systemd-tmpfiles-clean.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-tmpfiles-clean.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Generate summary of yesterday's process accounting.
-- Subject: Unit sysstat-summary.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysstat-summary.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started titanagent check exception timer.
-- Subject: Unit titanagent_check_exception.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target Basic System.
-- Subject: Unit basic.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit basic.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UNLOAD_REC_TIMEOUT=100
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + trap trap_handler 2 9 15
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + [[ start =~ force-.* ]]
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xno == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' XRedHat == XGentoo ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case $ACTION in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -z /etc/infiniband/pre-start-hook.sh ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /etc/infiniband/pre-start-hook.sh ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + start
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + is_active_vf
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + lspci
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep Mellanox
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + grep Virtual
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Load CPU microcode update...
-- Subject: Unit microcode.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit microcode.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting NTP client/server...
-- Subject: Unit chronyd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit chronyd.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Login Service...
-- Subject: Unit systemd-logind.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-logind.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started D-Bus System Message Bus.
-- Subject: Unit dbus.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dbus.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld chronyd[2412]: chronyd version 3.5 starting (+CMDMON +NTP +REFCLOCK +RTC +PRIVDROP +SCFILTER +SIGND +ASYNCDNS +SECHASH +IPV6 +DEBUG)
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Resets System Activity Logs...
-- Subject: Unit sysstat.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysstat.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld chronyd[2412]: Frequency -8.085 +/- 0.310 ppm read from /var/lib/chrony/drift
Apr 02 11:09:34 node-9.domain.tld chronyd[2412]: Using right/UTC timezone to obtain leap second data
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting IPv4 firewall with iptables...
-- Subject: Unit iptables.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit iptables.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting IPv6 firewall with ip6tables...
-- Subject: Unit ip6tables.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ip6tables.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target sshd-keygen.target.
-- Subject: Unit sshd-keygen.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sshd-keygen.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd-resolved[2344]: Positive Trust Anchors:
Apr 02 11:09:34 node-9.domain.tld systemd-resolved[2344]: . IN DS 19036 8 2 49aac11d7b6f6446702e54a1607371607a1a41855200fd2ce1cdde32f24e8fb5
Apr 02 11:09:34 node-9.domain.tld systemd-resolved[2344]: . IN DS 20326 8 2 e06d44b80b8f1d39a95c0b0d7c65d08458e880409bbc683457104237c7f8ec8d
Apr 02 11:09:34 node-9.domain.tld systemd-resolved[2344]: Negative trust anchors: 10.in-addr.arpa 16.172.in-addr.arpa 17.172.in-addr.arpa 18.172.in-addr.arpa 19.172.in-addr.arpa 20.172.in-addr.arpa 21.172.in-addr.arpa 22.172.in-addr.arpa 23.172.in-addr.arpa 24.172.in-addr.arpa 25.172.in-addr.arpa 26.172.in-addr.arpa 27.172.in-addr.arpa 28.172.in-addr.arpa 29.172.in-addr.arpa 30.172.in-addr.arpa 31.172.in-addr.arpa 168.192.in-addr.arpa d.f.ip6.arpa corp home internal intranet lan local private test
Apr 02 11:09:34 node-9.domain.tld systemd-resolved[2344]: Using system hostname 'node-9.domain.tld'.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Configure Open vSwitch.
-- Subject: Unit openvswitch-config.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit openvswitch-config.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Authorization Manager...
-- Subject: Unit polkit.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit polkit.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd-resolved[2344]: request_name_destroy_callback n_ref=1
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Self Monitoring and Reporting Technology (SMART) Daemon...
-- Subject: Unit smartd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit smartd.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started daily update of the root trust anchor for DNSSEC.
-- Subject: Unit unbound-anchor.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit unbound-anchor.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Kernel Samepage Merging...
-- Subject: Unit ksm.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksm.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting titanagent check exception...
-- Subject: Unit titanagent_check_exception.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd-logind[2408]: New seat seat0.
-- Subject: A new seat seat0 is now available
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new seat seat0 has been configured and is now available.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started irqbalance daemon.
-- Subject: Unit irqbalance.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit irqbalance.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -ne 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + return 1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/lsmod
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep -E '^be2net|^cxgb|^mlx|^iw_nes|^iw_cxgb|^ib_qib|^ib_mthca|^ocrdma|^ib_ipoib|^ib_srp|^ib_iser|^ib_uverbs|^ib_addr|^ib_mad|^ib_sa|^iw_cm|^ib_core|^mlxfw|^ib_ucm|^ib_cm|^rdma_ucm|^ib_umad|^rdma_cm|^compat|^ib_netlink|^rdma_rxe'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $1}'
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started titanagent punch card timer.
-- Subject: Unit titanagent_check.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd-logind[2408]: Watching system buttons on /dev/input/event0 (Power Button)
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting titanagent punch card...
-- Subject: Unit titanagent_check.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started dnf makecache --timer.
-- Subject: Unit dnf-makecache.timer has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit dnf-makecache.timer has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local 'loaded_modules=mlx5_ib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ib_uverbs
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ib_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: mlx5_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: mlxfw
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: mlxdevm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: mlx_compat'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target Timers.
-- Subject: Unit timers.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit timers.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/mlx5_ib/srcversion
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Network Name Resolution.
-- Subject: Unit systemd-resolved.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-resolved.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=7A5C52F25D687DFBFE770B3
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Resets System Activity Logs.
-- Subject: Unit sysstat.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysstat.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo mlx5_ib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Kernel Samepage Merging.
-- Subject: Unit ksm.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksm.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: smartd 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-147.5.1.es8_24.x86_64] (local build)
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Opened configuration file /etc/smartmontools/smartd.conf
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Configuration file /etc/smartmontools/smartd.conf was parsed, found DEVICESCAN, scanning devices
Apr 02 11:09:34 node-9.domain.tld dbus-daemon[2410]: [system] Successfully activated service 'org.freedesktop.systemd1'
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/sda, type changed from 'scsi' to 'sat'
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/sda [SAT], opened
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/sda [SAT], ThinkSystem M.2 VD, S/N:01aa3321fd7c0010, FW:MV.R00-0, 960 GB
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=7A5C52F25D687DFBFE770B3
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X7A5C52F25D687DFBFE770B3 '!=' X7A5C52F25D687DFBFE770B3 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld polkitd[2431]: Started polkitd version 0.115
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Kernel Samepage Merging (KSM) Tuning Daemon...
-- Subject: Unit ksmtuned.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksmtuned.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld ip6tables.init[2419]: ip6tables: Applying firewall rules: [  OK  ]
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/ib_uverbs/srcversion
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target Host and Network Name Lookups.
-- Subject: Unit nss-lookup.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit nss-lookup.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Login Service.
-- Subject: Unit systemd-logind.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-logind.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=0A84617C52EFD1A9CD7552E
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started IPv6 firewall with ip6tables.
-- Subject: Unit ip6tables.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ip6tables.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo ib_uverbs
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Kernel Samepage Merging (KSM) Tuning Daemon.
-- Subject: Unit ksmtuned.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksmtuned.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/sda [SAT], not found in smartd database.
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/sda [SAT], lacks SMART capability
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/sda [SAT], to proceed anyway, use '-T permissive' Directive.
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme0, opened
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme0, SSDPEYKX040T8L, S/N:BTLJ133202E54P0SGN, FW:VEV1LA34, 4.00 TB
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme0, is SMART capable. Adding to "monitor" list.
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme1, opened
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme1, SSDPEYKX040T8L, S/N:BTLJ112300ES4P0SGN, FW:VEV1LA34, 4.00 TB
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme1, is SMART capable. Adding to "monitor" list.
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme2, opened
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme2, SSDPEYKX040T8L, S/N:BTLJ1123022K4P0SGN, FW:VEV1LA34, 4.00 TB
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme2, is SMART capable. Adding to "monitor" list.
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme3, opened
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=0A84617C52EFD1A9CD7552E
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme3, SSDPEYKX040T8L, S/N:BTLJ133202004P0SGN, FW:VEV1LA34, 4.00 TB
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X0A84617C52EFD1A9CD7552E '!=' X0A84617C52EFD1A9CD7552E ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Device: /dev/nvme3, is SMART capable. Adding to "monitor" list.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/ib_core/srcversion
Apr 02 11:09:34 node-9.domain.tld smartd[2436]: Monitoring 0 ATA/SATA, 0 SCSI/SAS and 4 NVMe devices
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=07B556AF4284AAA6B33280F
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo ib_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started NTP client/server.
-- Subject: Unit chronyd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit chronyd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: microcode.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit microcode.service has successfully entered the 'dead' state.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=07B556AF4284AAA6B33280F
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X07B556AF4284AAA6B33280F '!=' X07B556AF4284AAA6B33280F ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Load CPU microcode update.
-- Subject: Unit microcode.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit microcode.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/mlx5_core/srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=2CE284E82D5E057994AA3EC
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo mlx5_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=2CE284E82D5E057994AA3EC
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X2CE284E82D5E057994AA3EC '!=' X2CE284E82D5E057994AA3EC ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/mlxfw/srcversion
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Self Monitoring and Reporting Technology (SMART) Daemon.
-- Subject: Unit smartd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit smartd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=43D7A3CCEF95850986FB5DC
Apr 02 11:09:34 node-9.domain.tld ovs-vsctl[2434]: ovs|00001|db_ctl_base|ERR|unix:/var/run/openvswitch/db.sock: database connection failed (No such file or directory)
Apr 02 11:09:34 node-9.domain.tld openvswitch-config[2425]: ovs-vsctl: unix:/var/run/openvswitch/db.sock: database connection failed (No such file or directory)
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo mlxfw
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=43D7A3CCEF95850986FB5DC
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X43D7A3CCEF95850986FB5DC '!=' X43D7A3CCEF95850986FB5DC ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/mlxdevm/srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=142A980086B007DF13578F2
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo mlxdevm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=142A980086B007DF13578F2
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X142A980086B007DF13578F2 '!=' X142A980086B007DF13578F2 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for loaded_module in $loaded_modules
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /bin/cat /sys/module/mlx_compat/srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local loaded_srcver=88A16AF87141D048C6FF1DA
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /sbin/modinfo mlx_compat
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep srcversion
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local curr_srcver=88A16AF87141D048C6FF1DA
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X88A16AF87141D048C6FF1DA '!=' X88A16AF87141D048C6FF1DA ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local goFlag=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + OIFS='
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: '
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + NIFS='
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: '
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + IFS='
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: '
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep -v :#
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep -rE 'options.*mlx' /etc/modprobe.d/atm-blacklist.conf /etc/modprobe.d/escache.conf /etc/modprobe.d/firewalld-sysctls.conf /etc/modprobe.d/ib_ipoib.conf /etc/modprobe.d/kvm.conf /etc/modprobe.d/l2tp_eth-blacklist.conf /etc/modprobe.d/l2tp_ip-blacklist.conf /etc/modprobe.d/l2tp_netlink-blacklist.conf /etc/modprobe.d/l2tp_ppp-blacklist.conf /etc/modprobe.d/lockd.conf /etc/modprobe.d/mlnx-bf.conf /etc/modprobe.d/mlnx.conf /etc/modprobe.d/nouveau-blacklist.conf /etc/modprobe.d/sctp-blacklist.conf /etc/modprobe.d/sctp_diag-blacklist.conf /etc/modprobe.d/sfc.conf /etc/modprobe.d/tuned.conf /etc/modprobe.d/vhost.conf /etc/modprobe.d/watchdog-blacklist.conf
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ cut -d: -f2-
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ uniq
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + IFS='
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: '
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + is_ivyb
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /usr/bin/lscpu
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep 'CPU family'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ cut -d: -f 2
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ sed -e 's/ //g'
Apr 02 11:09:34 node-9.domain.tld polkitd[2431]: Loading rules from directory /etc/polkit-1/rules.d
Apr 02 11:09:34 node-9.domain.tld polkitd[2431]: Loading rules from directory /usr/share/polkit-1/rules.d
Apr 02 11:09:34 node-9.domain.tld polkitd[2431]: Finished loading, compiling and executing 3 rules
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Authorization Manager.
-- Subject: Unit polkit.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit polkit.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld polkitd[2431]: Acquired the name org.freedesktop.PolicyKit1 on the system bus
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + cpu_family=6
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ /usr/bin/lscpu
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep Model:
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ cut -d: -f 2
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ sed -e 's/ //g'
Apr 02 11:09:34 node-9.domain.tld systemd[1]: titanagent_check.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check.service has successfully entered the 'dead' state.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started titanagent punch card.
-- Subject: Unit titanagent_check.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + cpu_model=106
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "${cpu_family}_${cpu_model}" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + return 1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' RedHat = SuSE ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module mlx5_ib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=mlx5_ib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo mlx5_ib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/hw/mlx5/mlx5_ib.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/hw/mlx5/mlx5_ib.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/hw/mlx5/mlx5_ib.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/hw/mlx5/mlx5_ib.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld systemd[1]: titanagent_check_exception.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check_exception.service has successfully entered the 'dead' state.
Apr 02 11:09:34 node-9.domain.tld iptables.init[2416]: iptables: Applying firewall rules: [  OK  ]
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started titanagent check exception.
-- Subject: Unit titanagent_check_exception.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started IPv4 firewall with iptables.
-- Subject: Unit iptables.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit iptables.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Reached target Network (Pre).
-- Subject: Unit network-pre.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit network-pre.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Open vSwitch Database Unit...
-- Subject: Unit ovsdb-server.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovsdb-server.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld chown[2714]: /usr/bin/chown: cannot access '/var/run/openvswitch': No such file or directory
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe mlx5_ib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + my_rc=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -ne 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module mlx5_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=mlx5_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo mlx5_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe mlx5_core
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + my_rc=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -ne 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for d in mlx5_fw fw_tracer
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -f /sys/kernel/debug/tracing/events/mlx5/mlx5_fw/enable ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo 1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + break
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /opt/xensource/bin/xe ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X '!=' X ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module ib_umad
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=ib_umad
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + ib_set_node_desc
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo ib_umad
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_umad.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_umad.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_umad.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_umad.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe ib_umad
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xyes == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module ib_uverbs
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=ib_uverbs
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo ib_uverbs
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_uverbs.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_uverbs.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_uverbs.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_uverbs.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe ib_uverbs
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 1 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module ib_ipoib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=ib_ipoib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo ib_ipoib
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/ulp/ipoib/ib_ipoib.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/ulp/ipoib/ib_ipoib.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/ulp/ipoib/ib_ipoib.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/ulp/ipoib/ib_ipoib.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe ib_ipoib
Apr 02 11:09:34 node-9.domain.tld kernel: mlx5_core 0000:4b:00.0: MLX5E: StrdRq(0) RqSz(1024) StrdSz(256) RxCqeCmprss(0)
Apr 02 11:09:34 node-9.domain.tld kernel: mlx5_core 0000:4b:00.0: MLX5E: StrdRq(0) RqSz(1024) StrdSz(256) RxCqeCmprss(0)
Apr 02 11:09:34 node-9.domain.tld ovs-ctl[2723]: Starting ovsdb-server [  OK  ]
Apr 02 11:09:34 node-9.domain.tld ovs-vsctl[2833]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait -- init -- set Open_vSwitch . db-version=8.3.0
Apr 02 11:09:34 node-9.domain.tld systemd-udevd[1496]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:34 node-9.domain.tld kernel: mlx5_core 0000:4b:00.1: MLX5E: StrdRq(0) RqSz(1024) StrdSz(256) RxCqeCmprss(0)
Apr 02 11:09:34 node-9.domain.tld kernel: mlx5_core 0000:4b:00.1: MLX5E: StrdRq(0) RqSz(1024) StrdSz(256) RxCqeCmprss(0)
Apr 02 11:09:34 node-9.domain.tld ovs-vsctl[2840]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait set Open_vSwitch . ovs-version=2.16.2.20220512 "external-ids:system-id=\"40ff292f-6627-432d-96c9-c76a9fee2eeb\"" "external-ids:rundir=\"/var/run/openvswitch\"" "system-type=\"escore\"" "system-version=\"8.4\""
Apr 02 11:09:34 node-9.domain.tld ovs-ctl[2723]: Configuring Open vSwitch system IDs [  OK  ]
Apr 02 11:09:34 node-9.domain.tld ovs-ctl[2723]: Enabling remote OVSDB managers [  OK  ]
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Open vSwitch Database Unit.
-- Subject: Unit ovsdb-server.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovsdb-server.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld ovs-vsctl[2848]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait add Open_vSwitch . external-ids hostname=node-9.domain.tld
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Open vSwitch Delete Transient Ports...
-- Subject: Unit ovs-delete-transient-ports.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovs-delete-transient-ports.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Created slice system-mlnx_interface_mgr.slice.
-- Subject: Unit system-mlnx_interface_mgr.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit system-mlnx_interface_mgr.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started mlnx_interface_mgr - configure ib0.
-- Subject: Unit mlnx_interface_mgr@ib0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit mlnx_interface_mgr@ib0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + i=ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + shift
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' -z ib0 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + KNOWN_CONF_VARS='TYPE BROADCAST MASTER BRIDGE BOOTPROTO IPADDR NETMASK PREFIX                  NAME DEVICE ONBOOT NM_CONTROLLED CONNECTED_MODE'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + OPENIBD_CONFIG=/etc/infiniband/openib.conf
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + CONFIG=/etc/infiniband/openib.conf
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + export LANG=C
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + LANG=C
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' '!' -f /etc/infiniband/openib.conf ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + OS_IS_BOOTING=0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ cat /var/run/mlx_ifc-ib0.bootid
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + last_bootID=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' X == X ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' -e /sys/class/net/ib0/parent ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ cat /proc/sys/kernel/random/boot_id
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ sed -e s/-//g
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + bootID=30f0605b3efd4ca9bec17272900ad44c
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + echo 30f0605b3efd4ca9bec17272900ad44c
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + [[ X == \X ]]
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + OS_IS_BOOTING=1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ cat /var/run/mlx_os_booting
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + start_time=1712027373
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' X1712027373 '!=' X ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ date +%s
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ tr -d '[:space:]'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + let run_time=1712027374-1712027373
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' 1 -lt 300 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + OS_IS_BOOTING=1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ cat /var/run/mlx_ifc.manual
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + last_bootID_manual=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + [[ X != \X ]]
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + . /etc/infiniband/openib.conf
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ ONBOOT=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ ALLOW_STOP=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ FORCE_MODE=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: +++ hostname -s
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ NODE_DESC=node-9
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ NODE_DESC_TIME_BEFORE_UPDATE=20
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ NODE_DESC_UPDATE_TIMEOUT=120
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ POST_START_DELAY=0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RUN_AFFINITY_TUNER=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RUN_MLNX_TUNE=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ LOAD_EIPOIB=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RENICE_IB_MAD=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RUN_SYSCTL=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ ENABLE_FW_TRACER=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ UMAD_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ UVERBS_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RDMA_CM_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RDMA_UCM_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ MLX5_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ IPOIB_LOAD=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ SET_IPOIB_CM=auto
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ RUN_FW_UPDATER_ONBOOT=no
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + IPOIB_MTU=65520
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' -f /etc/redhat-release ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + NETWORK_CONF_DIR=/etc/sysconfig/network-scripts
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + log_msg 'Setting up Mellanox network interface: ib0'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + logger -t mlnx_interface_mgr -i 'Setting up Mellanox network interface: ib0'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr[2882]: Setting up Mellanox network interface: ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' 1 -eq 1 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + case "$(echo "$i" | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ echo ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ cat /sys/module/ib_ipoib/parameters/send_queue_size
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + bring_up ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local i=ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + shift
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local RC=0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local IFCFG_FILE=/etc/sysconfig/network-scripts/ifcfg-ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' '!' -e /etc/sysconfig/network-scripts/ifcfg-ib0 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' -e /etc/sysconfig/network-scripts/ifcfg-ib0 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + . /etc/sysconfig/network-scripts/ifcfg-ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ NAME=ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ DEVICE=ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ TYPE=InfiniBand
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ BOOTPROTO=static
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ ONBOOT=yes
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ IPADDR=192.168.0.28
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ PREFIX=23
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' yes = no -o yes = NO ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local SET_CONNECTED_MODE=auto
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local is_ipoib_if=0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + case "$(echo "${i}" | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + ipoib_send_queue_size=128
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -z 128 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 128 -gt 1024 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo_success 'Loading HCA driver and Access Layer: '
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo -n Loading HCA driver and Access Layer:
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: Loading HCA driver and Access Layer:+ '[' color = color ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo -en '\033[60G'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: [20B blob data]
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: [  + '[' color = color ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo -en '\033[1;32m'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo -n OK
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: OK+ '[' color = color ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo -en '\033[0;39m'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + echo -n '  ]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]:   ]+ echo -e '\r'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: [1B blob data]
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + return 0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ echo ib0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' ' rdma_cm rdma_ucm' '!=' '' ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for mod in $POST_LOAD_MODULES
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case $mod in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module rdma_cm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=rdma_cm
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo rdma_cm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + is_ipoib_if=1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + grep -q ib_ipoib
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + /sbin/ethtool -i ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + is_ipoib_if=1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' 1 -eq 1 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' Xauto == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' Xauto == Xauto ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local drvname=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' -e /sys/class/net/ib0/device/driver/module ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_cm.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_cm.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_cm.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_cm.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: +++ readlink -f /sys/class/net/ib0/device/driver/module
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe rdma_cm
Apr 02 11:09:34 node-9.domain.tld systemd-udevd[1427]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started Open vSwitch Delete Transient Ports.
-- Subject: Unit ovs-delete-transient-ports.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovs-delete-transient-ports.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ basename /sys/module/mlx5_core
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + drvname=mlx5_core
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' Xmlx5_core == Xmlx5_core ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + is_connected_mode_supported ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local i=ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + shift
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local hca_type=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' -e /sys/class/net/ib0/device/infiniband ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ cat /sys/class/net/ib0/device/infiniband/mlx5_0/hca_type
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Starting Open vSwitch Forwarding Unit...
-- Subject: Unit ovs-vswitchd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovs-vswitchd.service has begun starting up.
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + hca_type=MT4123
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + echo -e MT4123
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + grep -qE '4113|4114'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + grep -q '^0' /sys/module/ib_ipoib/parameters/ipoib_enhanced
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + log_msg 'INFO: ib0 does not support connected mode'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + logger -t mlnx_interface_mgr -i 'INFO: ib0 does not support connected mode'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr[2916]: INFO: ib0 does not support connected mode
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + return 1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ ls -l /sys/class/net/ib0/queues/
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ grep rx-
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ wc -l
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ awk '{print $1}'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + local num_rx_queue=63
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' 63 -eq 1 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' '!' -e /etc/sysconfig/network-scripts/ifcfg-ib0 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' 1 -eq 1 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + log_msg 'OS is booting, will not run ifup on ib0'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + logger -t mlnx_interface_mgr -i 'OS is booting, will not run ifup on ib0'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr[2928]: OS is booting, will not run ifup on ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + unset TYPE BROADCAST MASTER BRIDGE BOOTPROTO IPADDR NETMASK PREFIX NAME DEVICE ONBOOT NM_CONTROLLED CONNECTED_MODE
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + return 6
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + '[' 6 -eq 1 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + case "$(echo "$i" | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ echo ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + log_msg 'Running: /bin/mlnx_conf_mgr.sh ib0'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + logger -t mlnx_interface_mgr -i 'Running: /bin/mlnx_conf_mgr.sh ib0'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr[2935]: Running: /bin/mlnx_conf_mgr.sh ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + /bin/mlnx_conf_mgr.sh ib0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -ne 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + for mod in $POST_LOAD_MODULES
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case $mod in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + load_module rdma_ucm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + local module=rdma_ucm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ modinfo rdma_ucm
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ grep filename
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ awk '{print $NF}'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + filename=/lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_ucm.ko
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -n /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_ucm.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -L /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_ucm.ko ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ rpm -qf /lib/modules/4.18.0-147.5.1.es8_24.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/rdma_ucm.ko --queryformat '[%{NAME}]'
Apr 02 11:09:34 node-9.domain.tld mlnx_conf_mgr[2983]: INFO: No configurations found for mlx5_0, skipping.
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + case "$(echo ${i} | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:34 node-9.domain.tld systemd[1]: Started mlnx_interface_mgr - configure ib1.
-- Subject: Unit mlnx_interface_mgr@ib1.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit mlnx_interface_mgr@ib1.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ echo ib0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_name=kmod-mlnx-ofa_kernel
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + rpm_query_passed=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 1 -eq 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + case "$rpm_name" in
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + /sbin/modprobe rdma_ucm
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ /bin/ls -1 '/etc/sysconfig/network-scripts/ifcfg-ib0.[0-9a-fA-F]*'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + i=ib1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + shift
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' -z ib1 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + KNOWN_CONF_VARS='TYPE BROADCAST MASTER BRIDGE BOOTPROTO IPADDR NETMASK PREFIX                  NAME DEVICE ONBOOT NM_CONTROLLED CONNECTED_MODE'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + OPENIBD_CONFIG=/etc/infiniband/openib.conf
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + CONFIG=/etc/infiniband/openib.conf
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + export LANG=C
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + LANG=C
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' '!' -f /etc/infiniband/openib.conf ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + OS_IS_BOOTING=0
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ cat /var/run/mlx_ifc-ib1.bootid
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: + CHILD_CONFS=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + last_bootID=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' X == X ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' -e /sys/class/net/ib1/parent ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ cut -d: -f1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2861]: ++ grep -E '=\s*"*ib0\.[0-9a-fA-F]*"*\s*$' /etc/sysconfig/network-scripts/ifcfg-br-mgmt /etc/sysconfig/network-scripts/ifcfg-br-prv /etc/sysconfig/network-scripts/ifcfg-br-roller /etc/sysconfig/network-scripts/ifcfg-br-storagepub /etc/sysconfig/network-scripts/ifcfg-br-vxlan /etc/sysconfig/network-scripts/ifcfg-enp0s20f0u1u6 /etc/sysconfig/network-scripts/ifcfg-ens27f0np0 /etc/sysconfig/network-scripts/ifcfg-ens27f1np1 /etc/sysconfig/network-scripts/ifcfg-ens2f0 /etc/sysconfig/network-scripts/ifcfg-ens2f1 /etc/sysconfig/network-scripts/ifcfg-ib0 /etc/sysconfig/network-scripts/ifcfg-ib1 /etc/sysconfig/network-scripts/ifcfg-lo /etc/sysconfig/network-scripts/ifdown /etc/sysconfig/network-scripts/ifdown-bnep /etc/sysconfig/network-scripts/ifdown-eth /etc/sysconfig/network-scripts/ifdown-ippp /etc/sysconfig/network-scripts/ifdown-ipv6 /etc/sysconfig/network-scripts/ifdown-isdn /etc/sysconfig/network-scripts/ifdown-post /etc/sysconfig/network-scripts/ifdown-routes /etc/sysconfig/network-scripts/ifdown-sit /etc/sysconfig/network-scripts/ifdown-tunnel /etc/sysconfig/network-scripts/ifup /etc/sysconfig/network-scripts/ifup-aliases /etc/sysconfig/network-scripts/ifup-bnep /etc/sysconfig/network-scripts/ifup-eth /etc/sysconfig/network-scripts/ifup-ippp /etc/sysconfig/network-scripts/ifup-ipv6 /etc/sysconfig/network-scripts/ifup-isdn /etc/sysconfig/network-scripts/ifup-plip /etc/sysconfig/network-scripts/ifup-plusb /etc/sysconfig/network-scripts/ifup-post /etc/sysconfig/network-scripts/ifup-routes /etc/sysconfig/network-scripts/ifup-sit /etc/sysconfig/network-scripts/ifup-tunnel /etc/sysconfig/network-scripts/ifup-wireless /etc/sysconfig/network-scripts/init.ipv6-global /etc/sysconfig/network-scripts/network-functions /etc/sysconfig/network-scripts/network-functions-ipv6 /etc/sysconfig/network-scripts/route-br-mgmt
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ cat /proc/sys/kernel/random/boot_id
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ sed -e s/-//g
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + bootID=30f0605b3efd4ca9bec17272900ad44c
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + echo 30f0605b3efd4ca9bec17272900ad44c
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + [[ X == \X ]]
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + OS_IS_BOOTING=1
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' 0 -ne 0 ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /sbin/udevstart ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /sbin/start_udev ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + UDEVSTART=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ cat /var/run/mlx_os_booting
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' '!' -z '' ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' X == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xno == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /sbin/sysctl_perf_tuning ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xno == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /usr/sbin/mlnx_affinity ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xno == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' -x /usr/sbin/mlnx_tune ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: + '[' Xno == Xyes ']'
Apr 02 11:09:34 node-9.domain.tld openibd[2319]: ++ ps -C irqbalance -o pid=
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + start_time=1712027373
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' X1712027373 '!=' X ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ date +%s
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ tr -d '[:space:]'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + let run_time=1712027374-1712027373
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' 1 -lt 300 ']'
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + OS_IS_BOOTING=1
Apr 02 11:09:34 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ cat /var/run/mlx_ifc.manual
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + last_bootID_manual=
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + [[ X != \X ]]
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + . /etc/infiniband/openib.conf
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ ONBOOT=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ ALLOW_STOP=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ FORCE_MODE=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: +++ hostname -s
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ NODE_DESC=node-9
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ NODE_DESC_TIME_BEFORE_UPDATE=20
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ NODE_DESC_UPDATE_TIMEOUT=120
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ POST_START_DELAY=0
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RUN_AFFINITY_TUNER=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RUN_MLNX_TUNE=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ LOAD_EIPOIB=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RENICE_IB_MAD=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RUN_SYSCTL=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ ENABLE_FW_TRACER=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ UMAD_LOAD=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ UVERBS_LOAD=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RDMA_CM_LOAD=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RDMA_UCM_LOAD=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ MLX5_LOAD=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ IPOIB_LOAD=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ SET_IPOIB_CM=auto
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ RUN_FW_UPDATER_ONBOOT=no
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + IPOIB_MTU=65520
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' -f /etc/redhat-release ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + NETWORK_CONF_DIR=/etc/sysconfig/network-scripts
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + log_msg 'Setting up Mellanox network interface: ib1'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + logger -t mlnx_interface_mgr -i 'Setting up Mellanox network interface: ib1'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr[3037]: Setting up Mellanox network interface: ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' 1 -eq 1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + case "$(echo "$i" | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ echo ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + bring_up ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local i=ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + shift
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local RC=0
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local IFCFG_FILE=/etc/sysconfig/network-scripts/ifcfg-ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' '!' -e /etc/sysconfig/network-scripts/ifcfg-ib1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' -e /etc/sysconfig/network-scripts/ifcfg-ib1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + . /etc/sysconfig/network-scripts/ifcfg-ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ NAME=ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ DEVICE=ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ TYPE=InfiniBand
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ BOOTPROTO=static
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ ONBOOT=yes
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ IPADDR=192.168.2.28
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ PREFIX=23
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' yes = no -o yes = NO ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local SET_CONNECTED_MODE=auto
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local is_ipoib_if=0
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + case "$(echo "${i}" | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ echo ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + is_ipoib_if=1
Apr 02 11:09:35 node-9.domain.tld systemd[1]: mlnx_interface_mgr@ib0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit mlnx_interface_mgr@ib0.service has successfully entered the 'dead' state.
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + /sbin/ethtool -i ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + grep -q ib_ipoib
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + is_ipoib_if=1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' 1 -eq 1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' Xauto == Xyes ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' Xauto == Xauto ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local drvname=
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' -e /sys/class/net/ib1/device/driver/module ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: +++ readlink -f /sys/class/net/ib1/device/driver/module
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + irqbalance_pid=' 2450'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + '[' 'X 2450' '!=' X ']'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + kill -s SIGHUP 2450
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + '[' '!' -z 0 ']'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + '[' 0 -gt 0 ']'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + grep -q '^#alias netdev-ib' /etc/modprobe.d/ib_ipoib.conf
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + [[ Xno == \X\y\e\s ]]
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + /bin/rm -f /var/run/mlx_os_booting
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + return 0
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + RC=0
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + '[' '!' -z /etc/infiniband/post-start-hook.sh ']'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + '[' -x /etc/infiniband/post-start-hook.sh ']'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + '[' 0 -ne 0 ']'
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + cleanup
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + /bin/rm -f /var/run/mlx_os_booting
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ basename /sys/module/mlx5_core
Apr 02 11:09:35 node-9.domain.tld openibd[2319]: + exit 0
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + drvname=mlx5_core
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' Xmlx5_core == Xmlx5_core ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + is_connected_mode_supported ib1
Apr 02 11:09:35 node-9.domain.tld systemd[1]: Started openibd - configure Mellanox devices.
-- Subject: Unit openibd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit openibd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local i=ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + shift
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local hca_type=
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' -e /sys/class/net/ib1/device/infiniband ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ cat /sys/class/net/ib1/device/infiniband/mlx5_1/hca_type
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + hca_type=MT4123
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + echo -e MT4123
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + grep -qE '4113|4114'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + grep -q '^0' /sys/module/ib_ipoib/parameters/ipoib_enhanced
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + log_msg 'INFO: ib1 does not support connected mode'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + logger -t mlnx_interface_mgr -i 'INFO: ib1 does not support connected mode'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr[3070]: INFO: ib1 does not support connected mode
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + return 1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ ls -l /sys/class/net/ib1/queues/
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ grep rx-
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ wc -l
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ awk '{print $1}'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + local num_rx_queue=63
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' 63 -eq 1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' '!' -e /etc/sysconfig/network-scripts/ifcfg-ib1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' 1 -eq 1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + log_msg 'OS is booting, will not run ifup on ib1'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + logger -t mlnx_interface_mgr -i 'OS is booting, will not run ifup on ib1'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr[3077]: OS is booting, will not run ifup on ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + unset TYPE BROADCAST MASTER BRIDGE BOOTPROTO IPADDR NETMASK PREFIX NAME DEVICE ONBOOT NM_CONTROLLED CONNECTED_MODE
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + return 6
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + '[' 6 -eq 1 ']'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + case "$(echo "$i" | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ echo ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + log_msg 'Running: /bin/mlnx_conf_mgr.sh ib1'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + logger -t mlnx_interface_mgr -i 'Running: /bin/mlnx_conf_mgr.sh ib1'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr[3081]: Running: /bin/mlnx_conf_mgr.sh ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + /bin/mlnx_conf_mgr.sh ib1
Apr 02 11:09:35 node-9.domain.tld kernel: openvswitch: Open vSwitch switching datapath 2.16.2.20220331
Apr 02 11:09:35 node-9.domain.tld kernel: openvswitch: LISP tunneling driver
Apr 02 11:09:35 node-9.domain.tld kernel: openvswitch: STT tunneling driver
Apr 02 11:09:35 node-9.domain.tld mlnx_conf_mgr[3106]: INFO: No configurations found for mlx5_1, skipping.
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + case "$(echo ${i} | tr '[:upper:]' '[:lower:]')" in
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ echo ib1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ tr '[:upper:]' '[:lower:]'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ /bin/ls -1 '/etc/sysconfig/network-scripts/ifcfg-ib1.[0-9a-fA-F]*'
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: + CHILD_CONFS=
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ cut -d: -f1
Apr 02 11:09:35 node-9.domain.tld mlnx_interface_mgr.sh[2987]: ++ grep -E '=\s*"*ib1\.[0-9a-fA-F]*"*\s*$' /etc/sysconfig/network-scripts/ifcfg-br-mgmt /etc/sysconfig/network-scripts/ifcfg-br-prv /etc/sysconfig/network-scripts/ifcfg-br-roller /etc/sysconfig/network-scripts/ifcfg-br-storagepub /etc/sysconfig/network-scripts/ifcfg-br-vxlan /etc/sysconfig/network-scripts/ifcfg-enp0s20f0u1u6 /etc/sysconfig/network-scripts/ifcfg-ens27f0np0 /etc/sysconfig/network-scripts/ifcfg-ens27f1np1 /etc/sysconfig/network-scripts/ifcfg-ens2f0 /etc/sysconfig/network-scripts/ifcfg-ens2f1 /etc/sysconfig/network-scripts/ifcfg-ib0 /etc/sysconfig/network-scripts/ifcfg-ib1 /etc/sysconfig/network-scripts/ifcfg-lo /etc/sysconfig/network-scripts/ifdown /etc/sysconfig/network-scripts/ifdown-bnep /etc/sysconfig/network-scripts/ifdown-eth /etc/sysconfig/network-scripts/ifdown-ippp /etc/sysconfig/network-scripts/ifdown-ipv6 /etc/sysconfig/network-scripts/ifdown-isdn /etc/sysconfig/network-scripts/ifdown-post /etc/sysconfig/network-scripts/ifdown-routes /etc/sysconfig/network-scripts/ifdown-sit /etc/sysconfig/network-scripts/ifdown-tunnel /etc/sysconfig/network-scripts/ifup /etc/sysconfig/network-scripts/ifup-aliases /etc/sysconfig/network-scripts/ifup-bnep /etc/sysconfig/network-scripts/ifup-eth /etc/sysconfig/network-scripts/ifup-ippp /etc/sysconfig/network-scripts/ifup-ipv6 /etc/sysconfig/network-scripts/ifup-isdn /etc/sysconfig/network-scripts/ifup-plip /etc/sysconfig/network-scripts/ifup-plusb /etc/sysconfig/network-scripts/ifup-post /etc/sysconfig/network-scripts/ifup-routes /etc/sysconfig/network-scripts/ifup-sit /etc/sysconfig/network-scripts/ifup-tunnel /etc/sysconfig/network-scripts/ifup-wireless /etc/sysconfig/network-scripts/init.ipv6-global /etc/sysconfig/network-scripts/network-functions /etc/sysconfig/network-scripts/network-functions-ipv6 /etc/sysconfig/network-scripts/route-br-mgmt
Apr 02 11:09:35 node-9.domain.tld systemd[1]: mlnx_interface_mgr@ib1.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit mlnx_interface_mgr@ib1.service has successfully entered the 'dead' state.
Apr 02 11:09:35 node-9.domain.tld ovs-ctl[2919]: Inserting openvswitch module [  OK  ]
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld kernel: device ovs-system entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for ovs-system: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: Timeout policy base is empty
Apr 02 11:09:35 node-9.domain.tld kernel: Failed to associated timeout policy `ovs_test_tp'
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: Could not generate persistent MAC address for br-mgmt: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device br-mgmt entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device br-Bond1 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld kernel: device ens27f1np1 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: Could not generate persistent MAC address for br-Bond1: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device ens2f0 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device br-vm-roller entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: Could not generate persistent MAC address for br-vm-roller: No such file or directory
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: Could not generate persistent MAC address for br-storagepub: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device br-storagepub entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device br-vxlan entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for br-vxlan: No such file or directory
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: Could not generate persistent MAC address for br-roller: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device br-roller entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device br-prv entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for br-prv: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: i40e 0000:98:00.0 ens2f0: port 6081 already offloaded
Apr 02 11:09:35 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: port 6081 already offloaded
Apr 02 11:09:35 node-9.domain.tld kernel: device genev_sys_6081 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1460]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1460]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1460]: Could not generate persistent MAC address for genev_sys_6081: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device mirror0 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for mirror0: No such file or directory
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: Could not generate persistent MAC address for br-int: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device br-int entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device ovn0 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1421]: Could not generate persistent MAC address for ovn0: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device br-ex entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for br-ex: No such file or directory
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for br-vm-mgmt: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device br-vm-mgmt entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device br-vm-storpub entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1496]: Could not generate persistent MAC address for br-vm-storpub: No such file or directory
Apr 02 11:09:35 node-9.domain.tld kernel: device ens27f0np0 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device ens2f1 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld kernel: device br-Bond0 entered promiscuous mode
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:09:35 node-9.domain.tld systemd-udevd[1596]: Could not generate persistent MAC address for br-Bond0: No such file or directory
Apr 02 11:09:35 node-9.domain.tld ovs-ctl[2919]: Starting ovs-vswitchd [  OK  ]
Apr 02 11:09:35 node-9.domain.tld ovs-vsctl[3250]: ovs|00001|vsctl|INFO|Called as ovs-vsctl --no-wait add Open_vSwitch . external-ids hostname=node-9.domain.tld
Apr 02 11:09:35 node-9.domain.tld ovs-ctl[2919]: Enabling remote OVSDB managers [  OK  ]
Apr 02 11:09:35 node-9.domain.tld systemd[1]: Started Open vSwitch Forwarding Unit.
-- Subject: Unit ovs-vswitchd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ovs-vswitchd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:35 node-9.domain.tld systemd[1]: Starting Open vSwitch...
-- Subject: Unit openvswitch.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit openvswitch.service has begun starting up.
Apr 02 11:09:35 node-9.domain.tld systemd[1]: Started Open vSwitch.
-- Subject: Unit openvswitch.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit openvswitch.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:35 node-9.domain.tld systemd[1]: Starting LSB: Bring up/down networking...
-- Subject: Unit network.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit network.service has begun starting up.
Apr 02 11:09:36 node-9.domain.tld systemd[1]: Listening on Load/Save RF Kill Switch Status /dev/rfkill Watch.
-- Subject: Unit systemd-rfkill.socket has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-rfkill.socket has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:36 node-9.domain.tld kernel: cfg80211: Loading compiled-in X.509 certificates for regulatory database
Apr 02 11:09:36 node-9.domain.tld kernel: cfg80211: Loaded X.509 cert 'sforshee: 00b28ddf47aef9cea7'
Apr 02 11:09:36 node-9.domain.tld network[3256]: Bringing up loopback interface:  [  OK  ]
Apr 02 11:09:36 node-9.domain.tld network[3256]: Bringing up interface br-mgmt:  [  OK  ]
Apr 02 11:09:37 node-9.domain.tld network[3256]: Bringing up interface br-roller:  [  OK  ]
Apr 02 11:09:37 node-9.domain.tld network[3256]: Bringing up interface br-storagepub:  [  OK  ]
Apr 02 11:09:37 node-9.domain.tld network[3256]: Bringing up interface enp0s20f0u1u6:  [  OK  ]
Apr 02 11:09:37 node-9.domain.tld kernel: i40e 0000:98:00.0 ens2f0: port 6081 already offloaded
Apr 02 11:09:37 node-9.domain.tld kernel: i40e 0000:98:00.0 ens2f0: port 6081 already offloaded
Apr 02 11:09:37 node-9.domain.tld network[3256]: Bringing up interface ens2f0:  [  OK  ]
Apr 02 11:09:38 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: changing MTU from 1500 to 1600
Apr 02 11:09:38 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: port 6081 already offloaded
Apr 02 11:09:38 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: port 6081 already offloaded
Apr 02 11:09:38 node-9.domain.tld network[3256]: Bringing up interface ens2f1:  [  OK  ]
Apr 02 11:09:38 node-9.domain.tld kernel: bnxt_en 0000:67:00.0 ens27f0np0: NIC Link is Up, 10000 Mbps full duplex, Flow control: ON - receive
Apr 02 11:09:38 node-9.domain.tld kernel: bnxt_en 0000:67:00.0 ens27f0np0: FEC autoneg off encodings: None
Apr 02 11:09:38 node-9.domain.tld network[3256]: Bringing up interface ens27f0np0:  [  OK  ]
Apr 02 11:09:38 node-9.domain.tld kernel: bnxt_en 0000:67:00.1 ens27f1np1: NIC Link is Up, 10000 Mbps full duplex, Flow control: ON - receive
Apr 02 11:09:38 node-9.domain.tld kernel: bnxt_en 0000:67:00.1 ens27f1np1: FEC autoneg off encodings: None
Apr 02 11:09:38 node-9.domain.tld network[3256]: Bringing up interface ens27f1np1:  [  OK  ]
Apr 02 11:09:40 node-9.domain.tld chronyd[2412]: Selected source 192.168.11.2
Apr 02 11:09:40 node-9.domain.tld chronyd[2412]: System clock TAI offset set to 37 seconds
Apr 02 11:09:43 node-9.domain.tld network[3256]: Bringing up interface ib0:  [  OK  ]
Apr 02 11:09:48 node-9.domain.tld network[3256]: Bringing up interface ib1:  [  OK  ]
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started LSB: Bring up/down networking.
-- Subject: Unit network.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit network.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Reached target Network.
-- Subject: Unit network.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit network.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Dynamic System Tuning Daemon...
-- Subject: Unit tuned.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit tuned.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting IPMI Driver...
-- Subject: Unit ipmi.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ipmi.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting OpenSSH server daemon...
-- Subject: Unit sshd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sshd.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Celery Service...
-- Subject: Unit coaster-celery-agent.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-celery-agent.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting GSSAPI Proxy Daemon...
-- Subject: Unit gssproxy.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit gssproxy.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting titanagent...
-- Subject: Unit titanagent.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Ethernet irq affinity scripts.
-- Subject: Unit ethernet-irq-affinity.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ethernet-irq-affinity.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Reached target Network is Online.
-- Subject: Unit network-online.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit network-online.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld sshd[4546]: Server listening on 0.0.0.0 port 22.
Apr 02 11:09:48 node-9.domain.tld sshd[4546]: Server listening on :: port 22.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Notify NFS peers of a restart...
-- Subject: Unit rpc-statd-notify.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rpc-statd-notify.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld set-smp.sh[4561]: x86_64 arch not supported
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Map RBD devices...
-- Subject: Unit rbdmap.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rbdmap.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld sm-notify[4569]: Version 2.3.3 starting
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting System Logging Service...
-- Subject: Unit rsyslog.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rsyslog.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started NvfILE Helperd.
-- Subject: Unit nvfile-helperd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit nvfile-helperd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started OpenSSH server daemon.
-- Subject: Unit sshd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sshd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started IPMI Driver.
-- Subject: Unit ipmi.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ipmi.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started GSSAPI Proxy Daemon.
-- Subject: Unit gssproxy.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit gssproxy.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld rsyslogd[4582]: [origin software="rsyslogd" swVersion="8.1911.0-7.es8.2" x-pid="4582" x-info="https://www.rsyslog.com"] start
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started System Logging Service.
-- Subject: Unit rsyslog.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rsyslog.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Notify NFS peers of a restart.
-- Subject: Unit rpc-statd-notify.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rpc-statd-notify.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Map RBD devices.
-- Subject: Unit rbdmap.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit rbdmap.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting stop ecr shim process...
-- Subject: Unit stop_ecr.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit stop_ecr.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Reached target NFS client services.
-- Subject: Unit nfs-client.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit nfs-client.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Reached target Remote File Systems (Pre).
-- Subject: Unit remote-fs-pre.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit remote-fs-pre.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Reached target Remote File Systems.
-- Subject: Unit remote-fs.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit remote-fs.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Crash recovery kernel arming...
-- Subject: Unit kdump.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit kdump.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Permit User Sessions...
-- Subject: Unit systemd-user-sessions.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-user-sessions.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Roller Coaster Agent service.
-- Subject: Unit coaster-agent.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-agent.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Start NvfILE Client...
-- Subject: Unit nvfile-client.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit nvfile-client.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started stop ecr shim process.
-- Subject: Unit stop_ecr.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit stop_ecr.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Permit User Sessions.
-- Subject: Unit systemd-user-sessions.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-user-sessions.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld nvfile-client[4603]: Starting NvfILE Client:
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Terminate Plymouth Boot Screen...
-- Subject: Unit plymouth-quit.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-quit.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld nvfile-client[4603]: - Loading NvfILE modules
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Hold until boot process finishes up...
-- Subject: Unit plymouth-quit-wait.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-quit-wait.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Command Scheduler.
-- Subject: Unit crond.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit crond.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld crond[4619]: (CRON) STARTUP (1.5.2)
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting containerd container runtime...
-- Subject: Unit containerd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit containerd.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld crond[4619]: (CRON) INFO (RANDOM_DELAY will be scaled with factor 41% if used.)
Apr 02 11:09:48 node-9.domain.tld crond[4619]: (CRON) INFO (running with inotify support)
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Received SIGRTMIN+21 from PID 852 (plymouthd).
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Received SIGRTMIN+21 from PID 852 (plymouthd).
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: modprobe(4614): File system registered. Type: nvfile. Version: 7.2.8
Apr 02 11:09:48 node-9.domain.tld nvfile-client[4603]: - Mounting directories from /etc/nvfile/nvfile-mounts.conf
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Terminate Plymouth Boot Screen.
-- Subject: Unit plymouth-quit.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-quit.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Hold until boot process finishes up.
-- Subject: Unit plymouth-quit-wait.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit plymouth-quit-wait.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Getty on tty1.
-- Subject: Unit getty@tty1.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit getty@tty1.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Reached target Login Prompts.
-- Subject: Unit getty.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit getty.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started containerd container runtime.
-- Subject: Unit containerd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit containerd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Starting Kubernetes Kubelet Server...
-- Subject: Unit kubelet.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit kubelet.service has begun starting up.
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + kubeenvfile=/etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + memoryfile=/etc/kubernetes/memory.yml
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + checkexist /etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + dstfile=/etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' '!' -f /etc/kubernetes/kubelet.env ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + checksize /etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + dstfile=/etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ du -b /etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print $1}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + size=2371
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 2371 -eq 0 ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + checkexist /etc/kubernetes/memory.yml
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + dstfile=/etc/kubernetes/memory.yml
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' '!' -f /etc/kubernetes/memory.yml ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ cat /etc/kubernetes/memory.yml
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk -F: '{print $2}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ grep kubepods_memory_limits
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + kubepods_memory_mb=
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' x = x ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ cat /etc/kubernetes/memory.yml
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ grep compute_reserved_memory
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk -F: '{print $2}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + kubepods_memory_mb=1021265
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 1021265x = x ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ cat /proc/meminfo
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ grep MemTotal
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print$2}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + memory_kb=1056261160
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ expr 1056261160 / 1024
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + memory_mb=1031505
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 1031505x = x ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + hp_mb=0
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ cat /proc/meminfo
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ grep HugePages_Total
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print$2}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + hp_total=0
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ cat /proc/meminfo
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ grep Hugepagesize
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print$2}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + hp_size=2048
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 2048 -ne 0 ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 0 -ne 0 ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ expr 1031505 - 1021265 - 0
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: enabling unsafe global rkey
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + system_reserved_mb=10240
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 10240 -lt 0 ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + tmpenvfile=/tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + cp -f /etc/kubernetes/kubelet.env /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + key=system-reserved
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + cat /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + grep system-reserved
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: --enforce-node-allocatable=pods  --cluster-dns=10.222.0.3 --cluster-domain=cluster.local --resolv-conf=/etc/resolv.conf --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml --system-reserved cpu=1024m,memory=10240Mi --kubepods-cpuset.cpus=0-15,32-47,16-31,48-63 --kubepods-cpuset.mems=0-1  --feature-gates=ExpandPersistentVolumes=True,GracefulNodeShutdown=true,PodOverhead=True   --eviction-hard=memory.available<512Mi  --eviction-soft-grace-period=memory.available=90s  --eviction-max-pod-grace-period=120  --runtime-request-timeout=10m  --eviction-soft=memory.available<2048Mi "
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + sed -i -r '/system-reserved/s/memory=[0-9]+/memory=10240/' /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: enabling unsafe global rkey
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ cat /sys/fs/cgroup/cpuset/cpuset.mems
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + top_cpuset_mems=0-1
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + key=kubepods-cpuset.mems
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + cat /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + grep kubepods-cpuset.mems
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: --enforce-node-allocatable=pods  --cluster-dns=10.222.0.3 --cluster-domain=cluster.local --resolv-conf=/etc/resolv.conf --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml --system-reserved cpu=1024m,memory=10240Mi --kubepods-cpuset.cpus=0-15,32-47,16-31,48-63 --kubepods-cpuset.mems=0-1  --feature-gates=ExpandPersistentVolumes=True,GracefulNodeShutdown=true,PodOverhead=True   --eviction-hard=memory.available<512Mi  --eviction-soft-grace-period=memory.available=90s  --eviction-max-pod-grace-period=120  --runtime-request-timeout=10m  --eviction-soft=memory.available<2048Mi "
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + sed -i -E 's/kubepods-cpuset.mems=[^ ]*/kubepods-cpuset.mems=0-1/' /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: enabling unsafe global rkey
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + checksize /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + dstfile=/tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ du -b /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print $1}'
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: enabling unsafe global rkey
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + size=2371
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 2371 -eq 0 ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ md5sum /tmp/env-kubelet
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print $1}'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + tmpmd5=58a1a365dc7a508c2d774b444923acfd
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ md5sum /etc/kubernetes/kubelet.env
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: ++ awk '{print $1}'
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: enabling unsafe global rkey
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + envmd5=58a1a365dc7a508c2d774b444923acfd
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + '[' 58a1a365dc7a508c2d774b444923acfd = 58a1a365dc7a508c2d774b444923acfd ']'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + echo 'no need update'
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: no need update
Apr 02 11:09:48 node-9.domain.tld kubereversecalc[4673]: + exit 0
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Kubernetes Kubelet Server.
-- Subject: Unit kubelet.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit kubelet.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld rsyslogd[4582]: imjournal: journal files changed, reloading...  [v8.1911.0-7.es8.2 try https://www.rsyslog.com/e/0 ]
Apr 02 11:09:48 node-9.domain.tld kernel: nvfile: mount(4696): NvfILE mount ready.
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Start NvfILE Client.
-- Subject: Unit nvfile-client.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit nvfile-client.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.725645446+08:00" level=info msg="starting containerd" revision= version=1.5.9
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.791439645+08:00" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.792236608+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.aufs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795090202+08:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.aufs\"..." error="aufs is not supported (modprobe aufs failed: exit status 1 \"modprobe: FATAL: Module aufs not found in directory /lib/modules/4.18.0-147.5.1.es8_24.x86_64\\n\"): skip plugin" type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795138541+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795744191+08:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.btrfs (xfs) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795777016+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.devmapper\"..." type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795845301+08:00" level=warning msg="failed to load plugin io.containerd.snapshotter.v1.devmapper" error="devmapper not configured"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795866299+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.795923237+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.796105865+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.zfs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.796311387+08:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.zfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.796341294+08:00" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.796394167+08:00" level=warning msg="could not use snapshotter devmapper in metadata plugin" error="devmapper not configured"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.796414600+08:00" level=info msg="metadata content store policy set" policy=shared
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.799879900+08:00" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.799924194+08:00" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800032176+08:00" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800107087+08:00" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800137220+08:00" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800162938+08:00" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800197979+08:00" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800223015+08:00" level=info msg="loading plugin \"io.containerd.service.v1.leases-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800252130+08:00" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800277021+08:00" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800304640+08:00" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800446758+08:00" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.800545259+08:00" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801261317+08:00" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801304302+08:00" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801397726+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801424647+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801449921+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801473388+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801498207+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801525513+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801547957+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801569605+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801592286+08:00" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801688984+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801718068+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801750471+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.801771272+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.802223324+08:00" level=warning msg="`mirrors` is deprecated, please use `config_path` instead"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.802242762+08:00" level=warning msg="`configs.tls` is deprecated, please use `config_path` instead"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.802403895+08:00" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} UntrustedWorkloadRuntime:{Type: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} Runtimes:map[ecr:{Type:io.containerd.ecr.v2 Engine: PodAnnotations:[io.katacontainers.*] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:true BaseRuntimeSpec:} kata:{Type:io.containerd.kata.v2 Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} runc:{Type:io.containerd.runc.v2 Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate:} Registry:{ConfigPath: Mirrors:map[docker.io:{Endpoints:[https://registry-1.docker.io]} dockerhub.citicsinfo.com:{Endpoints:[https://dockerhub.citicsinfo.com]} hub.easystack.io:{Endpoints:[https://hub.easystack.io]} hub.ecns.io:{Endpoints:[https://hub.ecns.io]}] Configs:map[dockerhub.citicsinfo.com:{Auth:<nil> TLS:0xc000731ac0} hub.easystack.io:{Auth:<nil> TLS:0xc000731bc0} hub.ecns.io:{Auth:<nil> TLS:0xc000731cc0}] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:hub.easystack.io/captain/pause-amd64:3.0 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:5 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/var/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/var/run/containerd/io.containerd.grpc.v1.cri}"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.802563363+08:00" level=info msg="Connect containerd service"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.802848220+08:00" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.807330331+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.810355828+08:00" level=info msg="Start subscribing containerd event"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.811219128+08:00" level=info msg="Start recovering state"
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.812085674+08:00" level=info msg=serving... address=/var/run/containerd/debug.sock
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.812227170+08:00" level=info msg=serving... address=/var/run/containerd/containerd.sock.ttrpc
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.812332223+08:00" level=info msg=serving... address=/var/run/containerd/containerd.sock
Apr 02 11:09:48 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:48.812797855+08:00" level=info msg="containerd successfully booted in 0.089968s"
Apr 02 11:09:48 node-9.domain.tld systemd[1]: Started Dynamic System Tuning Daemon.
-- Subject: Unit tuned.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit tuned.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:48 node-9.domain.tld bash[4560]: agent has quit
Apr 02 11:09:48 node-9.domain.tld coaster-startup-checker[4553]: The connection to the server 127.0.0.1:8443 was refused - did you specify the right host or port?
Apr 02 11:09:49 node-9.domain.tld sh[4589]: celery multi v3.1.19 (Cipater)
Apr 02 11:09:49 node-9.domain.tld sh[4589]: > Starting nodes...
Apr 02 11:09:49 node-9.domain.tld sh[4589]:         > celery_9@celery: OK
Apr 02 11:09:49 node-9.domain.tld systemd[1]: Started Celery Service.
-- Subject: Unit coaster-celery-agent.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-celery-agent.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --image-gc-whitelist has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --system-reserved has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --kubepods-cpuset.cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --kubepods-cpuset.mems has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --feature-gates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-hard has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-soft-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-soft has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: W0402 11:09:49.100715    4781 server.go:191] Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --image-gc-whitelist has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --system-reserved has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --kubepods-cpuset.cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --kubepods-cpuset.mems has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --feature-gates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-hard has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-soft-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --eviction-soft has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 02 11:09:49 node-9.domain.tld systemd[1]: Started Kubernetes systemd probe.
-- Subject: Unit run-r369c91459e4b420bacad85cd7a44f1f4.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit run-r369c91459e4b420bacad85cd7a44f1f4.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: I0402 11:09:49.127844    4781 server.go:416] Version: v1.20.14-es
Apr 02 11:09:49 node-9.domain.tld systemd[1]: run-r369c91459e4b420bacad85cd7a44f1f4.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-r369c91459e4b420bacad85cd7a44f1f4.scope has successfully entered the 'dead' state.
Apr 02 11:09:49 node-9.domain.tld kubelet[4781]: I0402 11:09:49.230207    4781 dynamic_cafile_content.go:167] Starting client-ca-bundle::/etc/kubernetes/ssl/ca.pem
Apr 02 11:09:49 node-9.domain.tld systemd[1]: titanagent.service: Can't open PID file /var/run/titanagent.pid (yet?) after start: No such file or directory
Apr 02 11:09:49 node-9.domain.tld systemd[1]: Started titanagent.
-- Subject: Unit titanagent.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:49 node-9.domain.tld systemd[1]: Reached target Multi-User System.
-- Subject: Unit multi-user.target has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit multi-user.target has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:49 node-9.domain.tld systemd[1]: Starting Update UTMP about System Runlevel Changes...
-- Subject: Unit systemd-update-utmp-runlevel.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-update-utmp-runlevel.service has begun starting up.
Apr 02 11:09:49 node-9.domain.tld systemd[1]: systemd-update-utmp-runlevel.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit systemd-update-utmp-runlevel.service has successfully entered the 'dead' state.
Apr 02 11:09:49 node-9.domain.tld systemd[1]: Started Update UTMP about System Runlevel Changes.
-- Subject: Unit systemd-update-utmp-runlevel.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit systemd-update-utmp-runlevel.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:49.901119776+08:00" level=info msg="Start event monitor"
Apr 02 11:09:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:49.901206495+08:00" level=info msg="Start snapshots syncer"
Apr 02 11:09:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:49.901234705+08:00" level=info msg="Start cni network conf syncer"
Apr 02 11:09:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:09:49.901259356+08:00" level=info msg="Start streaming server"
Apr 02 11:09:50 node-9.domain.tld kdumpctl[4599]: kdump: kexec: loaded kdump kernel
Apr 02 11:09:50 node-9.domain.tld kdumpctl[4599]: kdump: Starting kdump: [OK]
Apr 02 11:09:50 node-9.domain.tld systemd[1]: Started Crash recovery kernel arming.
-- Subject: Unit kdump.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit kdump.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:09:50 node-9.domain.tld systemd[1]: Startup finished in 6.982s (kernel) + 4.280s (initrd) + 18.958s (userspace) = 30.221s.
-- Subject: System start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- All system services necessary queued for starting at boot have been
-- started. Note that this does not mean that the machine is now idle as services
-- might still be busy with completing start-up.
-- 
-- Kernel start-up required 6982153 microseconds.
-- 
-- Initial RAM disk start-up required 4280733 microseconds.
-- 
-- Userspace start-up required 18958198 microseconds.
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:09:50 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:50.432 4602 ERROR coaster_agent.raidtools [-] Current raidtools cannot support this kind of raid card!
Apr 02 11:09:50 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:50.451 4602 INFO coaster_agent.agent [-] Coaster Agent running now...
Apr 02 11:09:50 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:50.452 4602 INFO coaster_agent.agent [-] Finished start iface_monitor thread.
Apr 02 11:09:50 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:50.454 4602 INFO coaster_agent.agent [-] Finished start report_runner thread.
Apr 02 11:09:50 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:50.455 4602 INFO coaster_agent.agent [-] Finished start kernel_monitor thread.
Apr 02 11:09:50 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:50.879 4602 WARNING coaster_agent.iface [-] Get nic slot: 00:14.0-1.6 device info with exception: Command 'timeout 5s /usr/sbin/lspci|grep '00:14.0-1.6'' returned non-zero exit status 1
Apr 02 11:09:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:51.616 4602 WARNING coaster_agent.iface [-] Get nic slot: 00:14.0-1.6 device info with exception: Command 'timeout 5s /usr/sbin/lspci|grep '00:14.0-1.6'' returned non-zero exit status 1
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.657 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.658 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.658 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.658 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.659 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.659 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.659 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.659 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.660 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.660 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.660 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.660 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.661 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.661 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.661 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:09:52.661 4602 WARNING coaster_agent.utils [-] No is not float. Return 0.0
Apr 02 11:09:54 node-9.domain.tld root[5634]: openibd: Set node_desc for mlx5_0: node-9 HCA-1
Apr 02 11:09:54 node-9.domain.tld root[5636]: openibd: Set node_desc for mlx5_1: node-9 HCA-2
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Starting system activity accounting tool...
-- Subject: Unit sysstat-collect.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysstat-collect.service has begun starting up.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Started Session 1 of user root.
-- Subject: Unit session-1.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-1.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Created slice system-user\x2druntime\x2ddir.slice.
-- Subject: Unit system-user\x2druntime\x2ddir.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit system-user\x2druntime\x2ddir.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: sysstat-collect.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit sysstat-collect.service has successfully entered the 'dead' state.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Started system activity accounting tool.
-- Subject: Unit sysstat-collect.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sysstat-collect.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Startup finished in 64ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 64598 microseconds.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld CROND[5686]: (root) CMD (flock -xn /tmp/drop_cache.lock /usr/bin/drop_cache.sh >> /var/log/drop_cache.log)
Apr 02 11:10:01 node-9.domain.tld crond[4619]: postdrop: warning: unable to look up public/pickup: No such file or directory
Apr 02 11:10:01 node-9.domain.tld postfix/postdrop[5699]: warning: unable to look up public/pickup: No such file or directory
Apr 02 11:10:01 node-9.domain.tld systemd[1]: session-1.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-1.scope has successfully entered the 'dead' state.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:01 node-9.domain.tld systemd[5675]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:01 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:02 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:02 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:03 node-9.domain.tld kernel: nvidia-uvm: Loaded the UVM driver, major device number 509.
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.414307    4781 server.go:645] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.415799    4781 container_manager_linux.go:274] container manager verified user specified cgroup-root exists: []
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.415849    4781 container_manager_linux.go:279] Creating Container Manager object based on Node Config: {RuntimeCgroupsName:/systemd/system.slice SystemCgroupsName: KubeletCgroupsName:/systemd/system.slice ContainerRuntime:remote CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: KubePodsCpusetCpus:0-15,32-47,16-31,48-63 KubePodsCpusetMems:0-1 EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[cpu:{i:{value:1024 scale:-3} d:{Dec:<nil>} s:1024m Format:DecimalSI} memory:{i:{value:10737418240 scale:0} d:{Dec:<nil>} s: Format:BinarySI}] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:512Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalTopologyManagerScope:container ExperimentalCPUManagerReconcilePeriod:10s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none}
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.416587    4781 topology_manager.go:120] [topologymanager] Creating topology manager with none policy per container scope
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.416601    4781 container_manager_linux.go:310] [topologymanager] Initializing Topology Manager with none policy and container-level scope
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.416607    4781 container_manager_linux.go:315] Creating device plugin manager: true
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.417581    4781 remote_runtime.go:62] parsed scheme: ""
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.417593    4781 remote_runtime.go:62] scheme "" not registered, fallback to default scheme
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.418568    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.418581    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.418625    4781 remote_image.go:50] parsed scheme: ""
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.418633    4781 remote_image.go:50] scheme "" not registered, fallback to default scheme
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.418642    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.418647    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.421743    4781 kubelet.go:394] Attempting to sync node with API server
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.422380    4781 kubelet.go:262] Adding pod path: /etc/kubernetes/manifests
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.422894    4781 kubelet.go:273] Adding apiserver pod source
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.422915    4781 apiserver.go:43] Waiting for node sync before watching apiserver pods
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: E0402 11:10:03.424580    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dnode-9&limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: E0402 11:10:03.424608    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://localhost:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:03 node-9.domain.tld kubelet[4781]: I0402 11:10:03.430303    4781 kuberuntime_manager.go:216] Container runtime containerd initialized, version: 1.5.9, apiVersion: v1alpha2
Apr 02 11:10:04 node-9.domain.tld kubelet[4781]: E0402 11:10:04.520997    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://localhost:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:04 node-9.domain.tld kubelet[4781]: E0402 11:10:04.748252    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dnode-9&limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:06 node-9.domain.tld kubelet[4781]: E0402 11:10:06.638316    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://localhost:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:07 node-9.domain.tld kubelet[4781]: E0402 11:10:07.568880    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dnode-9&limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.649216    4781 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]:         For verbose messaging see aws.Config.CredentialsChainVerboseErrors
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.652577    4781 server.go:1178] Started kubelet
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.653783    4781 cri_stats_provider.go:376] Failed to get the info of the filesystem with mountpoint "/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs": unable to find data in memory cache.
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.653821    4781 kubelet.go:1292] Image garbage collection failed once. Stats initialization may not have completed yet: invalid capacity 0 on image filesystem
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.654082    4781 server.go:148] Starting to listen on 10.10.11.12:10250
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.655150    4781 fs_resource_analyzer.go:64] Starting FS ResourceAnalyzer
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.657033    4781 volume_manager.go:279] Starting Kubelet Volume Manager
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.658625    4781 desired_state_of_world_populator.go:142] Desired state populator starts to run
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.662231    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://localhost:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.662364    4781 controller.go:144] failed to ensure lease exists, will retry in 200ms, error: Get "https://localhost:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node-9?timeout=10s": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.663584    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 2f5a63bbbf28ee17770d0b614a5c066e44b885801f20c4ec46249c4cf4b79db8
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.663619    4781 client.go:86] parsed scheme: "unix"
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.663664    4781 client.go:86] scheme "unix" not registered, fallback to default scheme
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.663458    4781 event.go:273] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"node-9.17c257dbbe1244fa", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"node-9", UID:"node-9", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"node-9"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc17afba466e4dafa, ext:20957433091, loc:(*time.Location)(0x71051a0)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc17afba466e4dafa, ext:20957433091, loc:(*time.Location)(0x71051a0)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://localhost:8443/api/v1/namespaces/default/events": dial tcp 127.0.0.1:8443: connect: connection refused'(may retry after sleeping)
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.663866    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.663896    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.665780    4781 server.go:414] Adding debug handlers to kubelet server.
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.667006851+08:00" level=info msg="RemoveContainer for \"2f5a63bbbf28ee17770d0b614a5c066e44b885801f20c4ec46249c4cf4b79db8\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.671068737+08:00" level=info msg="RemoveContainer for \"2f5a63bbbf28ee17770d0b614a5c066e44b885801f20c4ec46249c4cf4b79db8\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.671318    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: c70c01a7dff0fd4a0b02b3037c577054622eee1b0ce24b519411a48a56dd07fc
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.672902329+08:00" level=info msg="RemoveContainer for \"c70c01a7dff0fd4a0b02b3037c577054622eee1b0ce24b519411a48a56dd07fc\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.676054040+08:00" level=info msg="RemoveContainer for \"c70c01a7dff0fd4a0b02b3037c577054622eee1b0ce24b519411a48a56dd07fc\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.676268    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 66999488694dfbc6ee8c84897ff14a0312b5aa93b9d567493a840e4977d8514a
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.679227190+08:00" level=info msg="RemoveContainer for \"66999488694dfbc6ee8c84897ff14a0312b5aa93b9d567493a840e4977d8514a\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.682479134+08:00" level=info msg="RemoveContainer for \"66999488694dfbc6ee8c84897ff14a0312b5aa93b9d567493a840e4977d8514a\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.688050454+08:00" level=info msg="StopPodSandbox for \"d2a2bbd575cb669b2dce9714cc1e2c4cd5081452144ae5be8d53bf9e089f6a65\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.688368369+08:00" level=info msg="TearDown network for sandbox \"d2a2bbd575cb669b2dce9714cc1e2c4cd5081452144ae5be8d53bf9e089f6a65\" successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.688405110+08:00" level=info msg="StopPodSandbox for \"d2a2bbd575cb669b2dce9714cc1e2c4cd5081452144ae5be8d53bf9e089f6a65\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.688780361+08:00" level=info msg="RemovePodSandbox for \"d2a2bbd575cb669b2dce9714cc1e2c4cd5081452144ae5be8d53bf9e089f6a65\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.691609109+08:00" level=info msg="RemovePodSandbox \"d2a2bbd575cb669b2dce9714cc1e2c4cd5081452144ae5be8d53bf9e089f6a65\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.692047194+08:00" level=info msg="StopPodSandbox for \"f0aecffd2f510cdc1cb628fd0076527264aab3af4899560a86d0df4f40c19cae\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.692230681+08:00" level=info msg="TearDown network for sandbox \"f0aecffd2f510cdc1cb628fd0076527264aab3af4899560a86d0df4f40c19cae\" successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.692256891+08:00" level=info msg="StopPodSandbox for \"f0aecffd2f510cdc1cb628fd0076527264aab3af4899560a86d0df4f40c19cae\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.692607345+08:00" level=info msg="RemovePodSandbox for \"f0aecffd2f510cdc1cb628fd0076527264aab3af4899560a86d0df4f40c19cae\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.698012181+08:00" level=info msg="RemovePodSandbox \"f0aecffd2f510cdc1cb628fd0076527264aab3af4899560a86d0df4f40c19cae\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.698486594+08:00" level=info msg="StopPodSandbox for \"3423b2b68ae433bdd675fcb0cccb86275498ee80e14d94a1eb9de9f4baa9fb4f\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.698703649+08:00" level=info msg="TearDown network for sandbox \"3423b2b68ae433bdd675fcb0cccb86275498ee80e14d94a1eb9de9f4baa9fb4f\" successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.698747598+08:00" level=info msg="StopPodSandbox for \"3423b2b68ae433bdd675fcb0cccb86275498ee80e14d94a1eb9de9f4baa9fb4f\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.699075493+08:00" level=info msg="RemovePodSandbox for \"3423b2b68ae433bdd675fcb0cccb86275498ee80e14d94a1eb9de9f4baa9fb4f\""
Apr 02 11:10:09 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:09.701666918+08:00" level=info msg="RemovePodSandbox \"3423b2b68ae433bdd675fcb0cccb86275498ee80e14d94a1eb9de9f4baa9fb4f\" returns successfully"
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.758820    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.761252    4781 kubelet_node_status.go:71] Attempting to register node node-9
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.762230    4781 kubelet_node_status.go:93] Unable to register node "node-9" with API server: Post "https://localhost:8443/api/v1/nodes": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.787006    4781 cpu_manager.go:193] [cpumanager] starting with none policy
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.787033    4781 cpu_manager.go:194] [cpumanager] reconciling every 10s
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.787061    4781 state_mem.go:36] [cpumanager] initializing new in-memory state store
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.787608    4781 state_mem.go:88] [cpumanager] updated default cpuset: ""
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.787634    4781 state_mem.go:96] [cpumanager] updated cpuset assignments: "map[]"
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.787656    4781 policy_none.go:43] [cpumanager] none policy: Start
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.806551    4781 plugin_manager.go:114] Starting Kubelet Plugin Manager
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.807407    4781 eviction_manager.go:260] eviction manager: failed to get summary stats: failed to get node info: node "node-9" not found
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.814282    4781 kubelet_network_linux.go:56] Initialized IPv4 iptables rules.
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.814372    4781 status_manager.go:158] Starting to sync pod status with apiserver
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.814426    4781 kubelet.go:1829] Starting kubelet main sync loop.
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.814523    4781 kubelet.go:1853] skipping pod synchronization - PLEG is not healthy: pleg has yet to be successful
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.816626    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://localhost:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.858950    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.864190    4781 controller.go:144] failed to ensure lease exists, will retry in 400ms, error: Get "https://localhost:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node-9?timeout=10s": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.914710    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.917135    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.921215    4781 pod_container_deletor.go:79] Container "21cf4877a78a0e7b23b59b3d5f43b268d770d1e08cae40cd881720d71b9bf659" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.921260    4781 pod_container_deletor.go:79] Container "89bb258cbebc7e7dac3128e4f39e7d98fa8651001fc7b4de7678a84288139d7b" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.921286    4781 pod_container_deletor.go:79] Container "caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.921997    4781 status_manager.go:550] Failed to get status for pod "kube-proxy-node-9_kube-system(a27e5e09aca8f8046e3e245f3758bfa1)": Get "https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-proxy-node-9": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.924576    4781 status_manager.go:550] Failed to get status for pod "nginx-proxy-node-9_kube-system(53da5e67b54f38828d0097c82bd5d465)": Get "https://localhost:8443/api/v1/namespaces/kube-system/pods/nginx-proxy-node-9": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.925659    4781 pod_container_deletor.go:79] Container "1390743c8ef4bb263186c880d95ad5a57fff4a7132b1dbae74cb7a4f83a01d15" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.925710    4781 pod_container_deletor.go:79] Container "46c369de0cfef66f30487fc0ca273b2a4137a63435590c9615419e1516bf4d8e" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.925764    4781 pod_container_deletor.go:79] Container "6096f6c8972a415a61209197572cbb67a73d74a11fb51a61bdb9e6e869676fe6" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.929587    4781 pod_container_deletor.go:79] Container "74f9a854245beb3382b9ac1b3c35a6c6efcdb9d3605d41e15f36509a7f353620" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: W0402 11:10:09.929629    4781 pod_container_deletor.go:79] Container "7d8fe50352278acebd691c918af9a09113c7a391651dd28a021d85a0d0ab54f0" not found in pod's containers
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.959025    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: I0402 11:10:09.964194    4781 kubelet_node_status.go:71] Attempting to register node node-9
Apr 02 11:10:09 node-9.domain.tld kubelet[4781]: E0402 11:10:09.964967    4781 kubelet_node_status.go:93] Unable to register node "node-9" with API server: Post "https://localhost:8443/api/v1/nodes": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.059192    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060283    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "tz-config" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-tz-config") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060342    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "etc-kube-ssl" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-etc-kube-ssl") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060394    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "lib-modules" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-lib-modules") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060433    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-xtables-lock") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060472    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "ssl-certs-host" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-ssl-certs-host") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060513    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-kubeconfig") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060551    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "var-run-dbus" (UniqueName: "kubernetes.io/host-path/a27e5e09aca8f8046e3e245f3758bfa1-var-run-dbus") pod "kube-proxy-node-9" (UID: "a27e5e09aca8f8046e3e245f3758bfa1")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060593    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "tz-config" (UniqueName: "kubernetes.io/host-path/53da5e67b54f38828d0097c82bd5d465-tz-config") pod "nginx-proxy-node-9" (UID: "53da5e67b54f38828d0097c82bd5d465")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.060632    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "etc-nginx" (UniqueName: "kubernetes.io/host-path/53da5e67b54f38828d0097c82bd5d465-etc-nginx") pod "nginx-proxy-node-9" (UID: "53da5e67b54f38828d0097c82bd5d465")
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.159281    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.228001050+08:00" level=info msg="StopPodSandbox for \"74f9a854245beb3382b9ac1b3c35a6c6efcdb9d3605d41e15f36509a7f353620\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.228135662+08:00" level=info msg="Container to stop \"7bceedbd28db65771c98baed3745c27c00a456cf61683b46662471071c18c29b\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.228323318+08:00" level=info msg="TearDown network for sandbox \"74f9a854245beb3382b9ac1b3c35a6c6efcdb9d3605d41e15f36509a7f353620\" successfully"
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.228353388+08:00" level=info msg="StopPodSandbox for \"74f9a854245beb3382b9ac1b3c35a6c6efcdb9d3605d41e15f36509a7f353620\" returns successfully"
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.229085723+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:kube-proxy-node-9,Uid:a27e5e09aca8f8046e3e245f3758bfa1,Namespace:kube-system,Attempt:28,}"
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.229939296+08:00" level=info msg="StopPodSandbox for \"1390743c8ef4bb263186c880d95ad5a57fff4a7132b1dbae74cb7a4f83a01d15\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.230040516+08:00" level=info msg="Container to stop \"bbfb2d627de87ad258abfab755b2f6c952cfb441ff93955fab167f9a44ed557f\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.230215094+08:00" level=info msg="TearDown network for sandbox \"1390743c8ef4bb263186c880d95ad5a57fff4a7132b1dbae74cb7a4f83a01d15\" successfully"
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.230245146+08:00" level=info msg="StopPodSandbox for \"1390743c8ef4bb263186c880d95ad5a57fff4a7132b1dbae74cb7a4f83a01d15\" returns successfully"
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.230719499+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nginx-proxy-node-9,Uid:53da5e67b54f38828d0097c82bd5d465,Namespace:kube-system,Attempt:28,}"
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.259377    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.264877    4781 controller.go:144] failed to ensure lease exists, will retry in 800ms, error: Get "https://localhost:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/node-9?timeout=10s": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.303222747+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/8ea78636758329e927fcda98b7ed3f480d9ba2e7a5a002d898dc6ab439d7cb7a pid=5803
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.303183516+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7c9b52f7d5dcb82512538bdd0cef9a0e2a965f129ff8649575c373ec1d78e5d2 pid=5802
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.359515    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: I0402 11:10:10.367159    4781 kubelet_node_status.go:71] Attempting to register node node-9
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.367784    4781 kubelet_node_status.go:93] Unable to register node "node-9" with API server: Post "https://localhost:8443/api/v1/nodes": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.459652    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.525178432+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-node-9,Uid:a27e5e09aca8f8046e3e245f3758bfa1,Namespace:kube-system,Attempt:28,} returns sandbox id \"7c9b52f7d5dcb82512538bdd0cef9a0e2a965f129ff8649575c373ec1d78e5d2\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.525335140+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nginx-proxy-node-9,Uid:53da5e67b54f38828d0097c82bd5d465,Namespace:kube-system,Attempt:28,} returns sandbox id \"8ea78636758329e927fcda98b7ed3f480d9ba2e7a5a002d898dc6ab439d7cb7a\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.529615671+08:00" level=info msg="CreateContainer within sandbox \"8ea78636758329e927fcda98b7ed3f480d9ba2e7a5a002d898dc6ab439d7cb7a\" for container &ContainerMetadata{Name:nginx-proxy,Attempt:28,}"
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.530177683+08:00" level=info msg="CreateContainer within sandbox \"7c9b52f7d5dcb82512538bdd0cef9a0e2a965f129ff8649575c373ec1d78e5d2\" for container &ContainerMetadata{Name:kube-proxy,Attempt:28,}"
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.559786    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.582325872+08:00" level=info msg="CreateContainer within sandbox \"7c9b52f7d5dcb82512538bdd0cef9a0e2a965f129ff8649575c373ec1d78e5d2\" for &ContainerMetadata{Name:kube-proxy,Attempt:28,} returns container id \"0ce4ad35ba45a4b9e3c2c9056820247474b583b421065d65aa4482c7b1194d8d\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.583368378+08:00" level=info msg="StartContainer for \"0ce4ad35ba45a4b9e3c2c9056820247474b583b421065d65aa4482c7b1194d8d\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.585339562+08:00" level=info msg="CreateContainer within sandbox \"8ea78636758329e927fcda98b7ed3f480d9ba2e7a5a002d898dc6ab439d7cb7a\" for &ContainerMetadata{Name:nginx-proxy,Attempt:28,} returns container id \"c71c6f6d3aa7874cc38588a7fa5ea41aa0378a9511b3e47f07ad9a72bb6fe7ed\""
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.585872512+08:00" level=info msg="StartContainer for \"c71c6f6d3aa7874cc38588a7fa5ea41aa0378a9511b3e47f07ad9a72bb6fe7ed\""
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.659929    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.735103802+08:00" level=info msg="StartContainer for \"0ce4ad35ba45a4b9e3c2c9056820247474b583b421065d65aa4482c7b1194d8d\" returns successfully"
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.739224    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://localhost:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:10 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:10.748618295+08:00" level=info msg="StartContainer for \"c71c6f6d3aa7874cc38588a7fa5ea41aa0378a9511b3e47f07ad9a72bb6fe7ed\" returns successfully"
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.754865    4781 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://localhost:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 127.0.0.1:8443: connect: connection refused
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.760069    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.860568    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:10 node-9.domain.tld kubelet[4781]: E0402 11:10:10.960712    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:11 node-9.domain.tld kernel: IPVS: Registered protocols (TCP, UDP, SCTP, AH, ESP)
Apr 02 11:10:11 node-9.domain.tld kernel: IPVS: Connection hash table configured (size=4096, memory=64Kbytes)
Apr 02 11:10:11 node-9.domain.tld kernel: IPVS: ipvs loaded.
Apr 02 11:10:11 node-9.domain.tld kernel: IPVS: [rr] scheduler registered.
Apr 02 11:10:11 node-9.domain.tld kernel: IPVS: [wrr] scheduler registered.
Apr 02 11:10:11 node-9.domain.tld kernel: IPVS: [sh] scheduler registered.
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: E0402 11:10:11.060864    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: E0402 11:10:11.161012    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.170436    4781 kubelet_node_status.go:71] Attempting to register node node-9
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.188711    4781 kubelet_node_status.go:109] Node node-9 was previously registered
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.188871    4781 kubelet_node_status.go:74] Successfully registered node node-9
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.209432    4781 kuberuntime_manager.go:1006] updating runtime config through cri with podcidr 10.232.5.0/24
Apr 02 11:10:11 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:11.209813170+08:00" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.210390    4781 kubelet_network.go:77] Setting Pod CIDR:  -> 10.232.5.0/24
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.212344    4781 setters.go:578] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2024-04-02 11:10:11.212309976 +0800 CST m=+22.517210594 LastTransitionTime:2024-04-02 11:10:11.212309976 +0800 CST m=+22.517210594 Reason:KubeletNotReady Message:CSINode is not yet initialized}
Apr 02 11:10:11 node-9.domain.tld systemd[1]: run-containerd-runc-k8s.io-7c9b52f7d5dcb82512538bdd0cef9a0e2a965f129ff8649575c373ec1d78e5d2-runc.hIiFRF.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-runc-k8s.io-7c9b52f7d5dcb82512538bdd0cef9a0e2a965f129ff8649575c373ec1d78e5d2-runc.hIiFRF.mount has successfully entered the 'dead' state.
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: E0402 11:10:11.261161    4781 kubelet.go:2264] node "node-9" not found
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.423962    4781 apiserver.go:53] Watching apiserver
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.434646    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.434937    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.435129    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.435493    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.435988    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.436345    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.436692    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.437039    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.437382    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.437597    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.463713    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "hosts" (UniqueName: "kubernetes.io/host-path/c05731dc-7b08-4d36-94b2-66caa5aac9bc-hosts") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.463777    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host" (UniqueName: "kubernetes.io/host-path/2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51-host") pod "sriov-network-config-daemon-5bhw2" (UID: "2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.463813    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-device-plugin-etc" (UniqueName: "kubernetes.io/configmap/afbfb6ea-8822-48b1-aaf8-727736597883-k8s-device-plugin-etc") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.463845    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-pod-resources" (UniqueName: "kubernetes.io/host-path/afbfb6ea-8822-48b1-aaf8-727736597883-kube-pod-resources") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.463877    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-tlhsb" (UniqueName: "kubernetes.io/secret/57941549-7588-41af-a4fb-c364aec82fbe-default-token-tlhsb") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.463910    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "qgpu-manager-token-mw9zj" (UniqueName: "kubernetes.io/secret/d56ec515-f683-49ae-af58-15c5ec1b769b-qgpu-manager-token-mw9zj") pod "qgpu-manager-slc77" (UID: "d56ec515-f683-49ae-af58-15c5ec1b769b")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464036    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-dev" (UniqueName: "kubernetes.io/host-path/57941549-7588-41af-a4fb-c364aec82fbe-host-dev") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464210    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "run-nvidia" (UniqueName: "kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia") pod "gpu-feature-discovery-9bc7l" (UID: "a5c9f638-c13f-4618-9c2a-9d56ce52f483")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464325    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/660208d0-7a8e-41b9-8e05-f20b06a55843-cni") pod "kube-multus-ds-tlkh7" (UID: "660208d0-7a8e-41b9-8e05-f20b06a55843")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464402    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/603963b9-f4fc-4de2-a67a-e35b7a5f831a-cni") pod "kube-flannel-8jq85" (UID: "603963b9-f4fc-4de2-a67a-e35b7a5f831a")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464443    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cni-net-dir" (UniqueName: "kubernetes.io/host-path/ec9b65b9-ca91-4b88-8a70-ab3160752e06-cni-net-dir") pod "whereabouts-99wbm" (UID: "ec9b65b9-ca91-4b88-8a70-ab3160752e06")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464471    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "pod-resources" (UniqueName: "kubernetes.io/host-path/d56ec515-f683-49ae-af58-15c5ec1b769b-pod-resources") pod "qgpu-manager-slc77" (UID: "d56ec515-f683-49ae-af58-15c5ec1b769b")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464515    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-root" (UniqueName: "kubernetes.io/host-path/d56ec515-f683-49ae-af58-15c5ec1b769b-host-root") pod "qgpu-manager-slc77" (UID: "d56ec515-f683-49ae-af58-15c5ec1b769b")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464572    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "device-plugin" (UniqueName: "kubernetes.io/host-path/d56ec515-f683-49ae-af58-15c5ec1b769b-device-plugin") pod "qgpu-manager-slc77" (UID: "d56ec515-f683-49ae-af58-15c5ec1b769b")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464647    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-device-plugin-bin" (UniqueName: "kubernetes.io/configmap/afbfb6ea-8822-48b1-aaf8-727736597883-k8s-device-plugin-bin") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464698    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-sys" (UniqueName: "kubernetes.io/host-path/57941549-7588-41af-a4fb-c364aec82fbe-host-sys") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464736    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cnibin" (UniqueName: "kubernetes.io/host-path/660208d0-7a8e-41b9-8e05-f20b06a55843-cnibin") pod "kube-multus-ds-tlkh7" (UID: "660208d0-7a8e-41b9-8e05-f20b06a55843")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464770    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "multus-cfg" (UniqueName: "kubernetes.io/configmap/660208d0-7a8e-41b9-8e05-f20b06a55843-multus-cfg") pod "kube-multus-ds-tlkh7" (UID: "660208d0-7a8e-41b9-8e05-f20b06a55843")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464835    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cnibin" (UniqueName: "kubernetes.io/host-path/ec9b65b9-ca91-4b88-8a70-ab3160752e06-cnibin") pod "whereabouts-99wbm" (UID: "ec9b65b9-ca91-4b88-8a70-ab3160752e06")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464866    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-var" (UniqueName: "kubernetes.io/host-path/d56ec515-f683-49ae-af58-15c5ec1b769b-host-var") pod "qgpu-manager-slc77" (UID: "d56ec515-f683-49ae-af58-15c5ec1b769b")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464893    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "easystack-dm-bin" (UniqueName: "kubernetes.io/configmap/57941549-7588-41af-a4fb-c364aec82fbe-easystack-dm-bin") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464921    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-ns" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-host-ns") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464947    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-run" (UniqueName: "kubernetes.io/host-path/57941549-7588-41af-a4fb-c364aec82fbe-host-run") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.464999    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "whereabouts-token-2n5v7" (UniqueName: "kubernetes.io/secret/ec9b65b9-ca91-4b88-8a70-ab3160752e06-whereabouts-token-2n5v7") pod "whereabouts-99wbm" (UID: "ec9b65b9-ca91-4b88-8a70-ab3160752e06")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465027    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/host-path/c05731dc-7b08-4d36-94b2-66caa5aac9bc-config") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465049    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cni-bin" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-cni-bin") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465108    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nvidia-gpu-feature-discovery-token-tkq2c" (UniqueName: "kubernetes.io/secret/a5c9f638-c13f-4618-9c2a-9d56ce52f483-nvidia-gpu-feature-discovery-token-tkq2c") pod "gpu-feature-discovery-9bc7l" (UID: "a5c9f638-c13f-4618-9c2a-9d56ce52f483")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465159    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "share" (UniqueName: "kubernetes.io/host-path/c05731dc-7b08-4d36-94b2-66caa5aac9bc-share") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465197    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "ecr-deploy-sa-token-qwjwm" (UniqueName: "kubernetes.io/secret/c05731dc-7b08-4d36-94b2-66caa5aac9bc-ecr-deploy-sa-token-qwjwm") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465245    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-config" (UniqueName: "kubernetes.io/host-path/afbfb6ea-8822-48b1-aaf8-727736597883-kube-config") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465270    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/603963b9-f4fc-4de2-a67a-e35b7a5f831a-run") pod "kube-flannel-8jq85" (UID: "603963b9-f4fc-4de2-a67a-e35b7a5f831a")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465297    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-cni-bin" (UniqueName: "kubernetes.io/host-path/603963b9-f4fc-4de2-a67a-e35b7a5f831a-host-cni-bin") pod "kube-flannel-8jq85" (UID: "603963b9-f4fc-4de2-a67a-e35b7a5f831a")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465322    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "bin" (UniqueName: "kubernetes.io/host-path/c05731dc-7b08-4d36-94b2-66caa5aac9bc-bin") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465349    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "ssh-key" (UniqueName: "kubernetes.io/host-path/c05731dc-7b08-4d36-94b2-66caa5aac9bc-ssh-key") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465376    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-tlhsb" (UniqueName: "kubernetes.io/secret/afbfb6ea-8822-48b1-aaf8-727736597883-default-token-tlhsb") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465427    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "systemid" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-systemid") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465465    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-run-ovs" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-host-run-ovs") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465497    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "libmodules" (UniqueName: "kubernetes.io/host-path/57941549-7588-41af-a4fb-c364aec82fbe-libmodules") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465535    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cnibin" (UniqueName: "kubernetes.io/host-path/2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51-cnibin") pod "sriov-network-config-daemon-5bhw2" (UID: "2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465571    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "libmodules" (UniqueName: "kubernetes.io/host-path/afbfb6ea-8822-48b1-aaf8-727736597883-libmodules") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465620    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "localtime" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-localtime") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465682    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "sriov-network-config-daemon-token-hgnzx" (UniqueName: "kubernetes.io/secret/2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51-sriov-network-config-daemon-token-hgnzx") pod "sriov-network-config-daemon-5bhw2" (UID: "2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465727    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "multus-token-bvjjm" (UniqueName: "kubernetes.io/secret/660208d0-7a8e-41b9-8e05-f20b06a55843-multus-token-bvjjm") pod "kube-multus-ds-tlkh7" (UID: "660208d0-7a8e-41b9-8e05-f20b06a55843")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465756    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-dev" (UniqueName: "kubernetes.io/host-path/d56ec515-f683-49ae-af58-15c5ec1b769b-host-dev") pod "qgpu-manager-slc77" (UID: "d56ec515-f683-49ae-af58-15c5ec1b769b")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465806    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "output-dir" (UniqueName: "kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-output-dir") pod "gpu-feature-discovery-9bc7l" (UID: "a5c9f638-c13f-4618-9c2a-9d56ce52f483")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465841    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-sys" (UniqueName: "kubernetes.io/host-path/afbfb6ea-8822-48b1-aaf8-727736597883-host-sys") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465879    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-ovn-controller-token-wcdcc" (UniqueName: "kubernetes.io/secret/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-kube-ovn-controller-token-wcdcc") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465926    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-token-7r22l" (UniqueName: "kubernetes.io/secret/603963b9-f4fc-4de2-a67a-e35b7a5f831a-flannel-token-7r22l") pod "kube-flannel-8jq85" (UID: "603963b9-f4fc-4de2-a67a-e35b7a5f831a")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.465971    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-device-plugins" (UniqueName: "kubernetes.io/host-path/afbfb6ea-8822-48b1-aaf8-727736597883-kube-device-plugins") pod "k8s-device-plugin-jsbrz" (UID: "afbfb6ea-8822-48b1-aaf8-727736597883")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466018    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-run-ovn" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-host-run-ovn") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466085    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "easystack-dm-etc" (UniqueName: "kubernetes.io/configmap/57941549-7588-41af-a4fb-c364aec82fbe-easystack-dm-etc") pod "easystack-dm-jm6ch" (UID: "57941549-7588-41af-a4fb-c364aec82fbe")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466127    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "dmi-product-name" (UniqueName: "kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-dmi-product-name") pod "gpu-feature-discovery-9bc7l" (UID: "a5c9f638-c13f-4618-9c2a-9d56ce52f483")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466161    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-cli-tools" (UniqueName: "kubernetes.io/host-path/c05731dc-7b08-4d36-94b2-66caa5aac9bc-host-cli-tools") pod "secure-container-ecr-deploy-6ftxg" (UID: "c05731dc-7b08-4d36-94b2-66caa5aac9bc")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466199    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cni-conf" (UniqueName: "kubernetes.io/host-path/4b8e8baa-3846-4844-9c71-b5f31b75b8e1-cni-conf") pod "kube-ovn-cni-8nwdn" (UID: "4b8e8baa-3846-4844-9c71-b5f31b75b8e1")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466249    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-cfg" (UniqueName: "kubernetes.io/configmap/603963b9-f4fc-4de2-a67a-e35b7a5f831a-flannel-cfg") pod "kube-flannel-8jq85" (UID: "603963b9-f4fc-4de2-a67a-e35b7a5f831a")
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: I0402 11:10:11.466284    4781 reconciler.go:157] Reconciler: start to sync state
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.823519    4781 kubelet_volumes.go:113] Cleaned up orphaned volume from pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" at /var/lib/kubelet/pods/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf/volumes/kubernetes.io~secret/multus-token-bvjjm
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.824066    4781 kubelet_volumes.go:113] Cleaned up orphaned volume from pod "65a5f028-7090-4386-bebb-69a3b73669c0" at /var/lib/kubelet/pods/65a5f028-7090-4386-bebb-69a3b73669c0/volumes/kubernetes.io~secret/nvidia-operator-validator-token-7vtbt
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.824206    4781 kubelet_volumes.go:140] Cleaned up orphaned pod volumes dir from pod "65a5f028-7090-4386-bebb-69a3b73669c0" at /var/lib/kubelet/pods/65a5f028-7090-4386-bebb-69a3b73669c0/volumes
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.825356    4781 kubelet_volumes.go:113] Cleaned up orphaned volume from pod "6742bdb9-c2df-461f-8de0-5efe19995a11" at /var/lib/kubelet/pods/6742bdb9-c2df-461f-8de0-5efe19995a11/volumes/kubernetes.io~secret/kube-ovn-controller-token-wcdcc
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.825509    4781 kubelet_volumes.go:140] Cleaned up orphaned pod volumes dir from pod "6742bdb9-c2df-461f-8de0-5efe19995a11" at /var/lib/kubelet/pods/6742bdb9-c2df-461f-8de0-5efe19995a11/volumes
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.826487    4781 kubelet_volumes.go:113] Cleaned up orphaned volume from pod "6caf5509-9763-4bab-955a-26597b3c2af7" at /var/lib/kubelet/pods/6caf5509-9763-4bab-955a-26597b3c2af7/volumes/kubernetes.io~secret/nvidia-container-toolkit-token-ptxdz
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.826610    4781 kubelet_volumes.go:140] Cleaned up orphaned pod volumes dir from pod "6caf5509-9763-4bab-955a-26597b3c2af7" at /var/lib/kubelet/pods/6caf5509-9763-4bab-955a-26597b3c2af7/volumes
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.827956    4781 kubelet_volumes.go:113] Cleaned up orphaned volume from pod "c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1" at /var/lib/kubelet/pods/c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1/volumes/kubernetes.io~secret/sriov-network-config-daemon-token-hgnzx
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: W0402 11:10:11.828119    4781 kubelet_volumes.go:140] Cleaned up orphaned pod volumes dir from pod "c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1" at /var/lib/kubelet/pods/c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1/volumes
Apr 02 11:10:11 node-9.domain.tld kubelet[4781]: E0402 11:10:11.829013    4781 kubelet_volumes.go:225] orphaned pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" found, but failed to rmdir() volume at path /var/lib/kubelet/pods/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf/volumes/kubernetes.io~configmap/multus-cfg: directory not empty : There were a total of 2 errors similar to this. Turn up verbosity to see them.
Apr 02 11:10:12 node-9.domain.tld kubelet[4781]: I0402 11:10:12.452928    4781 request.go:655] Throttling request took 1.017266512s, request: GET:https://localhost:8443/api/v1/namespaces/eks-managed/configmaps?fieldSelector=metadata.name%3Dmultus-cni-config&limit=500&resourceVersion=0
Apr 02 11:10:13 node-9.domain.tld kubelet[4781]: E0402 11:10:13.829837    4781 kubelet_volumes.go:225] orphaned pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" found, but failed to rmdir() volume at path /var/lib/kubelet/pods/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf/volumes/kubernetes.io~configmap/multus-cfg: directory not empty : There were a total of 2 errors similar to this. Turn up verbosity to see them.
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.158759    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "sriov-network-config-daemon-token-hgnzx" (UniqueName: "kubernetes.io/secret/c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1-sriov-network-config-daemon-token-hgnzx") pod "c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1" (UID: "c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1")
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.158867    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "multus-token-bvjjm" (UniqueName: "kubernetes.io/secret/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf-multus-token-bvjjm") pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" (UID: "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf")
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.158920    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "kube-ovn-controller-token-wcdcc" (UniqueName: "kubernetes.io/secret/6742bdb9-c2df-461f-8de0-5efe19995a11-kube-ovn-controller-token-wcdcc") pod "6742bdb9-c2df-461f-8de0-5efe19995a11" (UID: "6742bdb9-c2df-461f-8de0-5efe19995a11")
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.158960    4781 empty_dir.go:494] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1/volumes/kubernetes.io~secret/sriov-network-config-daemon-token-hgnzx
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.158993    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1-sriov-network-config-daemon-token-hgnzx" (OuterVolumeSpecName: "sriov-network-config-daemon-token-hgnzx") pod "c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1" (UID: "c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1"). InnerVolumeSpecName "sriov-network-config-daemon-token-hgnzx". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.158969    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "nvidia-container-toolkit-token-ptxdz" (UniqueName: "kubernetes.io/secret/6caf5509-9763-4bab-955a-26597b3c2af7-nvidia-container-toolkit-token-ptxdz") pod "6caf5509-9763-4bab-955a-26597b3c2af7" (UID: "6caf5509-9763-4bab-955a-26597b3c2af7")
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.159072    4781 empty_dir.go:494] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/6caf5509-9763-4bab-955a-26597b3c2af7/volumes/kubernetes.io~secret/nvidia-container-toolkit-token-ptxdz
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.159106    4781 empty_dir.go:494] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/6742bdb9-c2df-461f-8de0-5efe19995a11/volumes/kubernetes.io~secret/kube-ovn-controller-token-wcdcc
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159099    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/6caf5509-9763-4bab-955a-26597b3c2af7-nvidia-container-toolkit-token-ptxdz" (OuterVolumeSpecName: "nvidia-container-toolkit-token-ptxdz") pod "6caf5509-9763-4bab-955a-26597b3c2af7" (UID: "6caf5509-9763-4bab-955a-26597b3c2af7"). InnerVolumeSpecName "nvidia-container-toolkit-token-ptxdz". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159118    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "multus-cfg" (UniqueName: "kubernetes.io/configmap/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf-multus-cfg") pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" (UID: "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf")
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159146    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/6742bdb9-c2df-461f-8de0-5efe19995a11-kube-ovn-controller-token-wcdcc" (OuterVolumeSpecName: "kube-ovn-controller-token-wcdcc") pod "6742bdb9-c2df-461f-8de0-5efe19995a11" (UID: "6742bdb9-c2df-461f-8de0-5efe19995a11"). InnerVolumeSpecName "kube-ovn-controller-token-wcdcc". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159239    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "nvidia-operator-validator-token-7vtbt" (UniqueName: "kubernetes.io/secret/65a5f028-7090-4386-bebb-69a3b73669c0-nvidia-operator-validator-token-7vtbt") pod "65a5f028-7090-4386-bebb-69a3b73669c0" (UID: "65a5f028-7090-4386-bebb-69a3b73669c0")
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.159285    4781 empty_dir.go:494] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf/volumes/kubernetes.io~secret/multus-token-bvjjm
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159317    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf-multus-token-bvjjm" (OuterVolumeSpecName: "multus-token-bvjjm") pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" (UID: "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf"). InnerVolumeSpecName "multus-token-bvjjm". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.159429    4781 empty_dir.go:494] Warning: Unmount skipped because path does not exist: /var/lib/kubelet/pods/65a5f028-7090-4386-bebb-69a3b73669c0/volumes/kubernetes.io~secret/nvidia-operator-validator-token-7vtbt
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159465    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/65a5f028-7090-4386-bebb-69a3b73669c0-nvidia-operator-validator-token-7vtbt" (OuterVolumeSpecName: "nvidia-operator-validator-token-7vtbt") pod "65a5f028-7090-4386-bebb-69a3b73669c0" (UID: "65a5f028-7090-4386-bebb-69a3b73669c0"). InnerVolumeSpecName "nvidia-operator-validator-token-7vtbt". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.159472    4781 empty_dir.go:520] Warning: Failed to clear quota on /var/lib/kubelet/pods/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf/volumes/kubernetes.io~configmap/multus-cfg: clearQuota called, but quotas disabled
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.159899    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf-multus-cfg" (OuterVolumeSpecName: "multus-cfg") pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" (UID: "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf"). InnerVolumeSpecName "multus-cfg". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: E0402 11:10:15.161012    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia podName:a5c9f638-c13f-4618-9c2a-9d56ce52f483 nodeName:}" failed. No retries permitted until 2024-04-02 11:10:15.660914158 +0800 CST m=+26.965814789 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"run-nvidia\" (UniqueName: \"kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia\") pod \"gpu-feature-discovery-9bc7l\" (UID: \"a5c9f638-c13f-4618-9c2a-9d56ce52f483\") : hostPath type check failed: /run/nvidia is not a directory"
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.162965    4781 reconciler.go:319] Volume detached for volume "multus-token-bvjjm" (UniqueName: "kubernetes.io/secret/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf-multus-token-bvjjm") on node "node-9" DevicePath ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.163006    4781 reconciler.go:319] Volume detached for volume "kube-ovn-controller-token-wcdcc" (UniqueName: "kubernetes.io/secret/6742bdb9-c2df-461f-8de0-5efe19995a11-kube-ovn-controller-token-wcdcc") on node "node-9" DevicePath ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.163029    4781 reconciler.go:319] Volume detached for volume "nvidia-container-toolkit-token-ptxdz" (UniqueName: "kubernetes.io/secret/6caf5509-9763-4bab-955a-26597b3c2af7-nvidia-container-toolkit-token-ptxdz") on node "node-9" DevicePath ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.163050    4781 reconciler.go:319] Volume detached for volume "multus-cfg" (UniqueName: "kubernetes.io/configmap/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf-multus-cfg") on node "node-9" DevicePath ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.163077    4781 reconciler.go:319] Volume detached for volume "nvidia-operator-validator-token-7vtbt" (UniqueName: "kubernetes.io/secret/65a5f028-7090-4386-bebb-69a3b73669c0-nvidia-operator-validator-token-7vtbt") on node "node-9" DevicePath ""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: I0402 11:10:15.163101    4781 reconciler.go:319] Volume detached for volume "sriov-network-config-daemon-token-hgnzx" (UniqueName: "kubernetes.io/secret/c291d260-d7fb-44ad-ab2d-6ae2e16ed1f1-sriov-network-config-daemon-token-hgnzx") on node "node-9" DevicePath ""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.352571806+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:sriov-network-config-daemon-5bhw2,Uid:2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51,Namespace:eks-managed,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.359036927+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:whereabouts-99wbm,Uid:ec9b65b9-ca91-4b88-8a70-ab3160752e06,Namespace:eks-managed,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.359097299+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:kube-ovn-cni-8nwdn,Uid:4b8e8baa-3846-4844-9c71-b5f31b75b8e1,Namespace:eks-managed,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.359799864+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:kube-multus-ds-tlkh7,Uid:660208d0-7a8e-41b9-8e05-f20b06a55843,Namespace:eks-managed,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.360391530+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:easystack-dm-jm6ch,Uid:57941549-7588-41af-a4fb-c364aec82fbe,Namespace:openstack,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.363097933+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:qgpu-manager-slc77,Uid:d56ec515-f683-49ae-af58-15c5ec1b769b,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.363441334+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:kube-flannel-8jq85,Uid:603963b9-f4fc-4de2-a67a-e35b7a5f831a,Namespace:kube-system,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.363591664+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:secure-container-ecr-deploy-6ftxg,Uid:c05731dc-7b08-4d36-94b2-66caa5aac9bc,Namespace:eks-managed,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.363578473+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:k8s-device-plugin-jsbrz,Uid:afbfb6ea-8822-48b1-aaf8-727736597883,Namespace:openstack,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.393669198+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248 pid=6047
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.423774347+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/3803b758108a770d59ecf9835e1e77e174ba13f135f13989f2b9a9f0c7f4253c pid=6088
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.424001742+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/ae6b22ac1641df1183b3c0873114abce596519dd138ad97b6bcb500186658634 pid=6089
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.429560914+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/74ae02cb737515a36538374bce5c1a5d5f0d52ba6eacdc8410b665dc9c50c4e7 pid=6109
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.429662608+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/495344792807e3bd00fbeabcb89e5ef1c6780cd0f3be7ce51c98bdbbb53264bd pid=6120
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.432444868+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c pid=6142
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.435281673+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/cccd696b1582526ae5b028c3d12cb8e0b6912210c84453fe3c6921086c6182a8 pid=6162
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.437864595+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899 pid=6181
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: E0402 11:10:15.467464    4781 kubelet.go:1662] Failed creating a mirror pod for "nginx-proxy-node-9_kube-system(53da5e67b54f38828d0097c82bd5d465)": pods "nginx-proxy-node-9" already exists
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.612199183+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:sriov-network-config-daemon-5bhw2,Uid:2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51,Namespace:eks-managed,Attempt:0,} returns sandbox id \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.616301716+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for container &ContainerMetadata{Name:sriov-cni,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.664633913+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:k8s-device-plugin-jsbrz,Uid:afbfb6ea-8822-48b1-aaf8-727736597883,Namespace:openstack,Attempt:0,} returns sandbox id \"cccd696b1582526ae5b028c3d12cb8e0b6912210c84453fe3c6921086c6182a8\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.664943405+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:secure-container-ecr-deploy-6ftxg,Uid:c05731dc-7b08-4d36-94b2-66caa5aac9bc,Namespace:eks-managed,Attempt:0,} returns sandbox id \"74ae02cb737515a36538374bce5c1a5d5f0d52ba6eacdc8410b665dc9c50c4e7\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.665074416+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-multus-ds-tlkh7,Uid:660208d0-7a8e-41b9-8e05-f20b06a55843,Namespace:eks-managed,Attempt:0,} returns sandbox id \"495344792807e3bd00fbeabcb89e5ef1c6780cd0f3be7ce51c98bdbbb53264bd\""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: E0402 11:10:15.665256    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia podName:a5c9f638-c13f-4618-9c2a-9d56ce52f483 nodeName:}" failed. No retries permitted until 2024-04-02 11:10:16.665171607 +0800 CST m=+27.970072256 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"run-nvidia\" (UniqueName: \"kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia\") pod \"gpu-feature-discovery-9bc7l\" (UID: \"a5c9f638-c13f-4618-9c2a-9d56ce52f483\") : hostPath type check failed: /run/nvidia is not a directory"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.666063233+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-flannel-8jq85,Uid:603963b9-f4fc-4de2-a67a-e35b7a5f831a,Namespace:kube-system,Attempt:0,} returns sandbox id \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: E0402 11:10:15.668467    4781 kubelet.go:1662] Failed creating a mirror pod for "kube-proxy-node-9_kube-system(a27e5e09aca8f8046e3e245f3758bfa1)": pods "kube-proxy-node-9" already exists
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.668625582+08:00" level=info msg="CreateContainer within sandbox \"74ae02cb737515a36538374bce5c1a5d5f0d52ba6eacdc8410b665dc9c50c4e7\" for container &ContainerMetadata{Name:ecr-deploy,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.668834285+08:00" level=info msg="CreateContainer within sandbox \"495344792807e3bd00fbeabcb89e5ef1c6780cd0f3be7ce51c98bdbbb53264bd\" for container &ContainerMetadata{Name:kube-multus,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.669791805+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for container &ContainerMetadata{Name:kube-flannel,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.683682886+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-ovn-cni-8nwdn,Uid:4b8e8baa-3846-4844-9c71-b5f31b75b8e1,Namespace:eks-managed,Attempt:0,} returns sandbox id \"ae6b22ac1641df1183b3c0873114abce596519dd138ad97b6bcb500186658634\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.684320830+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for &ContainerMetadata{Name:sriov-cni,Attempt:0,} returns container id \"d75ef1b670c5c6cbdd9861c87b98ddccd055e9183126b2a572ec6895e5f92bdc\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.684854263+08:00" level=info msg="StartContainer for \"d75ef1b670c5c6cbdd9861c87b98ddccd055e9183126b2a572ec6895e5f92bdc\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.685642613+08:00" level=info msg="CreateContainer within sandbox \"cccd696b1582526ae5b028c3d12cb8e0b6912210c84453fe3c6921086c6182a8\" for container &ContainerMetadata{Name:k8s-device-plugin,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.685892769+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:easystack-dm-jm6ch,Uid:57941549-7588-41af-a4fb-c364aec82fbe,Namespace:openstack,Attempt:0,} returns sandbox id \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.686210844+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.708231523+08:00" level=info msg="CreateContainer within sandbox \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\" for container &ContainerMetadata{Name:esdm-init,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.745130689+08:00" level=info msg="CreateContainer within sandbox \"495344792807e3bd00fbeabcb89e5ef1c6780cd0f3be7ce51c98bdbbb53264bd\" for &ContainerMetadata{Name:kube-multus,Attempt:0,} returns container id \"f424218924877ffac66168b65ab39af35da6ccba13b19caccdd8c44c58593015\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.745496121+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for &ContainerMetadata{Name:kube-flannel,Attempt:0,} returns container id \"54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.745645442+08:00" level=info msg="StartContainer for \"f424218924877ffac66168b65ab39af35da6ccba13b19caccdd8c44c58593015\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.746017484+08:00" level=info msg="StartContainer for \"54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.751333053+08:00" level=info msg="CreateContainer within sandbox \"74ae02cb737515a36538374bce5c1a5d5f0d52ba6eacdc8410b665dc9c50c4e7\" for &ContainerMetadata{Name:ecr-deploy,Attempt:0,} returns container id \"ef953bc21fad78c7dc70df098016f61cc6ba65565d541ddf64f99993aef74d9c\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.751801035+08:00" level=info msg="StartContainer for \"ef953bc21fad78c7dc70df098016f61cc6ba65565d541ddf64f99993aef74d9c\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.772675467+08:00" level=info msg="CreateContainer within sandbox \"cccd696b1582526ae5b028c3d12cb8e0b6912210c84453fe3c6921086c6182a8\" for &ContainerMetadata{Name:k8s-device-plugin,Attempt:0,} returns container id \"7afe4f846e199358a4651f890f649c2d9ce6583b88396671380d797fafd2dbc1\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.773383408+08:00" level=info msg="StartContainer for \"7afe4f846e199358a4651f890f649c2d9ce6583b88396671380d797fafd2dbc1\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.779869415+08:00" level=info msg="CreateContainer within sandbox \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\" for &ContainerMetadata{Name:esdm-init,Attempt:0,} returns container id \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.780401593+08:00" level=info msg="StartContainer for \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\""
Apr 02 11:10:15 node-9.domain.tld kubelet[4781]: W0402 11:10:15.825820    4781 kubelet_volumes.go:140] Cleaned up orphaned pod volumes dir from pod "2b3a2fa2-e8f5-4da5-b5da-0334f67995cf" at /var/lib/kubelet/pods/2b3a2fa2-e8f5-4da5-b5da-0334f67995cf/volumes
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.859101798+08:00" level=info msg="StartContainer for \"d75ef1b670c5c6cbdd9861c87b98ddccd055e9183126b2a572ec6895e5f92bdc\" returns successfully"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.860995074+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:whereabouts-99wbm,Uid:ec9b65b9-ca91-4b88-8a70-ab3160752e06,Namespace:eks-managed,Attempt:0,} returns sandbox id \"3803b758108a770d59ecf9835e1e77e174ba13f135f13989f2b9a9f0c7f4253c\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.862323094+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for container &ContainerMetadata{Name:sriov-infiniband-cni,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.864368834+08:00" level=info msg="CreateContainer within sandbox \"3803b758108a770d59ecf9835e1e77e174ba13f135f13989f2b9a9f0c7f4253c\" for container &ContainerMetadata{Name:whereabouts,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.928111434+08:00" level=info msg="StartContainer for \"54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7\" returns successfully"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.930318537+08:00" level=info msg="StartContainer for \"f424218924877ffac66168b65ab39af35da6ccba13b19caccdd8c44c58593015\" returns successfully"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.932239844+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for container &ContainerMetadata{Name:install-cni,Attempt:0,}"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.939695623+08:00" level=info msg="StartContainer for \"ef953bc21fad78c7dc70df098016f61cc6ba65565d541ddf64f99993aef74d9c\" returns successfully"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.951373569+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for &ContainerMetadata{Name:sriov-infiniband-cni,Attempt:0,} returns container id \"3332ec0e61e8b3a756559c03a1fd50bf6bc0a180dcc376e5f47847f8b0774fa5\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.951880146+08:00" level=info msg="StartContainer for \"3332ec0e61e8b3a756559c03a1fd50bf6bc0a180dcc376e5f47847f8b0774fa5\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.958098232+08:00" level=info msg="CreateContainer within sandbox \"3803b758108a770d59ecf9835e1e77e174ba13f135f13989f2b9a9f0c7f4253c\" for &ContainerMetadata{Name:whereabouts,Attempt:0,} returns container id \"9123b0b8e255e573a4544ef9e4999ae303629e5006c109a28c988f8c06c13c57\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.958660939+08:00" level=info msg="StartContainer for \"9123b0b8e255e573a4544ef9e4999ae303629e5006c109a28c988f8c06c13c57\""
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.972157603+08:00" level=info msg="StartContainer for \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\" returns successfully"
Apr 02 11:10:15 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:15.973218654+08:00" level=info msg="StartContainer for \"7afe4f846e199358a4651f890f649c2d9ce6583b88396671380d797fafd2dbc1\" returns successfully"
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.027174284+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for &ContainerMetadata{Name:install-cni,Attempt:0,} returns container id \"2d2712628ce5645ed7e3efee34c0eed44b935ac0a38daa5176c333cf3ccd4fe5\""
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.027752616+08:00" level=info msg="StartContainer for \"2d2712628ce5645ed7e3efee34c0eed44b935ac0a38daa5176c333cf3ccd4fe5\""
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.115194563+08:00" level=info msg="StartContainer for \"3332ec0e61e8b3a756559c03a1fd50bf6bc0a180dcc376e5f47847f8b0774fa5\" returns successfully"
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.118990436+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for container &ContainerMetadata{Name:sriov-network-config-daemon,Attempt:0,}"
Apr 02 11:10:16 node-9.domain.tld systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount972878877.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit var-lib-containerd-tmpmounts-containerd\x2dmount972878877.mount has successfully entered the 'dead' state.
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.226697985+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for &ContainerMetadata{Name:sriov-network-config-daemon,Attempt:0,} returns container id \"1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e\""
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.227404568+08:00" level=info msg="StartContainer for \"1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e\""
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.236539184+08:00" level=info msg="StartContainer for \"2d2712628ce5645ed7e3efee34c0eed44b935ac0a38daa5176c333cf3ccd4fe5\" returns successfully"
Apr 02 11:10:16 node-9.domain.tld kubelet[4781]: I0402 11:10:16.362988    4781 manager.go:408] Got registration request from device plugin with resource name "nvidia.com/gpu"
Apr 02 11:10:16 node-9.domain.tld kubelet[4781]: I0402 11:10:16.363282    4781 endpoint.go:196] parsed scheme: ""
Apr 02 11:10:16 node-9.domain.tld kubelet[4781]: I0402 11:10:16.363316    4781 endpoint.go:196] scheme "" not registered, fallback to default scheme
Apr 02 11:10:16 node-9.domain.tld kubelet[4781]: I0402 11:10:16.363362    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/lib/kubelet/device-plugins/nvidia-gpu.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:16 node-9.domain.tld kubelet[4781]: I0402 11:10:16.363391    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.410476880+08:00" level=info msg="StartContainer for \"1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e\" returns successfully"
Apr 02 11:10:16 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:16.667994972+08:00" level=info msg="StartContainer for \"9123b0b8e255e573a4544ef9e4999ae303629e5006c109a28c988f8c06c13c57\" returns successfully"
Apr 02 11:10:16 node-9.domain.tld kubelet[4781]: E0402 11:10:16.669355    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia podName:a5c9f638-c13f-4618-9c2a-9d56ce52f483 nodeName:}" failed. No retries permitted until 2024-04-02 11:10:18.669294869 +0800 CST m=+29.974195486 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"run-nvidia\" (UniqueName: \"kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia\") pod \"gpu-feature-discovery-9bc7l\" (UID: \"a5c9f638-c13f-4618-9c2a-9d56ce52f483\") : hostPath type check failed: /run/nvidia is not a directory"
Apr 02 11:10:18 node-9.domain.tld kubelet[4781]: E0402 11:10:18.677385    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia podName:a5c9f638-c13f-4618-9c2a-9d56ce52f483 nodeName:}" failed. No retries permitted until 2024-04-02 11:10:22.67732633 +0800 CST m=+33.982226951 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"run-nvidia\" (UniqueName: \"kubernetes.io/host-path/a5c9f638-c13f-4618-9c2a-9d56ce52f483-run-nvidia\") pod \"gpu-feature-discovery-9bc7l\" (UID: \"a5c9f638-c13f-4618-9c2a-9d56ce52f483\") : hostPath type check failed: /run/nvidia is not a directory"
Apr 02 11:10:18 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:18.768119596+08:00" level=info msg="trying next host" error="failed to do request: Head \"https://hub.easystack.io/v2/production/escloud-linux-source-kube-ovn/manifests/6.2.1\": dial tcp 10.222.255.254:443: connect: no route to host" host=hub.easystack.io
Apr 02 11:10:18 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:18.770950509+08:00" level=error msg="PullImage \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\" failed" error="failed to pull and unpack image \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\": failed to resolve reference \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\": failed to do request: Head \"https://hub.easystack.io/v2/production/escloud-linux-source-kube-ovn/manifests/6.2.1\": dial tcp 10.222.255.254:443: connect: no route to host"
Apr 02 11:10:18 node-9.domain.tld kubelet[4781]: E0402 11:10:18.771332    4781 remote_image.go:113] PullImage "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1" from image service failed: rpc error: code = Unknown desc = failed to pull and unpack image "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1": failed to resolve reference "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1": failed to do request: Head "https://hub.easystack.io/v2/production/escloud-linux-source-kube-ovn/manifests/6.2.1": dial tcp 10.222.255.254:443: connect: no route to host
Apr 02 11:10:18 node-9.domain.tld kubelet[4781]: E0402 11:10:18.771432    4781 kuberuntime_image.go:51] Pull image "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1" failed: rpc error: code = Unknown desc = failed to pull and unpack image "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1": failed to resolve reference "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1": failed to do request: Head "https://hub.easystack.io/v2/production/escloud-linux-source-kube-ovn/manifests/6.2.1": dial tcp 10.222.255.254:443: connect: no route to host
Apr 02 11:10:18 node-9.domain.tld kubelet[4781]: E0402 11:10:18.771631    4781 kuberuntime_manager.go:829] init container &Container{Name:install-cni,Image:hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1,Command:[/kube-ovn/install-cni.sh],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cni-bin,ReadOnly:false,MountPath:/opt/cni/bin,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-ovn-controller-token-wcdcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod kube-ovn-cni-8nwdn_eks-managed(4b8e8baa-3846-4844-9c71-b5f31b75b8e1): ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1": failed to resolve reference "hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1": failed to do request: Head "https://hub.easystack.io/v2/production/escloud-linux-source-kube-ovn/manifests/6.2.1": dial tcp 10.222.255.254:443: connect: no route to host
Apr 02 11:10:18 node-9.domain.tld kubelet[4781]: E0402 11:10:18.771710    4781 pod_workers.go:191] Error syncing pod 4b8e8baa-3846-4844-9c71-b5f31b75b8e1 ("kube-ovn-cni-8nwdn_eks-managed(4b8e8baa-3846-4844-9c71-b5f31b75b8e1)"), skipping: failed to "StartContainer" for "install-cni" with ErrImagePull: "rpc error: code = Unknown desc = failed to pull and unpack image \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\": failed to resolve reference \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\": failed to do request: Head \"https://hub.easystack.io/v2/production/escloud-linux-source-kube-ovn/manifests/6.2.1\": dial tcp 10.222.255.254:443: connect: no route to host"
Apr 02 11:10:18 node-9.domain.tld kubelet[4781]: E0402 11:10:18.892120    4781 pod_workers.go:191] Error syncing pod 4b8e8baa-3846-4844-9c71-b5f31b75b8e1 ("kube-ovn-cni-8nwdn_eks-managed(4b8e8baa-3846-4844-9c71-b5f31b75b8e1)"), skipping: failed to "StartContainer" for "install-cni" with ImagePullBackOff: "Back-off pulling image \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\""
Apr 02 11:10:19 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.518356952+08:00" level=info msg="shim disconnected" id=54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.518469251+08:00" level=warning msg="cleaning up after shim disconnected" id=54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7 namespace=k8s.io
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.518493695+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:19 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.543348132+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:19+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=6985\n"
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.545390472+08:00" level=info msg="shim disconnected" id=1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.545494292+08:00" level=warning msg="cleaning up after shim disconnected" id=1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e namespace=k8s.io
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.545518947+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.570589043+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:19+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=6999\n"
Apr 02 11:10:19 node-9.domain.tld kubelet[4781]: I0402 11:10:19.897152    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e
Apr 02 11:10:19 node-9.domain.tld kubelet[4781]: I0402 11:10:19.899005    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.901379661+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for container &ContainerMetadata{Name:sriov-network-config-daemon,Attempt:1,}"
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.902761959+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for container &ContainerMetadata{Name:kube-flannel,Attempt:1,}"
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.979142417+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for &ContainerMetadata{Name:sriov-network-config-daemon,Attempt:1,} returns container id \"f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621\""
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.979769116+08:00" level=info msg="StartContainer for \"f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621\""
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.980111296+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for &ContainerMetadata{Name:kube-flannel,Attempt:1,} returns container id \"fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98\""
Apr 02 11:10:19 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:19.980647730+08:00" level=info msg="StartContainer for \"fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98\""
Apr 02 11:10:20 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:20.142207305+08:00" level=info msg="StartContainer for \"fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98\" returns successfully"
Apr 02 11:10:20 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:20.143260085+08:00" level=info msg="StartContainer for \"f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621\" returns successfully"
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.369206    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.370988    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.420260    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.423752    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.454808    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.460549    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.488107    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "run-nvidia-validations" (UniqueName: "kubernetes.io/host-path/cd15a183-f8fb-4b52-a26d-5fe952a47e48-run-nvidia-validations") pod "nvidia-operator-validator-zlljs" (UID: "cd15a183-f8fb-4b52-a26d-5fe952a47e48")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.488285    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nvidia-operator-validator-token-7vtbt" (UniqueName: "kubernetes.io/secret/cd15a183-f8fb-4b52-a26d-5fe952a47e48-nvidia-operator-validator-token-7vtbt") pod "nvidia-operator-validator-zlljs" (UID: "cd15a183-f8fb-4b52-a26d-5fe952a47e48")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.488433    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "qgpu-exporter-token-mm8lc" (UniqueName: "kubernetes.io/secret/7081080d-560e-4046-91d7-1d5f2b1f8230-qgpu-exporter-token-mm8lc") pod "qgpu-exporter-2dc89" (UID: "7081080d-560e-4046-91d7-1d5f2b1f8230")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.488501    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "cgroup" (UniqueName: "kubernetes.io/host-path/7081080d-560e-4046-91d7-1d5f2b1f8230-cgroup") pod "qgpu-exporter-2dc89" (UID: "7081080d-560e-4046-91d7-1d5f2b1f8230")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.488618    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "driver-install-path" (UniqueName: "kubernetes.io/host-path/cd15a183-f8fb-4b52-a26d-5fe952a47e48-driver-install-path") pod "nvidia-operator-validator-zlljs" (UID: "cd15a183-f8fb-4b52-a26d-5fe952a47e48")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.503131    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.531314    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.547637    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.578499    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590019    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "dev-dir" (UniqueName: "kubernetes.io/host-path/513eedf5-3a31-4ffe-9572-f2e93d630947-dev-dir") pod "nvmatrix-k8stor-04-qm299" (UID: "513eedf5-3a31-4ffe-9572-f2e93d630947")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590087    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "scalelfash-conf" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-scalelfash-conf") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590168    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "containerd-socket" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-containerd-socket") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590231    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "registration-dir" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-registration-dir") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590292    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "csi-service-account-token-7gtnj" (UniqueName: "kubernetes.io/secret/513eedf5-3a31-4ffe-9572-f2e93d630947-csi-service-account-token-7gtnj") pod "nvmatrix-k8stor-04-qm299" (UID: "513eedf5-3a31-4ffe-9572-f2e93d630947")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590379    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "socket-dir" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-socket-dir") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590449    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "fluent-logging-bin" (UniqueName: "kubernetes.io/configmap/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-bin") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590496    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubelet-dir" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-kubelet-dir") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590573    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "storadm-exec" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-storadm-exec") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590689    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "roothome" (UniqueName: "kubernetes.io/host-path/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-roothome") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.590746    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nvidia-run-path" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-nvidia-run-path") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591049    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "varlibdockercontainers" (UniqueName: "kubernetes.io/host-path/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-varlibdockercontainers") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591141    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "driver-install-path" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-driver-install-path") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591272    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "crio-hooks" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-crio-hooks") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591351    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "containerd-config" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-containerd-config") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591430    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "tz-config" (UniqueName: "kubernetes.io/host-path/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-tz-config") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591504    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nvidia-container-toolkit-token-ptxdz" (UniqueName: "kubernetes.io/secret/505d6e19-c308-4800-85b7-d8a906c1ac02-nvidia-container-toolkit-token-ptxdz") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591589    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "run-nvidia-validations" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-run-nvidia-validations") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591659    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "fluent-logging-etc" (UniqueName: "kubernetes.io/configmap/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-etc") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591713    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "fluent-logging-fluentbit-token-8zgzg" (UniqueName: "kubernetes.io/secret/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-fluentbit-token-8zgzg") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591785    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "dev-dir" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-dev-dir") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591903    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "fluentbitetc" (UniqueName: "kubernetes.io/empty-dir/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluentbitetc") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.591953    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "sys-kernel-config" (UniqueName: "kubernetes.io/host-path/513eedf5-3a31-4ffe-9572-f2e93d630947-sys-kernel-config") pod "nvmatrix-k8stor-04-qm299" (UID: "513eedf5-3a31-4ffe-9572-f2e93d630947")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.592010    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "varlog" (UniqueName: "kubernetes.io/host-path/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-varlog") pod "fluentbit-zmjjt" (UID: "1014ef35-66b7-4144-8d62-cf6d3dd9b69a")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.592056    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nvidia-local" (UniqueName: "kubernetes.io/host-path/505d6e19-c308-4800-85b7-d8a906c1ac02-nvidia-local") pod "nvidia-container-toolkit-daemonset-qr9nk" (UID: "505d6e19-c308-4800-85b7-d8a906c1ac02")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.592179    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "scaleflash-csi-dir" (UniqueName: "kubernetes.io/host-path/805dbfe7-6703-408b-aa8f-ed778c183e6b-scaleflash-csi-dir") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.592229    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "sf-scaleflash" (UniqueName: "kubernetes.io/host-path/513eedf5-3a31-4ffe-9572-f2e93d630947-sf-scaleflash") pod "nvmatrix-k8stor-04-qm299" (UID: "513eedf5-3a31-4ffe-9572-f2e93d630947")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.592307    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "modules" (UniqueName: "kubernetes.io/host-path/513eedf5-3a31-4ffe-9572-f2e93d630947-modules") pod "nvmatrix-k8stor-04-qm299" (UID: "513eedf5-3a31-4ffe-9572-f2e93d630947")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.592433    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "csi-service-account-token-7gtnj" (UniqueName: "kubernetes.io/secret/805dbfe7-6703-408b-aa8f-ed778c183e6b-csi-service-account-token-7gtnj") pod "csi-node-scaleflashplugin-x4k72" (UID: "805dbfe7-6703-408b-aa8f-ed778c183e6b")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.595808    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:21 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:21.677596381+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:21 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:21.677884915+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,} failed, error" error="failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: E0402 11:10:21.678130    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: E0402 11:10:21.678209    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: E0402 11:10:21.678239    4781 kuberuntime_manager.go:755] createPodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: E0402 11:10:21.678315    4781 pod_workers.go:191] Error syncing pod cd15a183-f8fb-4b52-a26d-5fe952a47e48 ("nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)"), skipping: failed to "CreatePodSandbox" for "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" with CreatePodSandboxError: "CreatePodSandbox for pod \"nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)\" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.693055    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-rootfs" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-host-rootfs") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.693365    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-os-release" (UniqueName: "kubernetes.io/host-path/3f04228b-0006-4b96-acca-351a4d101cfa-host-os-release") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.693470    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "openvswitch" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-openvswitch") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.693578    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "hostetcudev" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-hostetcudev") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.693640    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-tlhsb" (UniqueName: "kubernetes.io/secret/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-default-token-tlhsb") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694074    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "hostrsa" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-hostrsa") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694180    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubectl" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-kubectl") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694352    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-sys" (UniqueName: "kubernetes.io/host-path/3f04228b-0006-4b96-acca-351a4d101cfa-host-sys") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694560    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "source-d" (UniqueName: "kubernetes.io/host-path/3f04228b-0006-4b96-acca-351a4d101cfa-source-d") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694620    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "hostopt" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-hostopt") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694824    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "features-d" (UniqueName: "kubernetes.io/host-path/3f04228b-0006-4b96-acca-351a4d101cfa-features-d") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694887    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-b9c7f" (UniqueName: "kubernetes.io/secret/3f04228b-0006-4b96-acca-351a4d101cfa-default-token-b9c7f") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.694935    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "ovn-bin" (UniqueName: "kubernetes.io/configmap/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-ovn-bin") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.695136    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nfd-worker-conf" (UniqueName: "kubernetes.io/configmap/3f04228b-0006-4b96-acca-351a4d101cfa-nfd-worker-conf") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.695214    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "hostnetscripts" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-hostnetscripts") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.695275    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "host-boot" (UniqueName: "kubernetes.io/host-path/3f04228b-0006-4b96-acca-351a4d101cfa-host-boot") pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb" (UID: "3f04228b-0006-4b96-acca-351a4d101cfa")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.695449    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "ovsconfig" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-ovsconfig") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld kubelet[4781]: I0402 11:10:21.695592    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "coredump" (UniqueName: "kubernetes.io/host-path/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-coredump") pod "ovn-controller-lcv2p" (UID: "6a2d4496-4f35-4f65-b4b6-e190f24c7be0")
Apr 02 11:10:21 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:21.980013426+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:qgpu-exporter-2dc89,Uid:7081080d-560e-4046-91d7-1d5f2b1f8230,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.009018307+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9 pid=7144
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.178158486+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:qgpu-exporter-2dc89,Uid:7081080d-560e-4046-91d7-1d5f2b1f8230,Namespace:imp-qgpu,Attempt:0,} returns sandbox id \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.181979334+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for container &ContainerMetadata{Name:qgpu-exporter,Attempt:0,}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.257505947+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for &ContainerMetadata{Name:qgpu-exporter,Attempt:0,} returns container id \"9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.258074660+08:00" level=info msg="StartContainer for \"9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4\""
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.266606    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.331296807+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:csi-node-scaleflashplugin-x4k72,Uid:805dbfe7-6703-408b-aa8f-ed778c183e6b,Namespace:scaleflash,Attempt:0,}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.332438040+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvmatrix-k8stor-04-qm299,Uid:513eedf5-3a31-4ffe-9572-f2e93d630947,Namespace:scaleflash,Attempt:0,}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.362677892+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-container-toolkit-daemonset-qr9nk,Uid:505d6e19-c308-4800-85b7-d8a906c1ac02,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.369627384+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/557c72fbb5dec53e29ae415b964cd5bb984f104b1a0cb450c8d078751d8f39f3 pid=7225
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.427224325+08:00" level=info msg="StartContainer for \"9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4\" returns successfully"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.511337630+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nvmatrix-k8stor-04-qm299,Uid:513eedf5-3a31-4ffe-9572-f2e93d630947,Namespace:scaleflash,Attempt:0,} returns sandbox id \"557c72fbb5dec53e29ae415b964cd5bb984f104b1a0cb450c8d078751d8f39f3\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.513574558+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/scaleflash/scaleflash:d3.6.0-406-es8_24-1\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.570740456+08:00" level=info msg="shim disconnected" id=fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.570844542+08:00" level=warning msg="cleaning up after shim disconnected" id=fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98 namespace=k8s.io
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.570877656+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.577309253+08:00" level=info msg="shim disconnected" id=f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.577419919+08:00" level=warning msg="cleaning up after shim disconnected" id=f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621 namespace=k8s.io
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.577448512+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.580105772+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/scaleflash/scaleflash:d3.6.0-406-es8_24-1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.585788303+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:b484aa69388db66388fb3c1288bfb4927340a492add5afc724a8bdddbbd5625b,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.589654480+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:22+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=7346\n"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.590705146+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/scaleflash/scaleflash:d3.6.0-406-es8_24-1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.596081852+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/scaleflash/scaleflash@sha256:0b8767b4688ff43535b6c467ff5846f3cde3655b2cb3590a6f1793007b0ff289,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.597312547+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/scaleflash/scaleflash:d3.6.0-406-es8_24-1\" returns image reference \"sha256:b484aa69388db66388fb3c1288bfb4927340a492add5afc724a8bdddbbd5625b\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.603673481+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:22+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=7356\n"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.606174347+08:00" level=info msg="CreateContainer within sandbox \"557c72fbb5dec53e29ae415b964cd5bb984f104b1a0cb450c8d078751d8f39f3\" for container &ContainerMetadata{Name:nvmatrix-k8stor-04,Attempt:0,}"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.652938    4781 request.go:655] Throttling request took 1.192184837s, request: GET:https://localhost:8443/api/v1/namespaces/openstack/configmaps?fieldSelector=metadata.name%3Dfluent-logging-bin&limit=500&resourceVersion=0
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.684769686+08:00" level=info msg="CreateContainer within sandbox \"557c72fbb5dec53e29ae415b964cd5bb984f104b1a0cb450c8d078751d8f39f3\" for &ContainerMetadata{Name:nvmatrix-k8stor-04,Attempt:0,} returns container id \"a4ccf9d89113922fd5f953e67ebf8d46f7c8138668f9273a82953a363fb91277\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.685501201+08:00" level=info msg="StartContainer for \"a4ccf9d89113922fd5f953e67ebf8d46f7c8138668f9273a82953a363fb91277\""
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.694117    4781 configmap.go:200] Couldn't get configMap openstack/fluent-logging-etc: failed to sync configmap cache: timed out waiting for the condition
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.694323    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-etc podName:1014ef35-66b7-4144-8d62-cf6d3dd9b69a nodeName:}" failed. No retries permitted until 2024-04-02 11:10:23.194246231 +0800 CST m=+34.499146844 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"fluent-logging-etc\" (UniqueName: \"kubernetes.io/configmap/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-etc\") pod \"fluentbit-zmjjt\" (UID: \"1014ef35-66b7-4144-8d62-cf6d3dd9b69a\") : failed to sync configmap cache: timed out waiting for the condition"
Apr 02 11:10:22 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:22 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.695650    4781 secret.go:195] Couldn't get secret openstack/fluent-logging-fluentbit-token-8zgzg: failed to sync secret cache: timed out waiting for the condition
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.695855    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-fluentbit-token-8zgzg podName:1014ef35-66b7-4144-8d62-cf6d3dd9b69a nodeName:}" failed. No retries permitted until 2024-04-02 11:10:23.195788802 +0800 CST m=+34.500689419 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"fluent-logging-fluentbit-token-8zgzg\" (UniqueName: \"kubernetes.io/secret/1014ef35-66b7-4144-8d62-cf6d3dd9b69a-fluent-logging-fluentbit-token-8zgzg\") pod \"fluentbit-zmjjt\" (UID: \"1014ef35-66b7-4144-8d62-cf6d3dd9b69a\") : failed to sync secret cache: timed out waiting for the condition"
Apr 02 11:10:22 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.720782797+08:00" level=info msg="shim disconnected" id=9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.720892749+08:00" level=warning msg="cleaning up after shim disconnected" id=9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4 namespace=k8s.io
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.720919309+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.739339039+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:22+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=7441\n"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.796799    4781 configmap.go:200] Couldn't get configMap openstack/ovn-bin: failed to sync configmap cache: timed out waiting for the condition
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.796889    4781 secret.go:195] Couldn't get secret kube-system/default-token-b9c7f: failed to sync secret cache: timed out waiting for the condition
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.796970    4781 configmap.go:200] Couldn't get configMap kube-system/nfd-worker-conf: failed to sync configmap cache: timed out waiting for the condition
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.796986    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-ovn-bin podName:6a2d4496-4f35-4f65-b4b6-e190f24c7be0 nodeName:}" failed. No retries permitted until 2024-04-02 11:10:23.296922779 +0800 CST m=+34.601823402 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"ovn-bin\" (UniqueName: \"kubernetes.io/configmap/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-ovn-bin\") pod \"ovn-controller-lcv2p\" (UID: \"6a2d4496-4f35-4f65-b4b6-e190f24c7be0\") : failed to sync configmap cache: timed out waiting for the condition"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.797057    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/3f04228b-0006-4b96-acca-351a4d101cfa-default-token-b9c7f podName:3f04228b-0006-4b96-acca-351a4d101cfa nodeName:}" failed. No retries permitted until 2024-04-02 11:10:23.297006295 +0800 CST m=+34.601906911 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"default-token-b9c7f\" (UniqueName: \"kubernetes.io/secret/3f04228b-0006-4b96-acca-351a4d101cfa-default-token-b9c7f\") pod \"qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb\" (UID: \"3f04228b-0006-4b96-acca-351a4d101cfa\") : failed to sync secret cache: timed out waiting for the condition"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.797107    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3f04228b-0006-4b96-acca-351a4d101cfa-nfd-worker-conf podName:3f04228b-0006-4b96-acca-351a4d101cfa nodeName:}" failed. No retries permitted until 2024-04-02 11:10:23.297071325 +0800 CST m=+34.601971936 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"nfd-worker-conf\" (UniqueName: \"kubernetes.io/configmap/3f04228b-0006-4b96-acca-351a4d101cfa-nfd-worker-conf\") pod \"qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb\" (UID: \"3f04228b-0006-4b96-acca-351a4d101cfa\") : failed to sync configmap cache: timed out waiting for the condition"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.843222667+08:00" level=info msg="StartContainer for \"a4ccf9d89113922fd5f953e67ebf8d46f7c8138668f9273a82953a363fb91277\" returns successfully"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.856508800+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.856646935+08:00" level=info msg="Container to stop \"d2d26ef245e6de97c1bd240718f49c9d29946487b85a5ffbb6f4602fe851e77a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.856733692+08:00" level=info msg="Container to stop \"c2fdfb69a7778a307702ed14eb829ee2ae564440efc1234a51853228ad673d7a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.915085    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.915633    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.916365425+08:00" level=info msg="RemoveContainer for \"54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7\""
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.916576    4781 pod_workers.go:191] Error syncing pod 603963b9-f4fc-4de2-a67a-e35b7a5f831a ("kube-flannel-8jq85_kube-system(603963b9-f4fc-4de2-a67a-e35b7a5f831a)"), skipping: failed to "StartContainer" for "kube-flannel" with CrashLoopBackOff: "back-off 10s restarting failed container=kube-flannel pod=kube-flannel-8jq85_kube-system(603963b9-f4fc-4de2-a67a-e35b7a5f831a)"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.920876    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: E0402 11:10:22.921530    4781 pod_workers.go:191] Error syncing pod 2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51 ("sriov-network-config-daemon-5bhw2_eks-managed(2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51)"), skipping: failed to "StartContainer" for "sriov-network-config-daemon" with CrashLoopBackOff: "back-off 10s restarting failed container=sriov-network-config-daemon pod=sriov-network-config-daemon-5bhw2_eks-managed(2a6834f6-02fe-4f1c-9dbf-87ff6f2b3a51)"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.921909    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.924740456+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for container &ContainerMetadata{Name:qgpu-exporter,Attempt:1,}"
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.927344157+08:00" level=info msg="RemoveContainer for \"54a811472fee4a435abd7b5b4ed9d52ef8cec1683c63f684bee7ee14c6e0b6c7\" returns successfully"
Apr 02 11:10:22 node-9.domain.tld kubelet[4781]: I0402 11:10:22.927543    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.928719463+08:00" level=info msg="RemoveContainer for \"1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e\""
Apr 02 11:10:22 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:22.938095243+08:00" level=info msg="RemoveContainer for \"1d6a0a0ec802b776a8ccfb6e48850352adeb4d7a3130bf7496ccf2c427e0409e\" returns successfully"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.002543621+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for &ContainerMetadata{Name:qgpu-exporter,Attempt:1,} returns container id \"92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af\""
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.003074146+08:00" level=info msg="StartContainer for \"92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af\""
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.017172458+08:00" level=info msg="TearDown network for sandbox \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" successfully"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.017245453+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" returns successfully"
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: I0402 11:10:23.017534    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d2d26ef245e6de97c1bd240718f49c9d29946487b85a5ffbb6f4602fe851e77a
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.019438943+08:00" level=info msg="RemoveContainer for \"d2d26ef245e6de97c1bd240718f49c9d29946487b85a5ffbb6f4602fe851e77a\""
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.031483026+08:00" level=info msg="RemoveContainer for \"d2d26ef245e6de97c1bd240718f49c9d29946487b85a5ffbb6f4602fe851e77a\" returns successfully"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.032297596+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,}"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.032580774+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,} failed, error" error="failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: E0402 11:10:23.032811    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: E0402 11:10:23.032902    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: E0402 11:10:23.032937    4781 kuberuntime_manager.go:755] createPodSandbox for pod "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: E0402 11:10:23.033027    4781 pod_workers.go:191] Error syncing pod a5c9f638-c13f-4618-9c2a-9d56ce52f483 ("gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)"), skipping: failed to "CreatePodSandbox" for "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" with CreatePodSandboxError: "CreatePodSandbox for pod \"gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)\" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.168749129+08:00" level=info msg="StartContainer for \"92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af\" returns successfully"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.268610358+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:fluentbit-zmjjt,Uid:1014ef35-66b7-4144-8d62-cf6d3dd9b69a,Namespace:openstack,Attempt:0,}"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.299919278+08:00" level=info msg="shim disconnected" id=92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.300030516+08:00" level=warning msg="cleaning up after shim disconnected" id=92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af namespace=k8s.io
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.300058632+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.301102474+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c pid=7625
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.322688083+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:23+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=7635\n"
Apr 02 11:10:23 node-9.domain.tld kernel: SPDMD-MISC:[INFO]spdmd_init:807 Loading spdmd shared library init: d3.6.0-406-gb4ca374 master  build at[2024-03-02-21:21:34] build on 4.18.0-147.5.1.es8_24.x86_64
Apr 02 11:10:23 node-9.domain.tld kernel: VBS:[INFO]vbs_init:6198 Loading vbs driver,version: d3.6.0-406-gb4ca374 master  build at[2024-03-02-21:22:05]
Apr 02 11:10:23 node-9.domain.tld kernel: VAM-MAIN:[INFO]vam_init:2259 Loading vam driver version : d3.6.0-406-gb4ca374 master  build at[2024-03-02-21:22:12]
Apr 02 11:10:23 node-9.domain.tld kernel: CLST:[INFO]clst_init:1468 Loading clst driver, version: d3.6.0-406-gb4ca374 master  build at[2024-03-02-21:22:25]
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.557532320+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fluentbit-zmjjt,Uid:1014ef35-66b7-4144-8d62-cf6d3dd9b69a,Namespace:openstack,Attempt:0,} returns sandbox id \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\""
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.563651534+08:00" level=info msg="CreateContainer within sandbox \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\" for container &ContainerMetadata{Name:init,Attempt:0,}"
Apr 02 11:10:23 node-9.domain.tld kernel: SPDMD-MISC:[INFO]spdmd_set:724 IBVersion=5.8,ffffffffb9554150
Apr 02 11:10:23 node-9.domain.tld systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount014586061.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit var-lib-containerd-tmpmounts-containerd\x2dmount014586061.mount has successfully entered the 'dead' state.
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.730590182+08:00" level=info msg="CreateContainer within sandbox \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\" for &ContainerMetadata{Name:init,Attempt:0,} returns container id \"e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6\""
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.731202844+08:00" level=info msg="StartContainer for \"e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6\""
Apr 02 11:10:23 node-9.domain.tld kernel: SPDMD-DEVICE:[INFO]spdmd_device_new:1137 Create device k8stor_04 0000000073cb0ff3
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Dev k8stor_04: unable to read RDB block 0
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel:  k8stor_04: unable to read partition table
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 137438953470, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: Buffer I/O error on dev k8stor_04, logical block 0, async page read
Apr 02 11:10:23 node-9.domain.tld kernel: nvme nvme4: creating 8 I/O queues.
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: I0402 11:10:23.931234    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: I0402 11:10:23.931804    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af
Apr 02 11:10:23 node-9.domain.tld kubelet[4781]: E0402 11:10:23.932391    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 10s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.932683584+08:00" level=info msg="RemoveContainer for \"9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4\""
Apr 02 11:10:23 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:23.941862808+08:00" level=info msg="RemoveContainer for \"9a1aa24c59f6b1234bdf1566e234eae57a99f8ea44d4007407a1f41663601cb4\" returns successfully"
Apr 02 11:10:23 node-9.domain.tld systemd-udevd[7906]: Process 'esstorage-udev-disk-hotplug add /dev/k8stor_04 ' failed with exit code 1.
Apr 02 11:10:24 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:24.030795134+08:00" level=info msg="StartContainer for \"e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6\" returns successfully"
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.310340    4781 secret.go:195] Couldn't get secret kube-system/default-token-b9c7f: failed to sync secret cache: timed out waiting for the condition
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.310353    4781 configmap.go:200] Couldn't get configMap kube-system/nfd-worker-conf: failed to sync configmap cache: timed out waiting for the condition
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.310510    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/3f04228b-0006-4b96-acca-351a4d101cfa-nfd-worker-conf podName:3f04228b-0006-4b96-acca-351a4d101cfa nodeName:}" failed. No retries permitted until 2024-04-02 11:10:25.310446879 +0800 CST m=+36.615347498 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"nfd-worker-conf\" (UniqueName: \"kubernetes.io/configmap/3f04228b-0006-4b96-acca-351a4d101cfa-nfd-worker-conf\") pod \"qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb\" (UID: \"3f04228b-0006-4b96-acca-351a4d101cfa\") : failed to sync configmap cache: timed out waiting for the condition"
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.310537    4781 configmap.go:200] Couldn't get configMap openstack/ovn-bin: failed to sync configmap cache: timed out waiting for the condition
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.310567    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/secret/3f04228b-0006-4b96-acca-351a4d101cfa-default-token-b9c7f podName:3f04228b-0006-4b96-acca-351a4d101cfa nodeName:}" failed. No retries permitted until 2024-04-02 11:10:25.310527975 +0800 CST m=+36.615428585 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"default-token-b9c7f\" (UniqueName: \"kubernetes.io/secret/3f04228b-0006-4b96-acca-351a4d101cfa-default-token-b9c7f\") pod \"qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb\" (UID: \"3f04228b-0006-4b96-acca-351a4d101cfa\") : failed to sync secret cache: timed out waiting for the condition"
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.310683    4781 nestedpendingoperations.go:301] Operation for "{volumeName:kubernetes.io/configmap/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-ovn-bin podName:6a2d4496-4f35-4f65-b4b6-e190f24c7be0 nodeName:}" failed. No retries permitted until 2024-04-02 11:10:25.310624364 +0800 CST m=+36.615524977 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"ovn-bin\" (UniqueName: \"kubernetes.io/configmap/6a2d4496-4f35-4f65-b4b6-e190f24c7be0-ovn-bin\") pod \"ovn-controller-lcv2p\" (UID: \"6a2d4496-4f35-4f65-b4b6-e190f24c7be0\") : failed to sync configmap cache: timed out waiting for the condition"
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: I0402 11:10:24.941526    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af
Apr 02 11:10:24 node-9.domain.tld kubelet[4781]: E0402 11:10:24.942051    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 10s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:10:25 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:25.485756729+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb,Uid:3f04228b-0006-4b96-acca-351a4d101cfa,Namespace:kube-system,Attempt:0,}"
Apr 02 11:10:25 node-9.domain.tld kernel: nvme nvme4: mapped 8/0/0 default/read/poll queues.
Apr 02 11:10:25 node-9.domain.tld kernel: nvme nvme4: new ctrl: NQN "vam_1", addr 192.168.0.41:4430
Apr 02 11:10:25 node-9.domain.tld systemd-udevd[8073]: Process 'esstorage-udev-disk-hotplug add /dev/nvme4n1 ' failed with exit code 1.
Apr 02 11:10:25 node-9.domain.tld systemd[1]: run-netns-cni\x2d473119a8\x2d5603\x2d4ebd\x2d509c\x2d11914deb9583.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d473119a8\x2d5603\x2d4ebd\x2d509c\x2d11914deb9583.mount has successfully entered the 'dead' state.
Apr 02 11:10:25 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:25.703906077+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:qgpu-manager-slc77,Uid:d56ec515-f683-49ae-af58-15c5ec1b769b,Namespace:imp-qgpu,Attempt:0,} failed, error" error="failed to setup network for sandbox \"2440553d07527384591066c49ac804a2eba356315eeb0dc141b8a53d661a7393\": Multus: [imp-qgpu/qgpu-manager-slc77]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:25 node-9.domain.tld kubelet[4781]: E0402 11:10:25.704269    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "2440553d07527384591066c49ac804a2eba356315eeb0dc141b8a53d661a7393": Multus: [imp-qgpu/qgpu-manager-slc77]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:25 node-9.domain.tld kubelet[4781]: E0402 11:10:25.704378    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "2440553d07527384591066c49ac804a2eba356315eeb0dc141b8a53d661a7393": Multus: [imp-qgpu/qgpu-manager-slc77]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:25 node-9.domain.tld kubelet[4781]: E0402 11:10:25.704412    4781 kuberuntime_manager.go:755] createPodSandbox for pod "qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "2440553d07527384591066c49ac804a2eba356315eeb0dc141b8a53d661a7393": Multus: [imp-qgpu/qgpu-manager-slc77]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:25 node-9.domain.tld kubelet[4781]: E0402 11:10:25.704509    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "CreatePodSandbox" for "qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)" with CreatePodSandboxError: "CreatePodSandbox for pod \"qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"2440553d07527384591066c49ac804a2eba356315eeb0dc141b8a53d661a7393\": Multus: [imp-qgpu/qgpu-manager-slc77]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:25 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:25.803420478+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:ovn-controller-lcv2p,Uid:6a2d4496-4f35-4f65-b4b6-e190f24c7be0,Namespace:openstack,Attempt:0,}"
Apr 02 11:10:25 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:25.839513983+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee pid=8158
Apr 02 11:10:25 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:25.988082101+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ovn-controller-lcv2p,Uid:6a2d4496-4f35-4f65-b4b6-e190f24c7be0,Namespace:openstack,Attempt:0,} returns sandbox id \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\""
Apr 02 11:10:25 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:25.994173516+08:00" level=info msg="CreateContainer within sandbox \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\" for container &ContainerMetadata{Name:init,Attempt:0,}"
Apr 02 11:10:26 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:26.047835082+08:00" level=info msg="CreateContainer within sandbox \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\" for &ContainerMetadata{Name:init,Attempt:0,} returns container id \"543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f\""
Apr 02 11:10:26 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:26.048408877+08:00" level=info msg="StartContainer for \"543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f\""
Apr 02 11:10:26 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:26.227933650+08:00" level=info msg="StartContainer for \"543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f\" returns successfully"
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.128734    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "sys" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-sys") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.128809    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "multipath-etc" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-multipath-etc") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.128923    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "node-exporter-bin" (UniqueName: "kubernetes.io/configmap/2684d301-20d6-43f1-8d3c-1128c2112486-node-exporter-bin") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129037    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-9h5bq" (UniqueName: "kubernetes.io/secret/e74039f9-9bee-4e6a-ade7-535f5d7a309e-default-token-9h5bq") pod "alluxio-fuse-hjvv5" (UID: "e74039f9-9bee-4e6a-ade7-535f5d7a309e")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129082    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "libvirt-runtimes" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-libvirt-runtimes") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129150    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-run") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129199    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "opt-eventmonitor" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-opt-eventmonitor") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129247    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubevirt-virt-handler-server-certs" (UniqueName: "kubernetes.io/secret/8c139073-51af-4078-b62c-d0b409dfacd4-kubevirt-virt-handler-server-certs") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129296    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "pod-etc-node-exporter" (UniqueName: "kubernetes.io/empty-dir/2684d301-20d6-43f1-8d3c-1128c2112486-pod-etc-node-exporter") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129408    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "profile-data" (UniqueName: "kubernetes.io/empty-dir/8c139073-51af-4078-b62c-d0b409dfacd4-profile-data") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129483    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubevirt-virt-handler-certs" (UniqueName: "kubernetes.io/secret/8c139073-51af-4078-b62c-d0b409dfacd4-kubevirt-virt-handler-certs") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129643    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "crictl" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-crictl") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129688    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "dev-ipmi" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-dev-ipmi") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129819    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "node-exporter-etc" (UniqueName: "kubernetes.io/configmap/2684d301-20d6-43f1-8d3c-1128c2112486-node-exporter-etc") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129869    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubectl" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-kubectl") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.129912    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubelet-pods" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-kubelet-pods") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130030    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubevirt-handler-token-bqqvl" (UniqueName: "kubernetes.io/secret/8c139073-51af-4078-b62c-d0b409dfacd4-kubevirt-handler-token-bqqvl") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130165    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "aipaas-public-alluxio-pv-data-alluxio-fuse-hostpath" (UniqueName: "kubernetes.io/host-path/e74039f9-9bee-4e6a-ade7-535f5d7a309e-aipaas-public-alluxio-pv-data-alluxio-fuse-hostpath") pod "alluxio-fuse-hjvv5" (UID: "e74039f9-9bee-4e6a-ade7-535f5d7a309e")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130272    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "ceph-etc" (UniqueName: "kubernetes.io/configmap/2684d301-20d6-43f1-8d3c-1128c2112486-ceph-etc") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130394    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "multipath-conf-etc" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-multipath-conf-etc") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130457    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "node-labeller" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-node-labeller") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130498    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "proc" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-proc") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130542    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "containerd-sock" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-containerd-sock") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130593    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-node-exporter-token-qw676" (UniqueName: "kubernetes.io/secret/2684d301-20d6-43f1-8d3c-1128c2112486-prometheus-node-exporter-token-qw676") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130687    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "virt-share-dir" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-virt-share-dir") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130762    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "boot-dir" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-boot-dir") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130837    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "varlog" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-varlog") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130884    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "collector-data" (UniqueName: "kubernetes.io/empty-dir/2684d301-20d6-43f1-8d3c-1128c2112486-collector-data") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.130966    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "dev" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-dev") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.131016    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "virt-private-dir" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-virt-private-dir") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.131093    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kubelet-pods-shortened" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-kubelet-pods-shortened") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.131129    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "virt-lib-dir" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-virt-lib-dir") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.131158    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "device-plugin" (UniqueName: "kubernetes.io/host-path/8c139073-51af-4078-b62c-d0b409dfacd4-device-plugin") pod "virt-handler-9ksxc" (UID: "8c139073-51af-4078-b62c-d0b409dfacd4")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.131188    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "alluxio-alluxio-conf" (UniqueName: "kubernetes.io/configmap/e74039f9-9bee-4e6a-ade7-535f5d7a309e-alluxio-alluxio-conf") pod "alluxio-fuse-hjvv5" (UID: "e74039f9-9bee-4e6a-ade7-535f5d7a309e")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.131239    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "crictl-etc" (UniqueName: "kubernetes.io/host-path/2684d301-20d6-43f1-8d3c-1128c2112486-crictl-etc") pod "node-exporter-7wq8b" (UID: "2684d301-20d6-43f1-8d3c-1128c2112486")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.231859    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "process-exporter-config" (UniqueName: "kubernetes.io/empty-dir/9c5e867e-3b9d-4b65-a2c8-f6da712c09df-process-exporter-config") pod "process-exporter-6s6t5" (UID: "9c5e867e-3b9d-4b65-a2c8-f6da712c09df")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.233961    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "process-exporter-bin" (UniqueName: "kubernetes.io/configmap/9c5e867e-3b9d-4b65-a2c8-f6da712c09df-process-exporter-bin") pod "process-exporter-6s6t5" (UID: "9c5e867e-3b9d-4b65-a2c8-f6da712c09df")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.234140    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "proc" (UniqueName: "kubernetes.io/host-path/9c5e867e-3b9d-4b65-a2c8-f6da712c09df-proc") pod "process-exporter-6s6t5" (UID: "9c5e867e-3b9d-4b65-a2c8-f6da712c09df")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.234281    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "process-exporter-etc" (UniqueName: "kubernetes.io/configmap/9c5e867e-3b9d-4b65-a2c8-f6da712c09df-process-exporter-etc") pod "process-exporter-6s6t5" (UID: "9c5e867e-3b9d-4b65-a2c8-f6da712c09df")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.237479    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "prometheus-process-exporter-token-4skjf" (UniqueName: "kubernetes.io/secret/9c5e867e-3b9d-4b65-a2c8-f6da712c09df-prometheus-process-exporter-token-4skjf") pod "process-exporter-6s6t5" (UID: "9c5e867e-3b9d-4b65-a2c8-f6da712c09df")
Apr 02 11:10:27 node-9.domain.tld kubelet[4781]: I0402 11:10:27.238976    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "sys" (UniqueName: "kubernetes.io/host-path/9c5e867e-3b9d-4b65-a2c8-f6da712c09df-sys") pod "process-exporter-6s6t5" (UID: "9c5e867e-3b9d-4b65-a2c8-f6da712c09df")
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.255893928+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,}"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.375247438+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:process-exporter-6s6t5,Uid:9c5e867e-3b9d-4b65-a2c8-f6da712c09df,Namespace:openstack,Attempt:0,}"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.407095813+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f pid=8304
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.510988203+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:virt-handler-9ksxc,Uid:8c139073-51af-4078-b62c-d0b409dfacd4,Namespace:kubevirt,Attempt:0,}"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.539858381+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:node-exporter-7wq8b,Uid:2684d301-20d6-43f1-8d3c-1128c2112486,Namespace:openstack,Attempt:0,}"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.570290409+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2 pid=8354
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.594657932+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:process-exporter-6s6t5,Uid:9c5e867e-3b9d-4b65-a2c8-f6da712c09df,Namespace:openstack,Attempt:0,} returns sandbox id \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\""
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.601077763+08:00" level=info msg="CreateContainer within sandbox \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\" for container &ContainerMetadata{Name:init,Attempt:0,}"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.661862567+08:00" level=info msg="CreateContainer within sandbox \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\" for &ContainerMetadata{Name:init,Attempt:0,} returns container id \"5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82\""
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.662475634+08:00" level=info msg="StartContainer for \"5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82\""
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.732117052+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:node-exporter-7wq8b,Uid:2684d301-20d6-43f1-8d3c-1128c2112486,Namespace:openstack,Attempt:0,} returns sandbox id \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\""
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.738538253+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for container &ContainerMetadata{Name:init,Attempt:0,}"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.795375442+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for &ContainerMetadata{Name:init,Attempt:0,} returns container id \"947673deee5679246096bd17bf7a8346f3605593b542a8d00e67b5e6a87b8fe5\""
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.795910835+08:00" level=info msg="StartContainer for \"947673deee5679246096bd17bf7a8346f3605593b542a8d00e67b5e6a87b8fe5\""
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.873889820+08:00" level=info msg="StartContainer for \"5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82\" returns successfully"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.997450606+08:00" level=info msg="shim disconnected" id=5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.997521181+08:00" level=warning msg="cleaning up after shim disconnected" id=5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82 namespace=k8s.io
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.997536282+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:27 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:27.999300555+08:00" level=info msg="StartContainer for \"947673deee5679246096bd17bf7a8346f3605593b542a8d00e67b5e6a87b8fe5\" returns successfully"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.024531315+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:28+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=8601\n"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.129998198+08:00" level=info msg="shim disconnected" id=947673deee5679246096bd17bf7a8346f3605593b542a8d00e67b5e6a87b8fe5
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.130091520+08:00" level=warning msg="cleaning up after shim disconnected" id=947673deee5679246096bd17bf7a8346f3605593b542a8d00e67b5e6a87b8fe5 namespace=k8s.io
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.130116148+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.155213267+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:28+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=8702\n"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.352947798+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:csi-node-scaleflashplugin-x4k72,Uid:805dbfe7-6703-408b-aa8f-ed778c183e6b,Namespace:scaleflash,Attempt:0,} failed, error" error="failed to setup network for sandbox \"9def084b8fbfa5f83d00a5ead176d4f1780c99619e12f3ec08f5cb239f111b14\": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.353360    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "9def084b8fbfa5f83d00a5ead176d4f1780c99619e12f3ec08f5cb239f111b14": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.353469    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "9def084b8fbfa5f83d00a5ead176d4f1780c99619e12f3ec08f5cb239f111b14": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.353502    4781 kuberuntime_manager.go:755] createPodSandbox for pod "csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "9def084b8fbfa5f83d00a5ead176d4f1780c99619e12f3ec08f5cb239f111b14": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.353607    4781 pod_workers.go:191] Error syncing pod 805dbfe7-6703-408b-aa8f-ed778c183e6b ("csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)"), skipping: failed to "CreatePodSandbox" for "csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)" with CreatePodSandboxError: "CreatePodSandbox for pod \"csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"9def084b8fbfa5f83d00a5ead176d4f1780c99619e12f3ec08f5cb239f111b14\": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.391892985+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-container-toolkit-daemonset-qr9nk,Uid:505d6e19-c308-4800-85b7-d8a906c1ac02,Namespace:imp-qgpu,Attempt:0,} failed, error" error="failed to setup network for sandbox \"67cd85f8059b5c583703a2f2bc7453444886e3ad9ccf9f0b3b0831368e98fb9d\": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.392220    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "67cd85f8059b5c583703a2f2bc7453444886e3ad9ccf9f0b3b0831368e98fb9d": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.392307    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "67cd85f8059b5c583703a2f2bc7453444886e3ad9ccf9f0b3b0831368e98fb9d": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.392355    4781 kuberuntime_manager.go:755] createPodSandbox for pod "nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "67cd85f8059b5c583703a2f2bc7453444886e3ad9ccf9f0b3b0831368e98fb9d": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:28 node-9.domain.tld kubelet[4781]: E0402 11:10:28.392459    4781 pod_workers.go:191] Error syncing pod 505d6e19-c308-4800-85b7-d8a906c1ac02 ("nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)"), skipping: failed to "CreatePodSandbox" for "nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)" with CreatePodSandboxError: "CreatePodSandbox for pod \"nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"67cd85f8059b5c583703a2f2bc7453444886e3ad9ccf9f0b3b0831368e98fb9d\": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:28 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-5276458dac86723bd1e6805f856c03517e124cbee52013699cd2ee9ed5799a82-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:28 node-9.domain.tld systemd[1]: run-netns-cni\x2d0f7e69ba\x2d89d8\x2d7d0a\x2d7c27\x2d232361b2c14f.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d0f7e69ba\x2d89d8\x2d7d0a\x2d7c27\x2d232361b2c14f.mount has successfully entered the 'dead' state.
Apr 02 11:10:28 node-9.domain.tld systemd[1]: run-netns-cni\x2d40fabc56\x2d4002\x2da172\x2de65a\x2d808b73f45317.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d40fabc56\x2d4002\x2da172\x2de65a\x2d808b73f45317.mount has successfully entered the 'dead' state.
Apr 02 11:10:28 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.808879749+08:00" level=info msg="shim disconnected" id=84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.808999022+08:00" level=warning msg="cleaning up after shim disconnected" id=84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5 namespace=k8s.io
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.809025596+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.834372100+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:28+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=8841\n"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.972704095+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-heat-engine:latest\""
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.974345431+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:csi-node-scaleflashplugin-x4k72,Uid:805dbfe7-6703-408b-aa8f-ed778c183e6b,Namespace:scaleflash,Attempt:0,}"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.974347161+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-container-toolkit-daemonset-qr9nk,Uid:505d6e19-c308-4800-85b7-d8a906c1ac02,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.975753255+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for container &ContainerMetadata{Name:node-exporter,Attempt:0,}"
Apr 02 11:10:28 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:28.985988933+08:00" level=info msg="CreateContainer within sandbox \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\" for container &ContainerMetadata{Name:esdm-init,Attempt:1,}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.049702662+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for &ContainerMetadata{Name:node-exporter,Attempt:0,} returns container id \"98c981b66708ebeb7bed656a583e27211c9b12dc178a028b54f459b59d10dab9\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.050201534+08:00" level=info msg="StartContainer for \"98c981b66708ebeb7bed656a583e27211c9b12dc178a028b54f459b59d10dab9\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.070746754+08:00" level=info msg="CreateContainer within sandbox \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\" for &ContainerMetadata{Name:esdm-init,Attempt:1,} returns container id \"3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.071322965+08:00" level=info msg="StartContainer for \"3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.072152727+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-heat-engine:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.077647933+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:47d73002a477ef63a584d10819587a32d50d5871be22ae2e7f281ce3214c1341,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.086342454+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-heat-engine:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.097503261+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-heat-engine@sha256:cc5a8f9ba4dcb798f6415beec7a64a058f83616e5674bb9685168c85352f72ca,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.101706854+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-heat-engine:latest\" returns image reference \"sha256:47d73002a477ef63a584d10819587a32d50d5871be22ae2e7f281ce3214c1341\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.119369260+08:00" level=info msg="CreateContainer within sandbox \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\" for container &ContainerMetadata{Name:process-exporter-init,Attempt:0,}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.204301940+08:00" level=info msg="CreateContainer within sandbox \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\" for &ContainerMetadata{Name:process-exporter-init,Attempt:0,} returns container id \"4771108ad1961eec5e088b495e7ab6eb7d43b12e4bd9d77ebbe5fd5cbd50f830\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.204939397+08:00" level=info msg="StartContainer for \"4771108ad1961eec5e088b495e7ab6eb7d43b12e4bd9d77ebbe5fd5cbd50f830\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.228872040+08:00" level=info msg="StartContainer for \"3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa\" returns successfully"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.267442057+08:00" level=info msg="StartContainer for \"98c981b66708ebeb7bed656a583e27211c9b12dc178a028b54f459b59d10dab9\" returns successfully"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.346172992+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for container &ContainerMetadata{Name:smartmon-collector,Attempt:0,}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.378194735+08:00" level=info msg="StartContainer for \"4771108ad1961eec5e088b495e7ab6eb7d43b12e4bd9d77ebbe5fd5cbd50f830\" returns successfully"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.427271288+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for &ContainerMetadata{Name:smartmon-collector,Attempt:0,} returns container id \"94d10d8222cfe726bda634609d2ce1e1f40464ec0f240701fc87d1e8c6933fda\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.427883529+08:00" level=info msg="StartContainer for \"94d10d8222cfe726bda634609d2ce1e1f40464ec0f240701fc87d1e8c6933fda\""
Apr 02 11:10:29 node-9.domain.tld kernel: SPDMD-DEVICE:[INFO]spdmd_device_disk_add:1787 Add disk /dev/nvme4n1 to k8stor_04
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.510257746+08:00" level=info msg="shim disconnected" id=4771108ad1961eec5e088b495e7ab6eb7d43b12e4bd9d77ebbe5fd5cbd50f830
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.510348262+08:00" level=warning msg="cleaning up after shim disconnected" id=4771108ad1961eec5e088b495e7ab6eb7d43b12e4bd9d77ebbe5fd5cbd50f830 namespace=k8s.io
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.510375343+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.537257261+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:29+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=9070\n"
Apr 02 11:10:29 node-9.domain.tld kernel: nvme nvme5: creating 8 I/O queues.
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.598483863+08:00" level=info msg="StartContainer for \"94d10d8222cfe726bda634609d2ce1e1f40464ec0f240701fc87d1e8c6933fda\" returns successfully"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.685497761+08:00" level=info msg="shim disconnected" id=e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.685626479+08:00" level=warning msg="cleaning up after shim disconnected" id=e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6 namespace=k8s.io
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.685652188+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.706066336+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for container &ContainerMetadata{Name:node-script-collector,Attempt:0,}"
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.713504772+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:29+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=9162\n"
Apr 02 11:10:29 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-e1c7dd612a701a0af5764be0de4d45a152d97914888824ea63538774e0e5b7e6-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.773022752+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for &ContainerMetadata{Name:node-script-collector,Attempt:0,} returns container id \"71f3fb1f3a5026ce918d447699d304870908f92388e7eec48bdbcdb9df18e614\""
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.773806032+08:00" level=info msg="StartContainer for \"71f3fb1f3a5026ce918d447699d304870908f92388e7eec48bdbcdb9df18e614\""
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:29 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:29.976776982+08:00" level=info msg="StartContainer for \"71f3fb1f3a5026ce918d447699d304870908f92388e7eec48bdbcdb9df18e614\" returns successfully"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.008164346+08:00" level=info msg="CreateContainer within sandbox \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\" for container &ContainerMetadata{Name:process-exporter,Attempt:0,}"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.012395374+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for container &ContainerMetadata{Name:event-monitor,Attempt:0,}"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.015512394+08:00" level=info msg="CreateContainer within sandbox \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\" for container &ContainerMetadata{Name:fluentbit-init,Attempt:0,}"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.103839620+08:00" level=info msg="CreateContainer within sandbox \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\" for &ContainerMetadata{Name:fluentbit-init,Attempt:0,} returns container id \"ce1191723592b4daa9c4c3830bc71e957661ca7c02d783cd870e026aeb6a3556\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.104474000+08:00" level=info msg="StartContainer for \"ce1191723592b4daa9c4c3830bc71e957661ca7c02d783cd870e026aeb6a3556\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.111033246+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for &ContainerMetadata{Name:event-monitor,Attempt:0,} returns container id \"150b62b7fe2c509e854014deedaba17fafcdff8852622dfb2f87cb7be27b5692\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.111926247+08:00" level=info msg="StartContainer for \"150b62b7fe2c509e854014deedaba17fafcdff8852622dfb2f87cb7be27b5692\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.116014592+08:00" level=info msg="CreateContainer within sandbox \"7119a05ac1835a1a6c8ff609745f20a6e2ee7954e2ffd75295e6ad683efa198f\" for &ContainerMetadata{Name:process-exporter,Attempt:0,} returns container id \"9bcb28856a9fd31ca2fb1362524a3dfbc22644d98f40bcc197bfb39886645e2c\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.116687122+08:00" level=info msg="StartContainer for \"9bcb28856a9fd31ca2fb1362524a3dfbc22644d98f40bcc197bfb39886645e2c\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.129494523+08:00" level=info msg="shim disconnected" id=3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.129601470+08:00" level=warning msg="cleaning up after shim disconnected" id=3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa namespace=k8s.io
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.129626369+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.154316321+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:30+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=9353\n"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.318555770+08:00" level=info msg="StartContainer for \"ce1191723592b4daa9c4c3830bc71e957661ca7c02d783cd870e026aeb6a3556\" returns successfully"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.321002209+08:00" level=info msg="StartContainer for \"150b62b7fe2c509e854014deedaba17fafcdff8852622dfb2f87cb7be27b5692\" returns successfully"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.330283743+08:00" level=info msg="StartContainer for \"9bcb28856a9fd31ca2fb1362524a3dfbc22644d98f40bcc197bfb39886645e2c\" returns successfully"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.333637928+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for container &ContainerMetadata{Name:ipmi-exporter,Attempt:0,}"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.436129473+08:00" level=info msg="CreateContainer within sandbox \"d084bb55a707105b240786c8540c00c5cbd383ddfa2ddae9cc7917ba2d9630f2\" for &ContainerMetadata{Name:ipmi-exporter,Attempt:0,} returns container id \"d34a2a2d50722528bfbd244f2cc58da50b7532f58a6a84707ec4344d29014c34\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.436881650+08:00" level=info msg="StartContainer for \"d34a2a2d50722528bfbd244f2cc58da50b7532f58a6a84707ec4344d29014c34\""
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.450342074+08:00" level=info msg="shim disconnected" id=ce1191723592b4daa9c4c3830bc71e957661ca7c02d783cd870e026aeb6a3556
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.450437042+08:00" level=warning msg="cleaning up after shim disconnected" id=ce1191723592b4daa9c4c3830bc71e957661ca7c02d783cd870e026aeb6a3556 namespace=k8s.io
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.450472741+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.474767492+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:30+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=9489\n"
Apr 02 11:10:30 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:30.638849461+08:00" level=info msg="StartContainer for \"d34a2a2d50722528bfbd244f2cc58da50b7532f58a6a84707ec4344d29014c34\" returns successfully"
Apr 02 11:10:30 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-3fa07814b2dd7664d682528f3137063ea76523bf389269ef22790b6098476aaa-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: I0402 11:10:31.014563    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: I0402 11:10:31.015065    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.016036580+08:00" level=info msg="RemoveContainer for \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\""
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.016308585+08:00" level=info msg="RemoveContainer for \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\""
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.016443441+08:00" level=error msg="RemoveContainer for \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\" failed" error="failed to set removing state for container \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\": container is already in removing state"
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: E0402 11:10:31.016764    4781 remote_runtime.go:294] RemoveContainer "84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5" from runtime service failed: rpc error: code = Unknown desc = failed to set removing state for container "84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5": container is already in removing state
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: E0402 11:10:31.016861    4781 kuberuntime_container.go:705] failed to remove pod init container "esdm-init": rpc error: code = Unknown desc = failed to set removing state for container "84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5": container is already in removing state; Skipping pod "easystack-dm-jm6ch_openstack(57941549-7588-41af-a4fb-c364aec82fbe)"
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.027978494+08:00" level=info msg="RemoveContainer for \"84ddf5a172c98bec82d69bd90c542119e02d0c7d12aba5413f72f9f5bbfa15b5\" returns successfully"
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.038521265+08:00" level=info msg="CreateContainer within sandbox \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\" for container &ContainerMetadata{Name:easystack-dm,Attempt:0,}"
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.048680403+08:00" level=info msg="CreateContainer within sandbox \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\" for container &ContainerMetadata{Name:fluentbit,Attempt:0,}"
Apr 02 11:10:31 node-9.domain.tld systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount014448101.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit var-lib-containerd-tmpmounts-containerd\x2dmount014448101.mount has successfully entered the 'dead' state.
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.129130945+08:00" level=info msg="CreateContainer within sandbox \"2f8416f05143a44ca1a59627ccc8adc26d029ff2009fbef2404d249cb0075e4c\" for &ContainerMetadata{Name:fluentbit,Attempt:0,} returns container id \"d3385bf9ca0d9e8aeb1efd10f633770c7d9799c311227ebd1974e1364e073480\""
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.129827992+08:00" level=info msg="StartContainer for \"d3385bf9ca0d9e8aeb1efd10f633770c7d9799c311227ebd1974e1364e073480\""
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.133863888+08:00" level=info msg="CreateContainer within sandbox \"44cf7931d8dde8004c912c99ddb0509cad8c4e04aa4c032ef039d2d9d408e97c\" for &ContainerMetadata{Name:easystack-dm,Attempt:0,} returns container id \"06785c05ddd23046433c7904a10c934f97470be3bcaa27fd7dbb4c91e3d3cb21\""
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.134504397+08:00" level=info msg="StartContainer for \"06785c05ddd23046433c7904a10c934f97470be3bcaa27fd7dbb4c91e3d3cb21\""
Apr 02 11:10:31 node-9.domain.tld kernel: nvme nvme5: mapped 8/0/0 default/read/poll queues.
Apr 02 11:10:31 node-9.domain.tld kernel: nvme nvme5: new ctrl: NQN "vam_2", addr 192.168.0.42:4430
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.299185087+08:00" level=info msg="StartContainer for \"06785c05ddd23046433c7904a10c934f97470be3bcaa27fd7dbb4c91e3d3cb21\" returns successfully"
Apr 02 11:10:31 node-9.domain.tld systemd-udevd[9881]: Process 'esstorage-udev-disk-hotplug add /dev/nvme5n1 ' failed with exit code 1.
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.354479152+08:00" level=info msg="StartContainer for \"d3385bf9ca0d9e8aeb1efd10f633770c7d9799c311227ebd1974e1364e073480\" returns successfully"
Apr 02 11:10:31 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:31.478906553+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb,Uid:3f04228b-0006-4b96-acca-351a4d101cfa,Namespace:kube-system,Attempt:0,} failed, error" error="failed to setup network for sandbox \"236a2ed26640c6bdf8a36f527e1699e63627751b4fd928ac4fed1f10824bb2b3\": Multus: [kube-system/qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: E0402 11:10:31.479258    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "236a2ed26640c6bdf8a36f527e1699e63627751b4fd928ac4fed1f10824bb2b3": Multus: [kube-system/qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: E0402 11:10:31.479360    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "236a2ed26640c6bdf8a36f527e1699e63627751b4fd928ac4fed1f10824bb2b3": Multus: [kube-system/qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: E0402 11:10:31.479398    4781 kuberuntime_manager.go:755] createPodSandbox for pod "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "236a2ed26640c6bdf8a36f527e1699e63627751b4fd928ac4fed1f10824bb2b3": Multus: [kube-system/qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:31 node-9.domain.tld kubelet[4781]: E0402 11:10:31.479523    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "CreatePodSandbox" for "qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)" with CreatePodSandboxError: "CreatePodSandbox for pod \"qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"236a2ed26640c6bdf8a36f527e1699e63627751b4fd928ac4fed1f10824bb2b3\": Multus: [kube-system/qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:31 node-9.domain.tld systemd[1]: run-netns-cni\x2d0ba59316\x2da2aa\x2da27a\x2d9a0c\x2d515ec4557a0d.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d0ba59316\x2da2aa\x2da27a\x2d9a0c\x2d515ec4557a0d.mount has successfully entered the 'dead' state.
Apr 02 11:10:32 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:32 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:32.734275314+08:00" level=info msg="shim disconnected" id=543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f
Apr 02 11:10:32 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:32.734379694+08:00" level=warning msg="cleaning up after shim disconnected" id=543f3a48338432331e34d1694bdc53431790d7b9f6b594fb0ca27362b770ad3f namespace=k8s.io
Apr 02 11:10:32 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:32.734404819+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:32 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:32.762048433+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:32+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=10311\n"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.042381232+08:00" level=info msg="CreateContainer within sandbox \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\" for container &ContainerMetadata{Name:ovn-controller-init,Attempt:0,}"
Apr 02 11:10:33 node-9.domain.tld systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount704441777.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit var-lib-containerd-tmpmounts-containerd\x2dmount704441777.mount has successfully entered the 'dead' state.
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.127490475+08:00" level=info msg="CreateContainer within sandbox \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\" for &ContainerMetadata{Name:ovn-controller-init,Attempt:0,} returns container id \"b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c\""
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.128170827+08:00" level=info msg="StartContainer for \"b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c\""
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.337091287+08:00" level=info msg="StartContainer for \"b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c\" returns successfully"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.473191987+08:00" level=error msg="Failed to destroy network for sandbox \"067598d318513cc195e17fef35c29416081b0f4dd410f160c465eeae5a36f3c0\"" error="delegateDel: error invoking ConflistDel - \"kube-ovn\": conflistDel: error in getting result from DelNetworkList: Post \"http://dummy/api/v1/del\": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.512902525+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,} failed, error" error="failed to setup network for sandbox \"067598d318513cc195e17fef35c29416081b0f4dd410f160c465eeae5a36f3c0\": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network \"kube-ovn\": delegateAdd: error invoking conflistAdd - \"kube-ovn\": conflistAdd: error in getting result from AddNetworkList: Post \"http://dummy/api/v1/add\": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory"
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.513298    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "067598d318513cc195e17fef35c29416081b0f4dd410f160c465eeae5a36f3c0": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network "kube-ovn": delegateAdd: error invoking conflistAdd - "kube-ovn": conflistAdd: error in getting result from AddNetworkList: Post "http://dummy/api/v1/add": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.513398    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "067598d318513cc195e17fef35c29416081b0f4dd410f160c465eeae5a36f3c0": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network "kube-ovn": delegateAdd: error invoking conflistAdd - "kube-ovn": conflistAdd: error in getting result from AddNetworkList: Post "http://dummy/api/v1/add": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.513432    4781 kuberuntime_manager.go:755] createPodSandbox for pod "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "067598d318513cc195e17fef35c29416081b0f4dd410f160c465eeae5a36f3c0": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network "kube-ovn": delegateAdd: error invoking conflistAdd - "kube-ovn": conflistAdd: error in getting result from AddNetworkList: Post "http://dummy/api/v1/add": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.513553    4781 pod_workers.go:191] Error syncing pod e74039f9-9bee-4e6a-ade7-535f5d7a309e ("alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)"), skipping: failed to "CreatePodSandbox" for "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" with CreatePodSandboxError: "CreatePodSandbox for pod \"alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"067598d318513cc195e17fef35c29416081b0f4dd410f160c465eeae5a36f3c0\": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network \"kube-ovn\": delegateAdd: error invoking conflistAdd - \"kube-ovn\": conflistAdd: error in getting result from AddNetworkList: Post \"http://dummy/api/v1/add\": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.566882598+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:virt-handler-9ksxc,Uid:8c139073-51af-4078-b62c-d0b409dfacd4,Namespace:kubevirt,Attempt:0,} failed, error" error="failed to setup network for sandbox \"0830cbe1645abe8359b5f498013974c748dbf75b31a1a38743b116e185f24d69\": Multus: [kubevirt/virt-handler-9ksxc]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.567220    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "0830cbe1645abe8359b5f498013974c748dbf75b31a1a38743b116e185f24d69": Multus: [kubevirt/virt-handler-9ksxc]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.567294    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "virt-handler-9ksxc_kubevirt(8c139073-51af-4078-b62c-d0b409dfacd4)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "0830cbe1645abe8359b5f498013974c748dbf75b31a1a38743b116e185f24d69": Multus: [kubevirt/virt-handler-9ksxc]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.567327    4781 kuberuntime_manager.go:755] createPodSandbox for pod "virt-handler-9ksxc_kubevirt(8c139073-51af-4078-b62c-d0b409dfacd4)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "0830cbe1645abe8359b5f498013974c748dbf75b31a1a38743b116e185f24d69": Multus: [kubevirt/virt-handler-9ksxc]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: E0402 11:10:33.567416    4781 pod_workers.go:191] Error syncing pod 8c139073-51af-4078-b62c-d0b409dfacd4 ("virt-handler-9ksxc_kubevirt(8c139073-51af-4078-b62c-d0b409dfacd4)"), skipping: failed to "CreatePodSandbox" for "virt-handler-9ksxc_kubevirt(8c139073-51af-4078-b62c-d0b409dfacd4)" with CreatePodSandboxError: "CreatePodSandbox for pod \"virt-handler-9ksxc_kubevirt(8c139073-51af-4078-b62c-d0b409dfacd4)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"0830cbe1645abe8359b5f498013974c748dbf75b31a1a38743b116e185f24d69\": Multus: [kubevirt/virt-handler-9ksxc]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:33 node-9.domain.tld systemd[1]: run-netns-cni\x2d03cc8493\x2d7155\x2dd099\x2de1ce\x2d61587abcaa07.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d03cc8493\x2d7155\x2dd099\x2de1ce\x2d61587abcaa07.mount has successfully entered the 'dead' state.
Apr 02 11:10:33 node-9.domain.tld systemd[1]: run-netns-cni\x2d4c2ffb6d\x2d25bc\x2dfd0c\x2d3555\x2d6322a5f79de3.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d4c2ffb6d\x2d25bc\x2dfd0c\x2d3555\x2d6322a5f79de3.mount has successfully entered the 'dead' state.
Apr 02 11:10:33 node-9.domain.tld kubelet[4781]: I0402 11:10:33.816196    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: f51bf85f101eb27ff97729a39b070ee1b7341bc75acb6c3cbce36dc10f142621
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.817045640+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\""
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.819551122+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for container &ContainerMetadata{Name:sriov-network-config-daemon,Attempt:2,}"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.901646326+08:00" level=info msg="CreateContainer within sandbox \"4191692e410488dfe79d4cad8a1e8d19b9d28505dda792aaa67c36668c4e4248\" for &ContainerMetadata{Name:sriov-network-config-daemon,Attempt:2,} returns container id \"b93f6d40806d2107e2eeceb3b44871cdfda084fdab2263d0df5f9198e9d06b4d\""
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.902255380+08:00" level=info msg="StartContainer for \"b93f6d40806d2107e2eeceb3b44871cdfda084fdab2263d0df5f9198e9d06b4d\""
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.970868600+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.974431626+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5a8f90ce0e994d12674df242f817dd264e60e0a5cc8e68ae3d4a40d2855abab7,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.979657427+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.986830191+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-kube-ovn@sha256:6668872472676ff9dfc58ecbf91fb64e2bd29b56189e48a6cc5f69fb47a4d269,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.988253940+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\" returns image reference \"sha256:5a8f90ce0e994d12674df242f817dd264e60e0a5cc8e68ae3d4a40d2855abab7\""
Apr 02 11:10:33 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:33.992395779+08:00" level=info msg="CreateContainer within sandbox \"ae6b22ac1641df1183b3c0873114abce596519dd138ad97b6bcb500186658634\" for container &ContainerMetadata{Name:install-cni,Attempt:0,}"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.046446537+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:virt-handler-9ksxc,Uid:8c139073-51af-4078-b62c-d0b409dfacd4,Namespace:kubevirt,Attempt:0,}"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.046747010+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,}"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.080845075+08:00" level=info msg="StartContainer for \"b93f6d40806d2107e2eeceb3b44871cdfda084fdab2263d0df5f9198e9d06b4d\" returns successfully"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.113176600+08:00" level=info msg="CreateContainer within sandbox \"ae6b22ac1641df1183b3c0873114abce596519dd138ad97b6bcb500186658634\" for &ContainerMetadata{Name:install-cni,Attempt:0,} returns container id \"800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6\""
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.113881175+08:00" level=info msg="StartContainer for \"800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6\""
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.319030683+08:00" level=info msg="StartContainer for \"800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6\" returns successfully"
Apr 02 11:10:34 node-9.domain.tld systemd[1]: run-netns-cni\x2d2d146e42\x2d0874\x2d7ea7\x2dc9fe\x2d10b6e59e640e.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d2d146e42\x2d0874\x2d7ea7\x2dc9fe\x2d10b6e59e640e.mount has successfully entered the 'dead' state.
Apr 02 11:10:34 node-9.domain.tld systemd[1]: run-netns-cni\x2d7e9e3e41\x2db6ea\x2dede2\x2d3a94\x2d70e7150e7ab3.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d7e9e3e41\x2db6ea\x2dede2\x2d3a94\x2d70e7150e7ab3.mount has successfully entered the 'dead' state.
Apr 02 11:10:34 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.764950550+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-container-toolkit-daemonset-qr9nk,Uid:505d6e19-c308-4800-85b7-d8a906c1ac02,Namespace:imp-qgpu,Attempt:0,} failed, error" error="failed to setup network for sandbox \"4a2a294934bbbc57b2df50a2943ccc2c6640b3e82047a7bf40c395fd3f480361\": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.765396    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "4a2a294934bbbc57b2df50a2943ccc2c6640b3e82047a7bf40c395fd3f480361": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.765502    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "4a2a294934bbbc57b2df50a2943ccc2c6640b3e82047a7bf40c395fd3f480361": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.765541    4781 kuberuntime_manager.go:755] createPodSandbox for pod "nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "4a2a294934bbbc57b2df50a2943ccc2c6640b3e82047a7bf40c395fd3f480361": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.765673    4781 pod_workers.go:191] Error syncing pod 505d6e19-c308-4800-85b7-d8a906c1ac02 ("nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)"), skipping: failed to "CreatePodSandbox" for "nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)" with CreatePodSandboxError: "CreatePodSandbox for pod \"nvidia-container-toolkit-daemonset-qr9nk_imp-qgpu(505d6e19-c308-4800-85b7-d8a906c1ac02)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"4a2a294934bbbc57b2df50a2943ccc2c6640b3e82047a7bf40c395fd3f480361\": Multus: [imp-qgpu/nvidia-container-toolkit-daemonset-qr9nk]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.774252411+08:00" level=info msg="shim disconnected" id=800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.774312947+08:00" level=warning msg="cleaning up after shim disconnected" id=800b93b03577cbfdb98c37c6ddaea70f87fff7f2b26203246b56d4e2343058f6 namespace=k8s.io
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.774322871+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.778900799+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:csi-node-scaleflashplugin-x4k72,Uid:805dbfe7-6703-408b-aa8f-ed778c183e6b,Namespace:scaleflash,Attempt:0,} failed, error" error="failed to setup network for sandbox \"cb876e4e3348a8cb0014d747cb644dbf060f59ede78ceaf6f430013e4530ae30\": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.779373    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "cb876e4e3348a8cb0014d747cb644dbf060f59ede78ceaf6f430013e4530ae30": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.779491    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "cb876e4e3348a8cb0014d747cb644dbf060f59ede78ceaf6f430013e4530ae30": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.779528    4781 kuberuntime_manager.go:755] createPodSandbox for pod "csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "cb876e4e3348a8cb0014d747cb644dbf060f59ede78ceaf6f430013e4530ae30": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network "cni0": delegateAdd: error invoking conflistAdd - "cni0": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: E0402 11:10:34.779639    4781 pod_workers.go:191] Error syncing pod 805dbfe7-6703-408b-aa8f-ed778c183e6b ("csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)"), skipping: failed to "CreatePodSandbox" for "csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)" with CreatePodSandboxError: "CreatePodSandbox for pod \"csi-node-scaleflashplugin-x4k72_scaleflash(805dbfe7-6703-408b-aa8f-ed778c183e6b)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"cb876e4e3348a8cb0014d747cb644dbf060f59ede78ceaf6f430013e4530ae30\": Multus: [scaleflash/csi-node-scaleflashplugin-x4k72]: error adding container to network \"cni0\": delegateAdd: error invoking conflistAdd - \"cni0\": conflistAdd: error in getting result from AddNetworkList: open /run/flannel/subnet.env: no such file or directory"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.801166335+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:34+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=11155\n"
Apr 02 11:10:34 node-9.domain.tld kubelet[4781]: I0402 11:10:34.815403    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fb95fa60561f437948ca14085b22c493ac3ede522361ccbe0929bb4fe7253a98
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.819976711+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for container &ContainerMetadata{Name:kube-flannel,Attempt:2,}"
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.918873079+08:00" level=info msg="CreateContainer within sandbox \"79271abb9efc279e782a9f224d49581596c3dd701c420fb6980ba1e61d792899\" for &ContainerMetadata{Name:kube-flannel,Attempt:2,} returns container id \"b325a297a13f884018bfd33c07e46ba404369dd42c6d7844345233dc8ce0a4bd\""
Apr 02 11:10:34 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:34.919509073+08:00" level=info msg="StartContainer for \"b325a297a13f884018bfd33c07e46ba404369dd42c6d7844345233dc8ce0a4bd\""
Apr 02 11:10:34 node-9.domain.tld sshd[11172]: Accepted publickey for root from 127.0.0.1 port 44858 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:34 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:34 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:34 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:34 node-9.domain.tld systemd[11193]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: New session 3 of user root.
-- Subject: A new session 3 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 3 has been created for the user root.
-- 
-- The leading process of the session is 11172.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started Session 3 of user root.
-- Subject: Unit session-3.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-3.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.055832726+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\""
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.103492889+08:00" level=info msg="StartContainer for \"b325a297a13f884018bfd33c07e46ba404369dd42c6d7844345233dc8ce0a4bd\" returns successfully"
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Startup finished in 109ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 109585 microseconds.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld sshd[11172]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.113288144+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.119192966+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5a8f90ce0e994d12674df242f817dd264e60e0a5cc8e68ae3d4a40d2855abab7,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.122925967+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.128196072+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:hub.easystack.io/production/escloud-linux-source-kube-ovn@sha256:6668872472676ff9dfc58ecbf91fb64e2bd29b56189e48a6cc5f69fb47a4d269,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.129552573+08:00" level=info msg="PullImage \"hub.easystack.io/production/escloud-linux-source-kube-ovn:6.2.1\" returns image reference \"sha256:5a8f90ce0e994d12674df242f817dd264e60e0a5cc8e68ae3d4a40d2855abab7\""
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.134009356+08:00" level=info msg="CreateContainer within sandbox \"ae6b22ac1641df1183b3c0873114abce596519dd138ad97b6bcb500186658634\" for container &ContainerMetadata{Name:cni-server,Attempt:0,}"
Apr 02 11:10:35 node-9.domain.tld sshd[11223]: Received disconnect from 127.0.0.1 port 44858:11: disconnected by user
Apr 02 11:10:35 node-9.domain.tld sshd[11223]: Disconnected from user root 127.0.0.1 port 44858
Apr 02 11:10:35 node-9.domain.tld sshd[11172]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: Session 3 logged out. Waiting for processes to exit.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: session-3.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-3.scope has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: Removed session 3.
-- Subject: Session 3 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 3 has been terminated.
Apr 02 11:10:35 node-9.domain.tld kernel: SPDMD-DEVICE:[INFO]spdmd_device_disk_add:1787 Add disk /dev/nvme5n1 to k8stor_04
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.208472229+08:00" level=info msg="CreateContainer within sandbox \"ae6b22ac1641df1183b3c0873114abce596519dd138ad97b6bcb500186658634\" for &ContainerMetadata{Name:cni-server,Attempt:0,} returns container id \"677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25\""
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.209173037+08:00" level=info msg="StartContainer for \"677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25\""
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11193]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:35 node-9.domain.tld kernel: nvme nvme6: creating 8 I/O queues.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld kubelet[4781]: W0402 11:10:35.274787    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/memory/user.slice/user-0.slice/session-3.scope": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/user.slice/user-0.slice/session-3.scope: no such file or directory
Apr 02 11:10:35 node-9.domain.tld kubelet[4781]: W0402 11:10:35.274872    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/pids/user.slice/user-0.slice/session-3.scope": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/pids/user.slice/user-0.slice/session-3.scope: no such file or directory
Apr 02 11:10:35 node-9.domain.tld kubelet[4781]: W0402 11:10:35.274908    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/memory/user.slice/user-0.slice/user@0.service/dbus.socket": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/user.slice/user-0.slice/user@0.service/dbus.socket: no such file or directory
Apr 02 11:10:35 node-9.domain.tld kubelet[4781]: W0402 11:10:35.274982    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/pids/user.slice/user-0.slice/user@0.service/dbus.socket": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/pids/user.slice/user-0.slice/user@0.service/dbus.socket: no such file or directory
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:35.408821432+08:00" level=info msg="StartContainer for \"677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25\" returns successfully"
Apr 02 11:10:35 node-9.domain.tld sshd[11348]: Accepted publickey for root from 127.0.0.1 port 44864 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: New session 5 of user root.
-- Subject: A new session 5 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 5 has been created for the user root.
-- 
-- The leading process of the session is 11348.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started Session 5 of user root.
-- Subject: Unit session-5.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-5.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Startup finished in 106ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 106176 microseconds.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld sshd[11348]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:35 node-9.domain.tld sshd[11360]: Received disconnect from 127.0.0.1 port 44864:11: disconnected by user
Apr 02 11:10:35 node-9.domain.tld sshd[11360]: Disconnected from user root 127.0.0.1 port 44864
Apr 02 11:10:35 node-9.domain.tld sshd[11348]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: Session 5 logged out. Waiting for processes to exit.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: session-5.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-5.scope has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: Removed session 5.
-- Subject: Session 5 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 5 has been terminated.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11351]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:35 node-9.domain.tld sshd[11381]: Accepted publickey for root from 127.0.0.1 port 44866 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:35 node-9.domain.tld systemd-logind[2408]: New session 7 of user root.
-- Subject: A new session 7 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 7 has been created for the user root.
-- 
-- The leading process of the session is 11381.
Apr 02 11:10:35 node-9.domain.tld systemd[1]: Started Session 7 of user root.
-- Subject: Unit session-7.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-7.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:35 node-9.domain.tld systemd[11398]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Startup finished in 112ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 112071 microseconds.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld sshd[11381]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:36 node-9.domain.tld sshd[11409]: Received disconnect from 127.0.0.1 port 44866:11: disconnected by user
Apr 02 11:10:36 node-9.domain.tld sshd[11409]: Disconnected from user root 127.0.0.1 port 44866
Apr 02 11:10:36 node-9.domain.tld sshd[11381]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:36 node-9.domain.tld systemd-logind[2408]: Session 7 logged out. Waiting for processes to exit.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: session-7.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-7.scope has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd-logind[2408]: Removed session 7.
-- Subject: Session 7 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 7 has been terminated.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11398]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld sshd[11435]: Accepted publickey for root from 127.0.0.1 port 44952 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:36 node-9.domain.tld kernel: tun: Universal TUN/TAP device driver, 1.6
Apr 02 11:10:36 node-9.domain.tld systemd-logind[2408]: New session 9 of user root.
-- Subject: A new session 9 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 9 has been created for the user root.
-- 
-- The leading process of the session is 11435.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started Session 9 of user root.
-- Subject: Unit session-9.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-9.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Startup finished in 109ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 109418 microseconds.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld sshd[11435]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:36 node-9.domain.tld sshd[11510]: Received disconnect from 127.0.0.1 port 44952:11: disconnected by user
Apr 02 11:10:36 node-9.domain.tld sshd[11510]: Disconnected from user root 127.0.0.1 port 44952
Apr 02 11:10:36 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:10:36.656 4602 INFO coaster_agent.netlink [-] Interface event 16 for ens27f0np0.
Apr 02 11:10:36 node-9.domain.tld sshd[11435]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:36 node-9.domain.tld systemd-logind[2408]: Session 9 logged out. Waiting for processes to exit.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: session-9.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-9.scope has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd-logind[2408]: Removed session 9.
-- Subject: Session 9 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 9 has been terminated.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:36 node-9.domain.tld systemd[11480]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[11483]: pam_unix(systemd-user:session): session closed for user root
Apr 02 11:10:36 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:36 node-9.domain.tld sshd[11550]: Accepted publickey for root from 127.0.0.1 port 44956 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.820846449+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.821227195+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,} failed, error" error="failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.822843    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.822990    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.823046    4781 kuberuntime_manager.go:755] createPodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.823172    4781 pod_workers.go:191] Error syncing pod cd15a183-f8fb-4b52-a26d-5fe952a47e48 ("nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)"), skipping: failed to "CreatePodSandbox" for "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" with CreatePodSandboxError: "CreatePodSandbox for pod \"nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)\" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.823144560+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\""
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.823328692+08:00" level=info msg="Container to stop \"c2fdfb69a7778a307702ed14eb829ee2ae564440efc1234a51853228ad673d7a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:10:36 node-9.domain.tld systemd-logind[2408]: New session 11 of user root.
-- Subject: A new session 11 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 11 has been created for the user root.
-- 
-- The leading process of the session is 11550.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started Session 11 of user root.
-- Subject: Unit session-11.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-11.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:36 node-9.domain.tld kernel: nvme nvme6: mapped 8/0/0 default/read/poll queues.
Apr 02 11:10:36 node-9.domain.tld kernel: nvme nvme6: new ctrl: NQN "vam_3", addr 192.168.0.43:4430
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.882209407+08:00" level=info msg="TearDown network for sandbox \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" successfully"
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.882310452+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" returns successfully"
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.883297871+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,}"
Apr 02 11:10:36 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:36.883574361+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,} failed, error" error="failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.883780    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.883879    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.883913    4781 kuberuntime_manager.go:755] createPodSandbox for pod "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:36 node-9.domain.tld kubelet[4781]: E0402 11:10:36.884004    4781 pod_workers.go:191] Error syncing pod a5c9f638-c13f-4618-9c2a-9d56ce52f483 ("gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)"), skipping: failed to "CreatePodSandbox" for "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" with CreatePodSandboxError: "CreatePodSandbox for pod \"gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)\" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld systemd[11584]: Startup finished in 93ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 93497 microseconds.
Apr 02 11:10:36 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:36 node-9.domain.tld sshd[11550]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:36 node-9.domain.tld systemd-udevd[11599]: Process 'esstorage-udev-disk-hotplug add /dev/nvme6n1 ' failed with exit code 1.
Apr 02 11:10:37 node-9.domain.tld sshd[11654]: Received disconnect from 127.0.0.1 port 44956:11: disconnected by user
Apr 02 11:10:37 node-9.domain.tld sshd[11654]: Disconnected from user root 127.0.0.1 port 44956
Apr 02 11:10:37 node-9.domain.tld sshd[11550]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: Session 11 logged out. Waiting for processes to exit.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: session-11.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-11.scope has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: Removed session 11.
-- Subject: Session 11 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 11 has been terminated.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11584]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld sshd[11689]: Accepted publickey for root from 127.0.0.1 port 44968 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: New session 13 of user root.
-- Subject: A new session 13 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 13 has been created for the user root.
-- 
-- The leading process of the session is 11689.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Started Session 13 of user root.
-- Subject: Unit session-13.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-13.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Startup finished in 111ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 111282 microseconds.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld sshd[11689]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:37 node-9.domain.tld sshd[11712]: Received disconnect from 127.0.0.1 port 44968:11: disconnected by user
Apr 02 11:10:37 node-9.domain.tld sshd[11712]: Disconnected from user root 127.0.0.1 port 44968
Apr 02 11:10:37 node-9.domain.tld sshd[11689]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: Session 13 logged out. Waiting for processes to exit.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: session-13.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-13.scope has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: Removed session 13.
-- Subject: Session 13 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 13 has been terminated.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11703]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld sshd[11734]: Accepted publickey for root from 127.0.0.1 port 44972 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: New session 15 of user root.
-- Subject: A new session 15 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 15 has been created for the user root.
-- 
-- The leading process of the session is 11734.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Started Session 15 of user root.
-- Subject: Unit session-15.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-15.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Startup finished in 103ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 103834 microseconds.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld sshd[11734]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:37 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: Changing Tx descriptor count from 512 to 4096.
Apr 02 11:10:37 node-9.domain.tld kernel: i40e 0000:98:00.1 ens2f1: Changing Rx descriptor count from 512 to 4096
Apr 02 11:10:37 node-9.domain.tld sshd[11756]: Received disconnect from 127.0.0.1 port 44972:11: disconnected by user
Apr 02 11:10:37 node-9.domain.tld sshd[11756]: Disconnected from user root 127.0.0.1 port 44972
Apr 02 11:10:37 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:10:37.892 4602 INFO coaster_agent.netlink [-] Interface event 16 for ens2f1.
Apr 02 11:10:37 node-9.domain.tld sshd[11734]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: Session 15 logged out. Waiting for processes to exit.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: session-15.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-15.scope has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd-logind[2408]: Removed session 15.
-- Subject: Session 15 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 15 has been terminated.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:37 node-9.domain.tld systemd[11745]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:37 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:38 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:38 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:38 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:38 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:38 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:38.144571227+08:00" level=info msg="shim disconnected" id=b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c
Apr 02 11:10:38 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:38.144692073+08:00" level=warning msg="cleaning up after shim disconnected" id=b019e6ac14bbfe04bd713e4937f60932b47a024a0823ca7e2da4978e6df8e34c namespace=k8s.io
Apr 02 11:10:38 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:38.144737305+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:38 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:38.169954008+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:38+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=11845\n"
Apr 02 11:10:38 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:38.815979711+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:qgpu-manager-slc77,Uid:d56ec515-f683-49ae-af58-15c5ec1b769b,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.108100300+08:00" level=info msg="CreateContainer within sandbox \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\" for container &ContainerMetadata{Name:ovn-controller,Attempt:0,}"
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.181649359+08:00" level=info msg="CreateContainer within sandbox \"773e03ead26ecd02f61a638f8ae67cbcb21536ec5e96d94d206bd91ed0a802ee\" for &ContainerMetadata{Name:ovn-controller,Attempt:0,} returns container id \"d54e7ba0e5a1853a1208b824475e6f81beac2dbe443ec2993dbe75c9c32d184f\""
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.182129424+08:00" level=info msg="StartContainer for \"d54e7ba0e5a1853a1208b824475e6f81beac2dbe443ec2993dbe75c9c32d184f\""
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.364213579+08:00" level=info msg="StartContainer for \"d54e7ba0e5a1853a1208b824475e6f81beac2dbe443ec2993dbe75c9c32d184f\" returns successfully"
Apr 02 11:10:39 node-9.domain.tld systemd-udevd[11964]: Using default interface naming scheme 'rhel-8.0'.
Apr 02 11:10:39 node-9.domain.tld systemd-udevd[11964]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:39 node-9.domain.tld systemd-udevd[11964]: Could not generate persistent MAC address for cni0: No such file or directory
Apr 02 11:10:39 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:10:39 node-9.domain.tld kubelet[4781]: I0402 11:10:39.817115    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af
Apr 02 11:10:39 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:10:39 node-9.domain.tld kernel: cni0: port 1(vethb4520201) entered blocking state
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.822172147+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for container &ContainerMetadata{Name:qgpu-exporter,Attempt:2,}"
Apr 02 11:10:39 node-9.domain.tld kernel: cni0: port 1(vethb4520201) entered disabled state
Apr 02 11:10:39 node-9.domain.tld kernel: device vethb4520201 entered promiscuous mode
Apr 02 11:10:39 node-9.domain.tld kernel: cni0: port 1(vethb4520201) entered blocking state
Apr 02 11:10:39 node-9.domain.tld kernel: cni0: port 1(vethb4520201) entered forwarding state
Apr 02 11:10:39 node-9.domain.tld systemd-udevd[11996]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:39 node-9.domain.tld systemd-udevd[11996]: Could not generate persistent MAC address for vethb4520201: No such file or directory
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.947358295+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for &ContainerMetadata{Name:qgpu-exporter,Attempt:2,} returns container id \"bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f\""
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.948004041+08:00" level=info msg="StartContainer for \"bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f\""
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.969889024+08:00" level=error msg="Failed to destroy network for sandbox \"4f79de8429885574d5a9f2ca1b41d976aa4b17b0be9a7f6bd7763f3a4890c481\"" error="delegateDel: error invoking ConflistDel - \"kube-ovn\": conflistDel: error in getting result from DelNetworkList: Post \"http://dummy/api/v1/del\": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory"
Apr 02 11:10:39 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:39.990988011+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,} failed, error" error="failed to setup network for sandbox \"4f79de8429885574d5a9f2ca1b41d976aa4b17b0be9a7f6bd7763f3a4890c481\": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network \"kube-ovn\": delegateAdd: error invoking conflistAdd - \"kube-ovn\": conflistAdd: error in getting result from AddNetworkList: Post \"http://dummy/api/v1/add\": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory"
Apr 02 11:10:39 node-9.domain.tld kubelet[4781]: E0402 11:10:39.991356    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to setup network for sandbox "4f79de8429885574d5a9f2ca1b41d976aa4b17b0be9a7f6bd7763f3a4890c481": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network "kube-ovn": delegateAdd: error invoking conflistAdd - "kube-ovn": conflistAdd: error in getting result from AddNetworkList: Post "http://dummy/api/v1/add": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory
Apr 02 11:10:39 node-9.domain.tld kubelet[4781]: E0402 11:10:39.991462    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "4f79de8429885574d5a9f2ca1b41d976aa4b17b0be9a7f6bd7763f3a4890c481": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network "kube-ovn": delegateAdd: error invoking conflistAdd - "kube-ovn": conflistAdd: error in getting result from AddNetworkList: Post "http://dummy/api/v1/add": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory
Apr 02 11:10:39 node-9.domain.tld kubelet[4781]: E0402 11:10:39.991499    4781 kuberuntime_manager.go:755] createPodSandbox for pod "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" failed: rpc error: code = Unknown desc = failed to setup network for sandbox "4f79de8429885574d5a9f2ca1b41d976aa4b17b0be9a7f6bd7763f3a4890c481": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network "kube-ovn": delegateAdd: error invoking conflistAdd - "kube-ovn": conflistAdd: error in getting result from AddNetworkList: Post "http://dummy/api/v1/add": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory
Apr 02 11:10:39 node-9.domain.tld kubelet[4781]: E0402 11:10:39.991605    4781 pod_workers.go:191] Error syncing pod e74039f9-9bee-4e6a-ade7-535f5d7a309e ("alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)"), skipping: failed to "CreatePodSandbox" for "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" with CreatePodSandboxError: "CreatePodSandbox for pod \"alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)\" failed: rpc error: code = Unknown desc = failed to setup network for sandbox \"4f79de8429885574d5a9f2ca1b41d976aa4b17b0be9a7f6bd7763f3a4890c481\": Multus: [aipaas-public-alluxio/alluxio-fuse-hjvv5]: error adding container to network \"kube-ovn\": delegateAdd: error invoking conflistAdd - \"kube-ovn\": conflistAdd: error in getting result from AddNetworkList: Post \"http://dummy/api/v1/add\": dial unix /run/openvswitch/kube-ovn-daemon.sock: connect: no such file or directory"
Apr 02 11:10:40 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:40.149113760+08:00" level=info msg="StartContainer for \"bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f\" returns successfully"
Apr 02 11:10:40 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:40.376018366+08:00" level=info msg="shim disconnected" id=bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:10:40 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:40.376126917+08:00" level=warning msg="cleaning up after shim disconnected" id=bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f namespace=k8s.io
Apr 02 11:10:40 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:40.376153417+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:40 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:40.401268677+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:40+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=12149\n"
Apr 02 11:10:40 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:40 node-9.domain.tld systemd[1]: run-netns-cni\x2dd1d05ec4\x2d7584\x2d0926\x2d57ea\x2d7d1a084d5120.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2dd1d05ec4\x2d7584\x2d0926\x2d57ea\x2d7d1a084d5120.mount has successfully entered the 'dead' state.
Apr 02 11:10:40 node-9.domain.tld kernel: SPDMD-DEVICE:[INFO]spdmd_device_disk_add:1787 Add disk /dev/nvme6n1 to k8stor_04
Apr 02 11:10:40 node-9.domain.tld kernel: nvme nvme7: creating 8 I/O queues.
Apr 02 11:10:41 node-9.domain.tld kubelet[4781]: I0402 11:10:41.098594    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af
Apr 02 11:10:41 node-9.domain.tld kubelet[4781]: I0402 11:10:41.099356    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:10:41 node-9.domain.tld kubelet[4781]: E0402 11:10:41.100136    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:10:41 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:41.100451476+08:00" level=info msg="RemoveContainer for \"92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af\""
Apr 02 11:10:41 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:41.109107946+08:00" level=info msg="RemoveContainer for \"92c57a64c2b1076101c5f37e03b8134dceaf9bad603f9f3e3298cb4d7bec45af\" returns successfully"
Apr 02 11:10:41 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:41.986070665+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/162ce4f42a01f76bdcf95b46a7b236fb24a99b6b6fc782ade0e0cfcd1316eae1 pid=12339
Apr 02 11:10:42 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:42.147677888+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:virt-handler-9ksxc,Uid:8c139073-51af-4078-b62c-d0b409dfacd4,Namespace:kubevirt,Attempt:0,} returns sandbox id \"162ce4f42a01f76bdcf95b46a7b236fb24a99b6b6fc782ade0e0cfcd1316eae1\""
Apr 02 11:10:42 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:42.152049575+08:00" level=info msg="CreateContainer within sandbox \"162ce4f42a01f76bdcf95b46a7b236fb24a99b6b6fc782ade0e0cfcd1316eae1\" for container &ContainerMetadata{Name:virt-launcher,Attempt:0,}"
Apr 02 11:10:42 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:42.218433948+08:00" level=info msg="CreateContainer within sandbox \"162ce4f42a01f76bdcf95b46a7b236fb24a99b6b6fc782ade0e0cfcd1316eae1\" for &ContainerMetadata{Name:virt-launcher,Attempt:0,} returns container id \"38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000\""
Apr 02 11:10:42 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:42.218993690+08:00" level=info msg="StartContainer for \"38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000\""
Apr 02 11:10:42 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:42.381243516+08:00" level=info msg="StartContainer for \"38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000\" returns successfully"
Apr 02 11:10:42 node-9.domain.tld kernel: nvme nvme7: mapped 8/0/0 default/read/poll queues.
Apr 02 11:10:42 node-9.domain.tld kernel: nvme nvme7: new ctrl: NQN "vam_4", addr 192.168.0.44:4430
Apr 02 11:10:42 node-9.domain.tld systemd-udevd[12406]: Process 'esstorage-udev-disk-hotplug add /dev/nvme7n1 ' failed with exit code 1.
Apr 02 11:10:43 node-9.domain.tld kvm[12486]: 1 guest now active
Apr 02 11:10:43 node-9.domain.tld kvm[12494]: 0 guests now active
Apr 02 11:10:43 node-9.domain.tld sshd[12490]: Accepted publickey for root from 192.168.11.11 port 45888 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:43 node-9.domain.tld systemd-logind[2408]: New session 17 of user root.
-- Subject: A new session 17 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 17 has been created for the user root.
-- 
-- The leading process of the session is 12490.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Started Session 17 of user root.
-- Subject: Unit session-17.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-17.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:43 node-9.domain.tld kvm[12513]: 1 guest now active
Apr 02 11:10:43 node-9.domain.tld kvm[12515]: 0 guests now active
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Startup finished in 113ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 113321 microseconds.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld sshd[12490]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:43 node-9.domain.tld sshd[12518]: Received disconnect from 192.168.11.11 port 45888:11: disconnected by user
Apr 02 11:10:43 node-9.domain.tld sshd[12518]: Disconnected from user root 192.168.11.11 port 45888
Apr 02 11:10:43 node-9.domain.tld sshd[12490]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:43 node-9.domain.tld systemd-logind[2408]: Session 17 logged out. Waiting for processes to exit.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: session-17.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-17.scope has successfully entered the 'dead' state.
Apr 02 11:10:43 node-9.domain.tld systemd-logind[2408]: Removed session 17.
-- Subject: Session 17 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 17 has been terminated.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:43 node-9.domain.tld systemd[12502]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld kvm[12547]: 1 guest now active
Apr 02 11:10:43 node-9.domain.tld kvm[12549]: 0 guests now active
Apr 02 11:10:43 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:43 node-9.domain.tld sshd[12539]: Accepted publickey for root from 192.168.11.11 port 45892 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:43 node-9.domain.tld systemd-logind[2408]: New session 19 of user root.
-- Subject: A new session 19 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 19 has been created for the user root.
-- 
-- The leading process of the session is 12539.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Started Session 19 of user root.
-- Subject: Unit session-19.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-19.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:43.697968678+08:00" level=info msg="shim disconnected" id=38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000
Apr 02 11:10:43 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:43.698080047+08:00" level=warning msg="cleaning up after shim disconnected" id=38312f4f1e8b50f16b02f4eb881b67599fb36ad3657c14e6300848b167286000 namespace=k8s.io
Apr 02 11:10:43 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:43.698105643+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:43 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:43.722739536+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:43+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=12580\n"
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld systemd[12577]: Startup finished in 114ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 114694 microseconds.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:43 node-9.domain.tld sshd[12539]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Reloading.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:43 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:25: Unknown lvalue 'ProtectHostname' in section 'Service'
Apr 02 11:10:44 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:26: Unknown lvalue 'ProtectKernelLogs' in section 'Service'
Apr 02 11:10:44 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:44.119999651+08:00" level=info msg="CreateContainer within sandbox \"162ce4f42a01f76bdcf95b46a7b236fb24a99b6b6fc782ade0e0cfcd1316eae1\" for container &ContainerMetadata{Name:virt-handler,Attempt:0,}"
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Reloading.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:44 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:44.192191916+08:00" level=info msg="CreateContainer within sandbox \"162ce4f42a01f76bdcf95b46a7b236fb24a99b6b6fc782ade0e0cfcd1316eae1\" for &ContainerMetadata{Name:virt-handler,Attempt:0,} returns container id \"8fee09b6d80f97407aaed5665e3bd65aed275a3c15b6be1940cbf8f1f89568c6\""
Apr 02 11:10:44 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:44.192869647+08:00" level=info msg="StartContainer for \"8fee09b6d80f97407aaed5665e3bd65aed275a3c15b6be1940cbf8f1f89568c6\""
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:25: Unknown lvalue 'ProtectHostname' in section 'Service'
Apr 02 11:10:44 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:26: Unknown lvalue 'ProtectKernelLogs' in section 'Service'
Apr 02 11:10:44 node-9.domain.tld sshd[12615]: Received disconnect from 192.168.11.11 port 45892:11: disconnected by user
Apr 02 11:10:44 node-9.domain.tld sshd[12615]: Disconnected from user root 192.168.11.11 port 45892
Apr 02 11:10:44 node-9.domain.tld sshd[12539]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:44 node-9.domain.tld systemd[1]: session-19.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-19.scope has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd-logind[2408]: Session 19 logged out. Waiting for processes to exit.
Apr 02 11:10:44 node-9.domain.tld systemd-logind[2408]: Removed session 19.
-- Subject: Session 19 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 19 has been terminated.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:44 node-9.domain.tld systemd[12577]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:44.381824689+08:00" level=info msg="StartContainer for \"8fee09b6d80f97407aaed5665e3bd65aed275a3c15b6be1940cbf8f1f89568c6\" returns successfully"
Apr 02 11:10:44 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld sshd[12700]: Accepted publickey for root from 192.168.11.11 port 45900 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:44 node-9.domain.tld systemd-logind[2408]: New session 21 of user root.
-- Subject: A new session 21 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 21 has been created for the user root.
-- 
-- The leading process of the session is 12700.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Started Session 21 of user root.
-- Subject: Unit session-21.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-21.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:44 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:10:44 node-9.domain.tld kubelet[4781]: W0402 11:10:44.545034    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/memory/system.slice/system-user\\x2druntime\\x2ddir.slice/user-runtime-dir@0.service": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/system.slice/system-user\x2druntime\x2ddir.slice/user-runtime-dir@0.service: no such file or directory
Apr 02 11:10:44 node-9.domain.tld kubelet[4781]: W0402 11:10:44.545160    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/pids/system.slice/system-user\\x2druntime\\x2ddir.slice/user-runtime-dir@0.service": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/pids/system.slice/system-user\x2druntime\x2ddir.slice/user-runtime-dir@0.service: no such file or directory
Apr 02 11:10:44 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:10:44 node-9.domain.tld kernel: cni0: port 2(vethe8b3d405) entered blocking state
Apr 02 11:10:44 node-9.domain.tld kernel: cni0: port 2(vethe8b3d405) entered disabled state
Apr 02 11:10:44 node-9.domain.tld kernel: device vethe8b3d405 entered promiscuous mode
Apr 02 11:10:44 node-9.domain.tld kernel: cni0: port 2(vethe8b3d405) entered blocking state
Apr 02 11:10:44 node-9.domain.tld kernel: cni0: port 2(vethe8b3d405) entered forwarding state
Apr 02 11:10:44 node-9.domain.tld systemd-udevd[12746]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:44 node-9.domain.tld systemd-udevd[12746]: Could not generate persistent MAC address for vethe8b3d405: No such file or directory
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Startup finished in 103ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 103293 microseconds.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld sshd[12700]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:44 node-9.domain.tld sshd[12769]: Received disconnect from 192.168.11.11 port 45900:11: disconnected by user
Apr 02 11:10:44 node-9.domain.tld sshd[12769]: Disconnected from user root 192.168.11.11 port 45900
Apr 02 11:10:44 node-9.domain.tld sshd[12700]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:44 node-9.domain.tld systemd-logind[2408]: Session 21 logged out. Waiting for processes to exit.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: session-21.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-21.scope has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd-logind[2408]: Removed session 21.
-- Subject: Session 21 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 21 has been terminated.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12729]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:44.816263342+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb,Uid:3f04228b-0006-4b96-acca-351a4d101cfa,Namespace:kube-system,Attempt:0,}"
Apr 02 11:10:44 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:44 node-9.domain.tld sshd[12798]: Accepted publickey for root from 192.168.11.11 port 45904 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:44 node-9.domain.tld systemd-logind[2408]: New session 23 of user root.
-- Subject: A new session 23 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 23 has been created for the user root.
-- 
-- The leading process of the session is 12798.
Apr 02 11:10:44 node-9.domain.tld systemd[1]: Started Session 23 of user root.
-- Subject: Unit session-23.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-23.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12841]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:44 node-9.domain.tld systemd[12841]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:44 node-9.domain.tld systemd[12841]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:44 node-9.domain.tld systemd[12841]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Startup finished in 123ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 123688 microseconds.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld sshd[12798]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:45 node-9.domain.tld sshd[12863]: Received disconnect from 192.168.11.11 port 45904:11: disconnected by user
Apr 02 11:10:45 node-9.domain.tld sshd[12863]: Disconnected from user root 192.168.11.11 port 45904
Apr 02 11:10:45 node-9.domain.tld sshd[12798]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:45 node-9.domain.tld systemd-logind[2408]: Session 23 logged out. Waiting for processes to exit.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: session-23.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-23.scope has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd-logind[2408]: Removed session 23.
-- Subject: Session 23 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 23 has been terminated.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12841]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld sshd[12900]: Accepted publickey for root from 192.168.11.11 port 45906 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:45 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd-logind[2408]: New session 25 of user root.
-- Subject: A new session 25 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 25 has been created for the user root.
-- 
-- The leading process of the session is 12900.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started Session 25 of user root.
-- Subject: Unit session-25.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-25.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Startup finished in 112ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 112487 microseconds.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld sshd[12900]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:45 node-9.domain.tld sshd[12931]: Received disconnect from 192.168.11.11 port 45906:11: disconnected by user
Apr 02 11:10:45 node-9.domain.tld sshd[12931]: Disconnected from user root 192.168.11.11 port 45906
Apr 02 11:10:45 node-9.domain.tld sshd[12900]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:45 node-9.domain.tld systemd-logind[2408]: Session 25 logged out. Waiting for processes to exit.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: session-25.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-25.scope has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd-logind[2408]: Removed session 25.
-- Subject: Session 25 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 25 has been terminated.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[12920]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:45 node-9.domain.tld sshd[12950]: Accepted publickey for root from 192.168.11.11 port 45908 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:45 node-9.domain.tld systemd-logind[2408]: New session 27 of user root.
-- Subject: A new session 27 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 27 has been created for the user root.
-- 
-- The leading process of the session is 12950.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started Session 27 of user root.
-- Subject: Unit session-27.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-27.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:45 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:45.816712775+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:csi-node-scaleflashplugin-x4k72,Uid:805dbfe7-6703-408b-aa8f-ed778c183e6b,Namespace:scaleflash,Attempt:0,}"
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld systemd[12963]: Startup finished in 115ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 115594 microseconds.
Apr 02 11:10:45 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:45 node-9.domain.tld sshd[12950]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.058864519+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad pid=13023
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.282105144+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:qgpu-manager-slc77,Uid:d56ec515-f683-49ae-af58-15c5ec1b769b,Namespace:imp-qgpu,Attempt:0,} returns sandbox id \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\""
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.286646350+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-installer,Attempt:0,}"
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.350429782+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-installer,Attempt:0,} returns container id \"432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434\""
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.351083611+08:00" level=info msg="StartContainer for \"432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434\""
Apr 02 11:10:46 node-9.domain.tld sshd[12984]: Received disconnect from 192.168.11.11 port 45908:11: disconnected by user
Apr 02 11:10:46 node-9.domain.tld sshd[12984]: Disconnected from user root 192.168.11.11 port 45908
Apr 02 11:10:46 node-9.domain.tld sshd[12950]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:46 node-9.domain.tld systemd-logind[2408]: Session 27 logged out. Waiting for processes to exit.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: session-27.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-27.scope has successfully entered the 'dead' state.
Apr 02 11:10:46 node-9.domain.tld systemd-logind[2408]: Removed session 27.
-- Subject: Session 27 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 27 has been terminated.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[12963]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:46 node-9.domain.tld kernel: SPDMD-DEVICE:[INFO]spdmd_device_disk_add:1787 Add disk /dev/nvme7n1 to k8stor_04
Apr 02 11:10:46 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.599164854+08:00" level=info msg="StartContainer for \"432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434\" returns successfully"
Apr 02 11:10:46 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:46 node-9.domain.tld sshd[13084]: Accepted publickey for root from 192.168.11.11 port 45928 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:46 node-9.domain.tld systemd-logind[2408]: New session 29 of user root.
-- Subject: A new session 29 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 29 has been created for the user root.
-- 
-- The leading process of the session is 13084.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Started Session 29 of user root.
-- Subject: Unit session-29.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-29.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:46 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:46.816266410+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-container-toolkit-daemonset-qr9nk,Uid:505d6e19-c308-4800-85b7-d8a906c1ac02,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld systemd[13110]: Startup finished in 170ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 170895 microseconds.
Apr 02 11:10:46 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:46 node-9.domain.tld sshd[13084]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131155    4781 manager.go:408] Got registration request from device plugin with resource name "devices.kubevirt.io/tun"
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131156    4781 manager.go:408] Got registration request from device plugin with resource name "devices.kubevirt.io/kvm"
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131423    4781 endpoint.go:196] parsed scheme: ""
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131446    4781 endpoint.go:196] scheme "" not registered, fallback to default scheme
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131495    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/lib/kubelet/device-plugins/kubevirt-tun.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131515    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131547    4781 endpoint.go:196] parsed scheme: ""
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131583    4781 endpoint.go:196] scheme "" not registered, fallback to default scheme
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131636    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/lib/kubelet/device-plugins/kubevirt-kvm.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.131658    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.144635    4781 manager.go:408] Got registration request from device plugin with resource name "devices.kubevirt.io/vhost-net"
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.144870    4781 endpoint.go:196] parsed scheme: ""
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.144903    4781 endpoint.go:196] scheme "" not registered, fallback to default scheme
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.144945    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/lib/kubelet/device-plugins/kubevirt-vhost-net.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:47 node-9.domain.tld kubelet[4781]: I0402 11:10:47.144964    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:47 node-9.domain.tld kernel: CLST:[INFO]clst_ioctl_plan:1203 plan set on 04-02-11-10-47.541=>2:128 3:127 4:126 1:131 
Apr 02 11:10:47 node-9.domain.tld kernel: CLST:[ERROR]clst_make_request:349 no found disk in group:4, block...
Apr 02 11:10:47 node-9.domain.tld kernel: CLST:[INFO]clst_module_delaywq_bio_add:185 Start a new round of bio blocking...
Apr 02 11:10:47 node-9.domain.tld sshd[13157]: Received disconnect from 192.168.11.11 port 45928:11: disconnected by user
Apr 02 11:10:47 node-9.domain.tld sshd[13157]: Disconnected from user root 192.168.11.11 port 45928
Apr 02 11:10:47 node-9.domain.tld sshd[13084]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:47 node-9.domain.tld systemd-logind[2408]: Session 29 logged out. Waiting for processes to exit.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: session-29.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-29.scope has successfully entered the 'dead' state.
Apr 02 11:10:47 node-9.domain.tld systemd-logind[2408]: Removed session 29.
-- Subject: Session 29 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 29 has been terminated.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[13110]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:47 node-9.domain.tld sshd[13286]: Accepted publickey for root from 192.168.11.11 port 45942 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:47 node-9.domain.tld systemd-logind[2408]: New session 31 of user root.
-- Subject: A new session 31 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 31 has been created for the user root.
-- 
-- The leading process of the session is 13286.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Started Session 31 of user root.
-- Subject: Unit session-31.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-31.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld systemd[13305]: Startup finished in 113ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 113767 microseconds.
Apr 02 11:10:47 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:47 node-9.domain.tld sshd[13286]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:48 node-9.domain.tld sshd[13320]: Received disconnect from 192.168.11.11 port 45942:11: disconnected by user
Apr 02 11:10:48 node-9.domain.tld sshd[13320]: Disconnected from user root 192.168.11.11 port 45942
Apr 02 11:10:48 node-9.domain.tld sshd[13286]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:48 node-9.domain.tld systemd-logind[2408]: Session 31 logged out. Waiting for processes to exit.
Apr 02 11:10:48 node-9.domain.tld systemd[1]: session-31.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-31.scope has successfully entered the 'dead' state.
Apr 02 11:10:48 node-9.domain.tld systemd-logind[2408]: Removed session 31.
-- Subject: Session 31 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 31 has been terminated.
Apr 02 11:10:48 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:48 node-9.domain.tld systemd[13305]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:48 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:49 node-9.domain.tld sshd[13387]: Accepted publickey for root from 192.168.11.11 port 45946 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:49 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:49 node-9.domain.tld systemd-logind[2408]: New session 33 of user root.
-- Subject: A new session 33 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 33 has been created for the user root.
-- 
-- The leading process of the session is 13387.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Started Session 33 of user root.
-- Subject: Unit session-33.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-33.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld systemd[13399]: Startup finished in 113ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 113720 microseconds.
Apr 02 11:10:49 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:49 node-9.domain.tld sshd[13387]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.818030516+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\""
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.818227819+08:00" level=info msg="Container to stop \"c2fdfb69a7778a307702ed14eb829ee2ae564440efc1234a51853228ad673d7a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.819965065+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.820461359+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,} failed, error" error="failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.821793    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.822001    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.822064    4781 kuberuntime_manager.go:755] createPodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.822266    4781 pod_workers.go:191] Error syncing pod cd15a183-f8fb-4b52-a26d-5fe952a47e48 ("nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)"), skipping: failed to "CreatePodSandbox" for "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" with CreatePodSandboxError: "CreatePodSandbox for pod \"nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)\" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.878313802+08:00" level=info msg="TearDown network for sandbox \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" successfully"
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.878384854+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" returns successfully"
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.879258688+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,}"
Apr 02 11:10:49 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:49.879507302+08:00" level=error msg="RunPodSandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,} failed, error" error="failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.879698    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.879810    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.879843    4781 kuberuntime_manager.go:755] createPodSandbox for pod "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for "nvidia" is configured
Apr 02 11:10:49 node-9.domain.tld kubelet[4781]: E0402 11:10:49.879930    4781 pod_workers.go:191] Error syncing pod a5c9f638-c13f-4618-9c2a-9d56ce52f483 ("gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)"), skipping: failed to "CreatePodSandbox" for "gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)" with CreatePodSandboxError: "CreatePodSandbox for pod \"gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)\" failed: rpc error: code = Unknown desc = failed to get sandbox runtime: no runtime for \"nvidia\" is configured"
Apr 02 11:10:50 node-9.domain.tld sshd[13408]: Received disconnect from 192.168.11.11 port 45946:11: disconnected by user
Apr 02 11:10:50 node-9.domain.tld sshd[13408]: Disconnected from user root 192.168.11.11 port 45946
Apr 02 11:10:50 node-9.domain.tld sshd[13387]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:50 node-9.domain.tld systemd-logind[2408]: Session 33 logged out. Waiting for processes to exit.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: session-33.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-33.scope has successfully entered the 'dead' state.
Apr 02 11:10:50 node-9.domain.tld systemd-logind[2408]: Removed session 33.
-- Subject: Session 33 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 33 has been terminated.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13399]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:50 node-9.domain.tld sshd[13584]: Accepted publickey for root from 192.168.11.11 port 45962 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:50 node-9.domain.tld systemd-logind[2408]: New session 35 of user root.
-- Subject: A new session 35 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 35 has been created for the user root.
-- 
-- The leading process of the session is 13584.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Started Session 35 of user root.
-- Subject: Unit session-35.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-35.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld systemd[13609]: Startup finished in 119ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 119891 microseconds.
Apr 02 11:10:50 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:50 node-9.domain.tld sshd[13584]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:50 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:10:50 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:10:50 node-9.domain.tld kernel: cni0: port 3(veth4f8bc5ba) entered blocking state
Apr 02 11:10:50 node-9.domain.tld systemd-udevd[13263]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:50 node-9.domain.tld kernel: cni0: port 3(veth4f8bc5ba) entered disabled state
Apr 02 11:10:50 node-9.domain.tld kernel: device veth4f8bc5ba entered promiscuous mode
Apr 02 11:10:50 node-9.domain.tld kernel: cni0: port 3(veth4f8bc5ba) entered blocking state
Apr 02 11:10:50 node-9.domain.tld kernel: cni0: port 3(veth4f8bc5ba) entered forwarding state
Apr 02 11:10:50 node-9.domain.tld systemd-udevd[13263]: Could not generate persistent MAC address for veth4f8bc5ba: No such file or directory
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:10:50 node-9.domain.tld coaster-startup-checker[4553]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:10:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:10:51.064 4602 WARNING coaster_agent.iface [-] Get nic slot: 00:14.0-1.6 device info with exception: Command 'timeout 5s /usr/sbin/lspci|grep '00:14.0-1.6'' returned non-zero exit status 1
Apr 02 11:10:51 node-9.domain.tld sshd[13622]: Received disconnect from 192.168.11.11 port 45962:11: disconnected by user
Apr 02 11:10:51 node-9.domain.tld sshd[13622]: Disconnected from user root 192.168.11.11 port 45962
Apr 02 11:10:51 node-9.domain.tld sshd[13584]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:51 node-9.domain.tld systemd-logind[2408]: Session 35 logged out. Waiting for processes to exit.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: session-35.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-35.scope has successfully entered the 'dead' state.
Apr 02 11:10:51 node-9.domain.tld systemd-logind[2408]: Removed session 35.
-- Subject: Session 35 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 35 has been terminated.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[13609]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:51 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:51 node-9.domain.tld sshd[13956]: Accepted publickey for root from 192.168.11.11 port 45980 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:51 node-9.domain.tld systemd-udevd[13263]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:51 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:10:51 node-9.domain.tld kernel: cni0: port 4(vethd898d260) entered blocking state
Apr 02 11:10:51 node-9.domain.tld kernel: cni0: port 4(vethd898d260) entered disabled state
Apr 02 11:10:51 node-9.domain.tld kernel: device vethd898d260 entered promiscuous mode
Apr 02 11:10:51 node-9.domain.tld kernel: cni0: port 4(vethd898d260) entered blocking state
Apr 02 11:10:51 node-9.domain.tld kernel: cni0: port 4(vethd898d260) entered forwarding state
Apr 02 11:10:51 node-9.domain.tld systemd-udevd[13263]: Could not generate persistent MAC address for vethd898d260: No such file or directory
Apr 02 11:10:51 node-9.domain.tld systemd-logind[2408]: New session 37 of user root.
-- Subject: A new session 37 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 37 has been created for the user root.
-- 
-- The leading process of the session is 13956.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Started Session 37 of user root.
-- Subject: Unit session-37.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-37.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld systemd[14067]: Startup finished in 119ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 119658 microseconds.
Apr 02 11:10:51 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:51 node-9.domain.tld sshd[13956]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:51 node-9.domain.tld kubelet[4781]: I0402 11:10:51.816238    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:10:51 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:51.816796974+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,}"
Apr 02 11:10:51 node-9.domain.tld kubelet[4781]: E0402 11:10:51.816888    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]: Start to check OSD by-path after node start up.
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]: Node-9 is not ready in EOS. Wait 30s and retry.
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]: Node-9 is ready in EOS.
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]: Traceback (most recent call last):
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]:     sys.exit(main())
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:10:51 node-9.domain.tld coaster-startup-checker[4553]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:10:51 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:10:51 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:10:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:10:52.049 4602 WARNING coaster_agent.iface [-] Get nic slot: 00:14.0-1.6 device info with exception: Command 'timeout 5s /usr/sbin/lspci|grep '00:14.0-1.6'' returned non-zero exit status 1
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.223115878+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c pid=14309
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.453915892+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb,Uid:3f04228b-0006-4b96-acca-351a4d101cfa,Namespace:kube-system,Attempt:0,} returns sandbox id \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\""
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.458599721+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for container &ContainerMetadata{Name:worker,Attempt:0,}"
Apr 02 11:10:52 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.515172571+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for &ContainerMetadata{Name:worker,Attempt:0,} returns container id \"14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3\""
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.515788687+08:00" level=info msg="StartContainer for \"14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3\""
Apr 02 11:10:52 node-9.domain.tld systemd-udevd[13263]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:52 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:10:52 node-9.domain.tld kernel: cni0: port 5(vetha46d57ef) entered blocking state
Apr 02 11:10:52 node-9.domain.tld kernel: cni0: port 5(vetha46d57ef) entered disabled state
Apr 02 11:10:52 node-9.domain.tld kernel: device vetha46d57ef entered promiscuous mode
Apr 02 11:10:52 node-9.domain.tld kernel: cni0: port 5(vetha46d57ef) entered blocking state
Apr 02 11:10:52 node-9.domain.tld kernel: cni0: port 5(vetha46d57ef) entered forwarding state
Apr 02 11:10:52 node-9.domain.tld systemd-udevd[13263]: Could not generate persistent MAC address for vetha46d57ef: No such file or directory
Apr 02 11:10:52 node-9.domain.tld sshd[14124]: Received disconnect from 192.168.11.11 port 45980:11: disconnected by user
Apr 02 11:10:52 node-9.domain.tld sshd[14124]: Disconnected from user root 192.168.11.11 port 45980
Apr 02 11:10:52 node-9.domain.tld sshd[13956]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:52 node-9.domain.tld systemd-logind[2408]: Session 37 logged out. Waiting for processes to exit.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: session-37.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-37.scope has successfully entered the 'dead' state.
Apr 02 11:10:52 node-9.domain.tld systemd-logind[2408]: Removed session 37.
-- Subject: Session 37 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 37 has been terminated.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:52 node-9.domain.tld systemd[14067]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[14072]: pam_unix(systemd-user:session): session closed for user root
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.691385188+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/72329939b866269245cbfccc22b207759118cd4cd3e301b46729f0c425cfa4d6 pid=14441
Apr 02 11:10:52 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.750997842+08:00" level=info msg="StartContainer for \"14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3\" returns successfully"
Apr 02 11:10:52 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:52 node-9.domain.tld sshd[14421]: Accepted publickey for root from 192.168.11.11 port 45994 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:52 node-9.domain.tld systemd-logind[2408]: New session 39 of user root.
-- Subject: A new session 39 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 39 has been created for the user root.
-- 
-- The leading process of the session is 14421.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Started Session 39 of user root.
-- Subject: Unit session-39.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-39.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.881422303+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:csi-node-scaleflashplugin-x4k72,Uid:805dbfe7-6703-408b-aa8f-ed778c183e6b,Namespace:scaleflash,Attempt:0,} returns sandbox id \"72329939b866269245cbfccc22b207759118cd4cd3e301b46729f0c425cfa4d6\""
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.884838533+08:00" level=info msg="CreateContainer within sandbox \"72329939b866269245cbfccc22b207759118cd4cd3e301b46729f0c425cfa4d6\" for container &ContainerMetadata{Name:driver-registrar,Attempt:0,}"
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld systemd[14501]: Startup finished in 131ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 131202 microseconds.
Apr 02 11:10:52 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:52 node-9.domain.tld sshd[14421]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.954139269+08:00" level=info msg="CreateContainer within sandbox \"72329939b866269245cbfccc22b207759118cd4cd3e301b46729f0c425cfa4d6\" for &ContainerMetadata{Name:driver-registrar,Attempt:0,} returns container id \"d1365a5548df6fa30cead50b81321ca9bf5f5d2c8e7f8ae8b35377527b68f729\""
Apr 02 11:10:52 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:52.954839999+08:00" level=info msg="StartContainer for \"d1365a5548df6fa30cead50b81321ca9bf5f5d2c8e7f8ae8b35377527b68f729\""
Apr 02 11:10:53 node-9.domain.tld kernel: CLST:[ERROR]clst_module_delaywq:173 Plan has not changed:1(retry:1)
Apr 02 11:10:53 node-9.domain.tld systemd-udevd[13264]: Process 'esstorage-udev-disk-hotplug add /dev/test001 ' failed with exit code 1.
Apr 02 11:10:53 node-9.domain.tld kubelet[4781]: W0402 11:10:53.129129    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/memory/system.slice/system-user\\x2druntime\\x2ddir.slice/user-runtime-dir@0.service": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/system.slice/system-user\x2druntime\x2ddir.slice/user-runtime-dir@0.service: no such file or directory
Apr 02 11:10:53 node-9.domain.tld kubelet[4781]: W0402 11:10:53.129229    4781 watcher.go:95] Error while processing event ("/sys/fs/cgroup/pids/system.slice/system-user\\x2druntime\\x2ddir.slice/user-runtime-dir@0.service": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/pids/system.slice/system-user\x2druntime\x2ddir.slice/user-runtime-dir@0.service: no such file or directory
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.146072613+08:00" level=info msg="StartContainer for \"d1365a5548df6fa30cead50b81321ca9bf5f5d2c8e7f8ae8b35377527b68f729\" returns successfully"
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.147580159+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/scaleflash/scaleflash-csi-driver:v0-0-gf7dd319-2308161010\""
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.235597130+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/scaleflash/scaleflash-csi-driver:v0-0-gf7dd319-2308161010,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.238358932+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:08ceb49bfcb52a83517fe4ec9f32df7286c015abb4c0c6b967c4860d37069558,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.241827297+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/scaleflash/scaleflash-csi-driver:v0-0-gf7dd319-2308161010,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.244543821+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/scaleflash/scaleflash-csi-driver@sha256:3031209ff18750a12d2732c5d89974206752365829fb7bdf6f17dc505640d6f7,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.245614419+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/scaleflash/scaleflash-csi-driver:v0-0-gf7dd319-2308161010\" returns image reference \"sha256:08ceb49bfcb52a83517fe4ec9f32df7286c015abb4c0c6b967c4860d37069558\""
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.274087141+08:00" level=info msg="CreateContainer within sandbox \"72329939b866269245cbfccc22b207759118cd4cd3e301b46729f0c425cfa4d6\" for container &ContainerMetadata{Name:scaleflash-driver,Attempt:0,}"
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.335055805+08:00" level=info msg="CreateContainer within sandbox \"72329939b866269245cbfccc22b207759118cd4cd3e301b46729f0c425cfa4d6\" for &ContainerMetadata{Name:scaleflash-driver,Attempt:0,} returns container id \"4e5817e2b4836f484f5feb04605726b028b7e3e26b114b24d3f6989578220acc\""
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.335677246+08:00" level=info msg="StartContainer for \"4e5817e2b4836f484f5feb04605726b028b7e3e26b114b24d3f6989578220acc\""
Apr 02 11:10:53 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:53.503325136+08:00" level=info msg="StartContainer for \"4e5817e2b4836f484f5feb04605726b028b7e3e26b114b24d3f6989578220acc\" returns successfully"
Apr 02 11:10:53 node-9.domain.tld sshd[14540]: Received disconnect from 192.168.11.11 port 45994:11: disconnected by user
Apr 02 11:10:53 node-9.domain.tld sshd[14540]: Disconnected from user root 192.168.11.11 port 45994
Apr 02 11:10:53 node-9.domain.tld sshd[14421]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:53 node-9.domain.tld systemd-logind[2408]: Session 39 logged out. Waiting for processes to exit.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: session-39.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-39.scope has successfully entered the 'dead' state.
Apr 02 11:10:53 node-9.domain.tld systemd-logind[2408]: Removed session 39.
-- Subject: Session 39 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 39 has been terminated.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:53 node-9.domain.tld systemd[14501]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:53 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:54 node-9.domain.tld sshd[14765]: Accepted publickey for root from 192.168.11.11 port 46008 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:54 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:54 node-9.domain.tld systemd-logind[2408]: New session 41 of user root.
-- Subject: A new session 41 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 41 has been created for the user root.
-- 
-- The leading process of the session is 14765.
Apr 02 11:10:54 node-9.domain.tld systemd[1]: Started Session 41 of user root.
-- Subject: Unit session-41.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-41.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:54 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:54.085778356+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/f0ece7896e3d765b8b122da786f2d6c4f6e5c8be2fb11ea571b76d2e82ee71d7 pid=14822
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld systemd[14797]: Startup finished in 134ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 134446 microseconds.
Apr 02 11:10:54 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:54 node-9.domain.tld sshd[14765]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:54 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:54.286650790+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-container-toolkit-daemonset-qr9nk,Uid:505d6e19-c308-4800-85b7-d8a906c1ac02,Namespace:imp-qgpu,Attempt:0,} returns sandbox id \"f0ece7896e3d765b8b122da786f2d6c4f6e5c8be2fb11ea571b76d2e82ee71d7\""
Apr 02 11:10:54 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:54.294448648+08:00" level=info msg="CreateContainer within sandbox \"f0ece7896e3d765b8b122da786f2d6c4f6e5c8be2fb11ea571b76d2e82ee71d7\" for container &ContainerMetadata{Name:driver-validation,Attempt:0,}"
Apr 02 11:10:54 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:54.360034222+08:00" level=info msg="CreateContainer within sandbox \"f0ece7896e3d765b8b122da786f2d6c4f6e5c8be2fb11ea571b76d2e82ee71d7\" for &ContainerMetadata{Name:driver-validation,Attempt:0,} returns container id \"c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8\""
Apr 02 11:10:54 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:54.360652896+08:00" level=info msg="StartContainer for \"c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8\""
Apr 02 11:10:54 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:54.557551657+08:00" level=info msg="StartContainer for \"c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8\" returns successfully"
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.814428    4781 operation_generator.go:181] parsed scheme: ""
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.814477    4781 operation_generator.go:181] scheme "" not registered, fallback to default scheme
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.814522    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/lib/kubelet/plugins_registry/csi-scaleflash-reg.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.814546    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.816842    4781 csi_plugin.go:101] kubernetes.io/csi: Trying to validate a new CSI Driver with name: csi-scaleflash endpoint: /var/lib/kubelet/plugins/csi-scaleflash/csi.sock versions: 1.0.0
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.816937    4781 csi_plugin.go:114] kubernetes.io/csi: Register new plugin with name: csi-scaleflash at endpoint: /var/lib/kubelet/plugins/csi-scaleflash/csi.sock
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.817023    4781 clientconn.go:106] parsed scheme: ""
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.817047    4781 clientconn.go:106] scheme "" not registered, fallback to default scheme
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.817150    4781 passthrough.go:48] ccResolverWrapper: sending update to cc: {[{/var/lib/kubelet/plugins/csi-scaleflash/csi.sock  <nil> 0 <nil>}] <nil> <nil>}
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.817169    4781 clientconn.go:948] ClientConn switching balancer to "pick_first"
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: I0402 11:10:54.817243    4781 clientconn.go:897] blockingPicker: the picked transport is not ready, loop back to repick
Apr 02 11:10:54 node-9.domain.tld kubelet[4781]: E0402 11:10:54.872868    4781 nodeinfomanager.go:574] Invalid attach limit value 0 cannot be added to CSINode object for "csi-scaleflash"
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Stopping Kernel Samepage Merging (KSM) Tuning Daemon...
-- Subject: Unit ksmtuned.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksmtuned.service has begun shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: ksmtuned.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit ksmtuned.service has successfully entered the 'dead' state.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Stopped Kernel Samepage Merging (KSM) Tuning Daemon.
-- Subject: Unit ksmtuned.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksmtuned.service has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Stopping Kernel Samepage Merging...
-- Subject: Unit ksm.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksm.service has begun shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: ksm.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit ksm.service has successfully entered the 'dead' state.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Stopped Kernel Samepage Merging.
-- Subject: Unit ksm.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit ksm.service has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Reloading.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:25: Unknown lvalue 'ProtectHostname' in section 'Service'
Apr 02 11:10:55 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:26: Unknown lvalue 'ProtectKernelLogs' in section 'Service'
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Started /usr/bin/systemctl start man-db-cache-update.
-- Subject: Unit run-r573e411908e946edab0dfc51b47915e0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit run-r573e411908e946edab0dfc51b47915e0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: cgroup compatibility translation between legacy and unified hierarchy settings activated. See cgroup-compat debug messages for details.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Starting man-db-cache-update.service...
-- Subject: Unit man-db-cache-update.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit man-db-cache-update.service has begun starting up.
Apr 02 11:10:55 node-9.domain.tld sshd[14856]: Received disconnect from 192.168.11.11 port 46008:11: disconnected by user
Apr 02 11:10:55 node-9.domain.tld sshd[14856]: Disconnected from user root 192.168.11.11 port 46008
Apr 02 11:10:55 node-9.domain.tld sshd[14765]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:55 node-9.domain.tld systemd-logind[2408]: Session 41 logged out. Waiting for processes to exit.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: session-41.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-41.scope has successfully entered the 'dead' state.
Apr 02 11:10:55 node-9.domain.tld systemd-logind[2408]: Removed session 41.
-- Subject: Session 41 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 41 has been terminated.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:55 node-9.domain.tld systemd[14797]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:55 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld sshd[15305]: Accepted publickey for root from 192.168.11.11 port 46020 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd-logind[2408]: New session 43 of user root.
-- Subject: A new session 43 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 43 has been created for the user root.
-- 
-- The leading process of the session is 15305.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Started Session 43 of user root.
-- Subject: Unit session-43.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-43.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:56 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:56.485936970+08:00" level=info msg="shim disconnected" id=c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8
Apr 02 11:10:56 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:56.486062881+08:00" level=warning msg="cleaning up after shim disconnected" id=c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8 namespace=k8s.io
Apr 02 11:10:56 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:56.486092969+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: run-containerd-io.containerd.runtime.v2.task-k8s.io-c408a53de97626ef0925c81f397a2ac8acd71f20840aa4bb78b402be07155fd8-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:56.513080661+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:10:56+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=15795\n"
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Startup finished in 129ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 129359 microseconds.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld sshd[15305]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:56 node-9.domain.tld sshd[15843]: Received disconnect from 192.168.11.11 port 46020:11: disconnected by user
Apr 02 11:10:56 node-9.domain.tld sshd[15843]: Disconnected from user root 192.168.11.11 port 46020
Apr 02 11:10:56 node-9.domain.tld sshd[15305]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:56 node-9.domain.tld systemd-logind[2408]: Session 43 logged out. Waiting for processes to exit.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: session-43.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-43.scope has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld systemd-logind[2408]: Removed session 43.
-- Subject: Session 43 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 43 has been terminated.
Apr 02 11:10:56 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:56 node-9.domain.tld systemd[15684]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:57 node-9.domain.tld sshd[16312]: Accepted publickey for root from 192.168.11.11 port 46044 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:57 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:57 node-9.domain.tld systemd-logind[2408]: New session 45 of user root.
-- Subject: A new session 45 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 45 has been created for the user root.
-- 
-- The leading process of the session is 16312.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Started Session 45 of user root.
-- Subject: Unit session-45.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-45.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:57 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:57.183924710+08:00" level=info msg="CreateContainer within sandbox \"f0ece7896e3d765b8b122da786f2d6c4f6e5c8be2fb11ea571b76d2e82ee71d7\" for container &ContainerMetadata{Name:nvidia-container-toolkit-ctr,Attempt:0,}"
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:57.256257587+08:00" level=info msg="CreateContainer within sandbox \"f0ece7896e3d765b8b122da786f2d6c4f6e5c8be2fb11ea571b76d2e82ee71d7\" for &ContainerMetadata{Name:nvidia-container-toolkit-ctr,Attempt:0,} returns container id \"7c9e9c0b37298648bc60570edc8284b98d166c5ee2a013338a2bc45dded5fe37\""
Apr 02 11:10:57 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:57.256822460+08:00" level=info msg="StartContainer for \"7c9e9c0b37298648bc60570edc8284b98d166c5ee2a013338a2bc45dded5fe37\""
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld systemd[16499]: Startup finished in 135ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 135657 microseconds.
Apr 02 11:10:57 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:57 node-9.domain.tld sshd[16312]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:57 node-9.domain.tld containerd[4662]: time="2024-04-02T11:10:57.447395389+08:00" level=info msg="StartContainer for \"7c9e9c0b37298648bc60570edc8284b98d166c5ee2a013338a2bc45dded5fe37\" returns successfully"
Apr 02 11:10:57 node-9.domain.tld systemd-udevd[17135]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:57 node-9.domain.tld systemd-udevd[17135]: Could not generate persistent MAC address for 6b90400e4766_c: No such file or directory
Apr 02 11:10:57 node-9.domain.tld systemd-udevd[17136]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:10:57 node-9.domain.tld systemd-udevd[17136]: Could not generate persistent MAC address for 6b90400e4766_h: No such file or directory
Apr 02 11:10:57 node-9.domain.tld kernel: device 6b90400e4766_h entered promiscuous mode
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.657497    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unavailable desc = transport is closing
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.657596    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" failed: rpc error: code = Unavailable desc = transport is closing
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: W0402 11:10:57.657600    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.657629    4781 kuberuntime_manager.go:755] createPodSandbox for pod "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" failed: rpc error: code = Unavailable desc = transport is closing
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: W0402 11:10:57.657631    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: W0402 11:10:57.657703    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.657736    4781 pod_workers.go:191] Error syncing pod e74039f9-9bee-4e6a-ade7-535f5d7a309e ("alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)"), skipping: failed to "CreatePodSandbox" for "alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)" with CreatePodSandboxError: "CreatePodSandbox for pod \"alluxio-fuse-hjvv5_aipaas-public-alluxio(e74039f9-9bee-4e6a-ade7-535f5d7a309e)\" failed: rpc error: code = Unavailable desc = transport is closing"
Apr 02 11:10:57 node-9.domain.tld systemd[1]: containerd.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit containerd.service has successfully entered the 'dead' state.
Apr 02 11:10:57 node-9.domain.tld kernel: eth0: renamed from 6b90400e4766_c
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: W0402 11:10:57.758080    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:57 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:10:57 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.822797    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.822936    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.822959    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: E0402 11:10:57.823036    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:57 node-9.domain.tld kubelet[4781]: W0402 11:10:57.896008    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: W0402 11:10:58.118515    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: E0402 11:10:58.180007    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: E0402 11:10:58.180099    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: E0402 11:10:58.180132    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:58 node-9.domain.tld sshd[16733]: Received disconnect from 192.168.11.11 port 46044:11: disconnected by user
Apr 02 11:10:58 node-9.domain.tld sshd[16733]: Disconnected from user root 192.168.11.11 port 46044
Apr 02 11:10:58 node-9.domain.tld sshd[16312]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:58 node-9.domain.tld systemd-logind[2408]: Session 45 logged out. Waiting for processes to exit.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: session-45.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-45.scope has successfully entered the 'dead' state.
Apr 02 11:10:58 node-9.domain.tld systemd-logind[2408]: Removed session 45.
-- Subject: Session 45 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 45 has been terminated.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[16499]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld sshd[18050]: Accepted publickey for root from 192.168.11.11 port 46056 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:58 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:58 node-9.domain.tld systemd-logind[2408]: New session 47 of user root.
-- Subject: A new session 47 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 47 has been created for the user root.
-- 
-- The leading process of the session is 18050.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Started Session 47 of user root.
-- Subject: Unit session-47.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-47.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: W0402 11:10:58.478598    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld systemd[18266]: Startup finished in 124ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 124560 microseconds.
Apr 02 11:10:58 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:58 node-9.domain.tld sshd[18050]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: W0402 11:10:58.657915    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: W0402 11:10:58.657948    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: E0402 11:10:58.719148    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: E0402 11:10:58.719258    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:58 node-9.domain.tld kubelet[4781]: E0402 11:10:58.719306    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld sshd[18458]: Received disconnect from 192.168.11.11 port 46056:11: disconnected by user
Apr 02 11:10:59 node-9.domain.tld sshd[18458]: Disconnected from user root 192.168.11.11 port 46056
Apr 02 11:10:59 node-9.domain.tld sshd[18050]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:59 node-9.domain.tld systemd-logind[2408]: Session 47 logged out. Waiting for processes to exit.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: session-47.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-47.scope has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd-logind[2408]: Removed session 47.
-- Subject: Session 47 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 47 has been terminated.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[18266]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: W0402 11:10:59.129578    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:10:59 node-9.domain.tld sshd[19195]: Accepted publickey for root from 192.168.11.11 port 46064 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:59 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:59 node-9.domain.tld systemd-logind[2408]: New session 49 of user root.
-- Subject: A new session 49 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 49 has been created for the user root.
-- 
-- The leading process of the session is 19195.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Started Session 49 of user root.
-- Subject: Unit session-49.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-49.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.181144    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.181233    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.181267    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Startup finished in 123ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 123243 microseconds.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld sshd[19195]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:10:59 node-9.domain.tld sshd[19604]: Received disconnect from 192.168.11.11 port 46064:11: disconnected by user
Apr 02 11:10:59 node-9.domain.tld sshd[19604]: Disconnected from user root 192.168.11.11 port 46064
Apr 02 11:10:59 node-9.domain.tld sshd[19195]: pam_unix(sshd:session): session closed for user root
Apr 02 11:10:59 node-9.domain.tld systemd-logind[2408]: Session 49 logged out. Waiting for processes to exit.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: session-49.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-49.scope has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd-logind[2408]: Removed session 49.
-- Subject: Session 49 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 49 has been terminated.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[19408]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.823263    4781 remote_runtime.go:510] Status from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.823271    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.823349    4781 kubelet.go:2173] Container runtime sanity check failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.823363    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.823387    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.823458    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.868097    4781 remote_runtime.go:312] ListContainers with filter &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld kubelet[4781]: E0402 11:10:59.868202    4781 container_log_manager.go:183] Failed to rotate container logs: failed to list containers: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:10:59 node-9.domain.tld sshd[20194]: Accepted publickey for root from 192.168.11.11 port 46068 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:10:59 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:10:59 node-9.domain.tld systemd-logind[2408]: New session 51 of user root.
-- Subject: A new session 51 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 51 has been created for the user root.
-- 
-- The leading process of the session is 20194.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Started Session 51 of user root.
-- Subject: Unit session-51.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-51.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:10:59 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:10:59 node-9.domain.tld systemd[20373]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Startup finished in 130ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 130759 microseconds.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld sshd[20194]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: W0402 11:11:00.096561    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.181437    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.181526    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.181573    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: W0402 11:11:00.205218    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:00 node-9.domain.tld sshd[20543]: Received disconnect from 192.168.11.11 port 46068:11: disconnected by user
Apr 02 11:11:00 node-9.domain.tld sshd[20543]: Disconnected from user root 192.168.11.11 port 46068
Apr 02 11:11:00 node-9.domain.tld sshd[20194]: pam_unix(sshd:session): session closed for user root
Apr 02 11:11:00 node-9.domain.tld systemd-logind[2408]: Session 51 logged out. Waiting for processes to exit.
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: W0402 11:11:00.511455    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:00 node-9.domain.tld systemd[1]: session-51.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-51.scope has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd-logind[2408]: Removed session 51.
-- Subject: Session 51 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 51 has been terminated.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:00 node-9.domain.tld systemd[20373]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld sshd[20748]: Accepted publickey for root from 192.168.11.11 port 46074 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:11:00 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd-logind[2408]: New session 53 of user root.
-- Subject: A new session 53 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 53 has been created for the user root.
-- 
-- The leading process of the session is 20748.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Started Session 53 of user root.
-- Subject: Unit session-53.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-53.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Startup finished in 126ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 126685 microseconds.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld sshd[20748]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.815424    4781 remote_runtime.go:143] StopPodSandbox "caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.815526    4781 kuberuntime_manager.go:923] Failed to stop sandbox {"containerd" "caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc"}
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.815618    4781 kuberuntime_manager.go:702] killPodWithSyncResult failed: failed to "KillPodSandbox" for "a5c9f638-c13f-4618-9c2a-9d56ce52f483" with KillPodSandboxError: "rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\""
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.815658    4781 pod_workers.go:191] Error syncing pod a5c9f638-c13f-4618-9c2a-9d56ce52f483 ("gpu-feature-discovery-9bc7l_imp-qgpu(a5c9f638-c13f-4618-9c2a-9d56ce52f483)"), skipping: failed to "KillPodSandbox" for "a5c9f638-c13f-4618-9c2a-9d56ce52f483" with KillPodSandboxError: "rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\""
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.836157    4781 remote_image.go:151] ImageFsInfo from image service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:00 node-9.domain.tld kubelet[4781]: E0402 11:11:00.836211    4781 eviction_manager.go:260] eviction manager: failed to get summary stats: failed to get imageFs stats: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:00 node-9.domain.tld sshd[21017]: Received disconnect from 192.168.11.11 port 46074:11: disconnected by user
Apr 02 11:11:00 node-9.domain.tld sshd[21017]: Disconnected from user root 192.168.11.11 port 46074
Apr 02 11:11:00 node-9.domain.tld sshd[20748]: pam_unix(sshd:session): session closed for user root
Apr 02 11:11:00 node-9.domain.tld systemd-logind[2408]: Session 53 logged out. Waiting for processes to exit.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: session-53.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-53.scope has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd-logind[2408]: Removed session 53.
-- Subject: Session 53 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 53 has been terminated.
Apr 02 11:11:00 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:00 node-9.domain.tld systemd[20841]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:11:01 node-9.domain.tld sshd[21228]: Accepted publickey for root from 192.168.11.11 port 46076 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:11:01 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:11:01 node-9.domain.tld systemd-logind[2408]: New session 55 of user root.
-- Subject: A new session 55 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 55 has been created for the user root.
-- 
-- The leading process of the session is 21228.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Started Session 55 of user root.
-- Subject: Unit session-55.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-55.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.181796    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.181888    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.181924    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld systemd[21390]: Startup finished in 119ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 119178 microseconds.
Apr 02 11:11:01 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:01 node-9.domain.tld sshd[21228]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.366447    4781 remote_runtime.go:86] Version from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.719237    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.719357    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.719409    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.822874    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.822950    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.822971    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:01 node-9.domain.tld kubelet[4781]: E0402 11:11:01.823043    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:02 node-9.domain.tld kubelet[4781]: W0402 11:11:02.018222    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:02 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:11:02 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 1.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:11:02 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:11:02 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:02 node-9.domain.tld kubelet[4781]: E0402 11:11:02.182145    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:02 node-9.domain.tld kubelet[4781]: E0402 11:11:02.182217    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:02 node-9.domain.tld kubelet[4781]: E0402 11:11:02.182251    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: W0402 11:11:03.043694    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.182554    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.182645    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.182679    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: W0402 11:11:03.385047    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:03 node-9.domain.tld coaster-startup-checker[22457]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: I0402 11:11:03.815826    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816023    4781 remote_runtime.go:116] RunPodSandbox from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816106    4781 kuberuntime_sandbox.go:70] CreatePodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816141    4781 kuberuntime_manager.go:755] createPodSandbox for pod "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816231    4781 pod_workers.go:191] Error syncing pod cd15a183-f8fb-4b52-a26d-5fe952a47e48 ("nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)"), skipping: failed to "CreatePodSandbox" for "nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)" with CreatePodSandboxError: "CreatePodSandbox for pod \"nvidia-operator-validator-zlljs_imp-qgpu(cd15a183-f8fb-4b52-a26d-5fe952a47e48)\" failed: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\""
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816478    4781 remote_image.go:87] ImageStatus "dockerhub.citicsinfo.com/imp-qgpu/qgpu-exporter:v1.0.14" from image service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816549    4781 kuberuntime_image.go:86] ImageStatus for image {"dockerhub.citicsinfo.com/imp-qgpu/qgpu-exporter:v1.0.14" [{"kubernetes.io/config.source" "api"} {"kubernetes.io/config.seen" "2024-04-02T11:10:21.370126946+08:00"}]} failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816743    4781 kuberuntime_manager.go:829] container &Container{Name:qgpu-exporter,Image:dockerhub.citicsinfo.com/imp-qgpu/qgpu-exporter:v1.0.14,Command:[elastic-gpu-exporter --node=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:PORT,Value:5678,ValueFrom:nil,},EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cgroup,ReadOnly:true,MountPath:/host/sys,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:qgpu-exporter-token-mm8lc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230): ImageInspectError: Failed to inspect image "dockerhub.citicsinfo.com/imp-qgpu/qgpu-exporter:v1.0.14": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.816799    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with ImageInspectError: "Failed to inspect image \"dockerhub.citicsinfo.com/imp-qgpu/qgpu-exporter:v1.0.14\": rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\""
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.822856    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.822936    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.822959    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:03 node-9.domain.tld kubelet[4781]: E0402 11:11:03.823027    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.182916    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.182994    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.183028    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Started /usr/bin/systemctl start man-db-cache-update.
-- Subject: Unit run-r5a4f812a675c423d80600db5baa704ae.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit run-r5a4f812a675c423d80600db5baa704ae.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Reloading.
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: W0402 11:11:04.287560    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check_exception.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.timer is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent_check.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Configuration file /usr/lib/systemd/system/titanagent.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:25: Unknown lvalue 'ProtectHostname' in section 'Service'
Apr 02 11:11:04 node-9.domain.tld systemd[1]: /usr/lib/systemd/system/rdma-ndd.service:26: Unknown lvalue 'ProtectKernelLogs' in section 'Service'
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]: Start to check OSD by-path after node start up.
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]: Node-9 is ready in EOS.
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]: Traceback (most recent call last):
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]:     sys.exit(main())
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:11:04 node-9.domain.tld coaster-startup-checker[22457]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:11:04 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:11:04 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.719207    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.719316    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.719367    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.823583    4781 remote_runtime.go:510] Status from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld kubelet[4781]: E0402 11:11:04.823673    4781 kubelet.go:2173] Container runtime sanity check failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:04 node-9.domain.tld sshd[21559]: Received disconnect from 192.168.11.11 port 46076:11: disconnected by user
Apr 02 11:11:04 node-9.domain.tld sshd[21559]: Disconnected from user root 192.168.11.11 port 46076
Apr 02 11:11:04 node-9.domain.tld sshd[21228]: pam_unix(sshd:session): session closed for user root
Apr 02 11:11:04 node-9.domain.tld systemd-logind[2408]: Session 55 logged out. Waiting for processes to exit.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: session-55.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-55.scope has successfully entered the 'dead' state.
Apr 02 11:11:04 node-9.domain.tld systemd-logind[2408]: Removed session 55.
-- Subject: Session 55 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 55 has been terminated.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:04 node-9.domain.tld systemd[21390]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:04 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld sshd[25930]: Accepted publickey for root from 192.168.11.11 port 46120 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:11:05 node-9.domain.tld systemd-logind[2408]: New session 57 of user root.
-- Subject: A new session 57 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 57 has been created for the user root.
-- 
-- The leading process of the session is 25930.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Started Session 57 of user root.
-- Subject: Unit session-57.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-57.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.183135    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.183198    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.183232    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Startup finished in 119ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 119713 microseconds.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld sshd[25930]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:11:05 node-9.domain.tld sshd[26303]: Received disconnect from 192.168.11.11 port 46120:11: disconnected by user
Apr 02 11:11:05 node-9.domain.tld sshd[26303]: Disconnected from user root 192.168.11.11 port 46120
Apr 02 11:11:05 node-9.domain.tld sshd[25930]: pam_unix(sshd:session): session closed for user root
Apr 02 11:11:05 node-9.domain.tld systemd-logind[2408]: Session 57 logged out. Waiting for processes to exit.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: session-57.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-57.scope has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd-logind[2408]: Removed session 57.
-- Subject: Session 57 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 57 has been terminated.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:05 node-9.domain.tld systemd[26113]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:11:05 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.822944    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.823033    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.823055    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:05 node-9.domain.tld kubelet[4781]: E0402 11:11:05.823120    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:06 node-9.domain.tld kubelet[4781]: E0402 11:11:06.183427    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:06 node-9.domain.tld kubelet[4781]: E0402 11:11:06.183504    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:06 node-9.domain.tld kubelet[4781]: E0402 11:11:06.183542    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.183750    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.183823    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.183856    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: W0402 11:11:07.222215    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: W0402 11:11:07.320304    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {/run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: W0402 11:11:07.432145    4781 clientconn.go:1223] grpc: addrConn.createTransport failed to connect to {unix:///run/containerd/containerd.sock  <nil> 0 <nil>}. Err :connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused". Reconnecting...
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.719113    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.719209    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.719263    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Scheduled restart job, restart counter is at 1.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit containerd.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: Stopped containerd container runtime.
-- Subject: Unit containerd.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit containerd.service has finished shutting down.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 5802 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 5803 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6047 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6088 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6089 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6109 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6120 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6142 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6162 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6181 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 7144 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 7225 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 7625 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 8158 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 8304 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 8354 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 12339 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 13023 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 14309 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 14441 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 14822 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: Starting containerd container runtime...
-- Subject: Unit containerd.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit containerd.service has begun starting up.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 5802 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 5803 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6047 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6088 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6089 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6109 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6120 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6142 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6162 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 6181 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 7144 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 7225 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 7625 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 8158 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 8304 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 8354 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 12339 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 13023 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 14309 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 14441 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: containerd.service: Found left-over process 14822 (containerd-shim) in control group while starting unit. Ignoring.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.
Apr 02 11:11:07 node-9.domain.tld systemd[1]: Started containerd container runtime.
-- Subject: Unit containerd.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit containerd.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.822619    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.822698    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.822732    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:07 node-9.domain.tld kubelet[4781]: E0402 11:11:07.822802    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.852633732+08:00" level=info msg="starting containerd" revision= version=1.5.9
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.916354036+08:00" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.916494978+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.aufs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.920110075+08:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.aufs\"..." error="aufs is not supported (modprobe aufs failed: exit status 1 \"modprobe: FATAL: Module aufs not found in directory /lib/modules/4.18.0-147.5.1.es8_24.x86_64\\n\"): skip plugin" type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.920166078+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.922399813+08:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.btrfs (xfs) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.922437301+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.devmapper\"..." type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.922510043+08:00" level=warning msg="failed to load plugin io.containerd.snapshotter.v1.devmapper" error="devmapper not configured"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.922531772+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.922572740+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.922735319+08:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.zfs\"..." type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.924728175+08:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.zfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.924758848+08:00" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.924800784+08:00" level=warning msg="could not use snapshotter devmapper in metadata plugin" error="devmapper not configured"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.924820149+08:00" level=info msg="metadata content store policy set" policy=shared
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925098512+08:00" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925137521+08:00" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925225247+08:00" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925282720+08:00" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925307521+08:00" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925331120+08:00" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925364490+08:00" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925387773+08:00" level=info msg="loading plugin \"io.containerd.service.v1.leases-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925413843+08:00" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925444908+08:00" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925468228+08:00" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.925564285+08:00" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.988237297+08:00" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989274232+08:00" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989470062+08:00" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989629869+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989673187+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989709879+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989762570+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989798404+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989837742+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989871975+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989905561+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.989941722+08:00" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.990023248+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.990064174+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.990099686+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.990128965+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.990833131+08:00" level=warning msg="`mirrors` is deprecated, please use `config_path` instead"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.990859811+08:00" level=warning msg="`configs.tls` is deprecated, please use `config_path` instead"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.991079054+08:00" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:nvidia DefaultRuntime:{Type: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} UntrustedWorkloadRuntime:{Type: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} Runtimes:map[ecr:{Type:io.containerd.ecr.v2 Engine: PodAnnotations:[io.katacontainers.*] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:true BaseRuntimeSpec:} kata:{Type:io.containerd.kata.v2 Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} nvidia:{Type:io.containerd.runc.v2 Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[BinaryName:/usr/local/nvidia/toolkit/nvidia-container-runtime] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} nvidia-experimental:{Type:io.containerd.runc.v2 Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[BinaryName:/usr/local/nvidia/toolkit/nvidia-container-runtime-experimental] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:} runc:{Type:io.containerd.runc.v2 Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate:} Registry:{ConfigPath: Mirrors:map[docker.io:{Endpoints:[https://registry-1.docker.io]} dockerhub.citicsinfo.com:{Endpoints:[https://dockerhub.citicsinfo.com]} hub.easystack.io:{Endpoints:[https://hub.easystack.io]} hub.ecns.io:{Endpoints:[https://hub.ecns.io]}] Configs:map[dockerhub.citicsinfo.com:{Auth:<nil> TLS:0xc000981080} hub.easystack.io:{Auth:<nil> TLS:0xc000981180} hub.ecns.io:{Auth:<nil> TLS:0xc000981280}] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:hub.easystack.io/captain/pause-amd64:3.0 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:5 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/var/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/var/run/containerd/io.containerd.grpc.v1.cri}"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.991255101+08:00" level=info msg="Connect containerd service"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.991367087+08:00" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.992707661+08:00" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.992781571+08:00" level=info msg="Start subscribing containerd event"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.992930815+08:00" level=info msg="Start recovering state"
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.994351169+08:00" level=info msg=serving... address=/var/run/containerd/debug.sock
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.994534936+08:00" level=info msg=serving... address=/var/run/containerd/containerd.sock.ttrpc
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.994681258+08:00" level=info msg=serving... address=/var/run/containerd/containerd.sock
Apr 02 11:11:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:07.994769851+08:00" level=info msg="containerd successfully booted in 0.146418s"
Apr 02 11:11:08 node-9.domain.tld kubelet[4781]: E0402 11:11:08.184067    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:08 node-9.domain.tld kubelet[4781]: E0402 11:11:08.184140    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:08 node-9.domain.tld kubelet[4781]: E0402 11:11:08.184178    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:08 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:08.879325145+08:00" level=info msg="Start event monitor"
Apr 02 11:11:08 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:08.879421753+08:00" level=info msg="Start snapshots syncer"
Apr 02 11:11:08 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:08.879449867+08:00" level=info msg="Start cni network conf syncer"
Apr 02 11:11:08 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:08.879477442+08:00" level=info msg="Start streaming server"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.184447    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.184538    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.184576    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.668947    4781 remote_image.go:71] ListImages with filter nil from image service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.669015    4781 kuberuntime_image.go:101] ListImages failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: W0402 11:11:09.669027    4781 image_gc_manager.go:199] [imageGCManager] Failed to update image list: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.706417    4781 remote_runtime.go:312] ListContainers with filter &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.706513    4781 kuberuntime_container.go:382] getKubeletContainers failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.706553    4781 remote_runtime.go:312] ListContainers with filter &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.706578    4781 kuberuntime_container.go:382] getKubeletContainers failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.709506    4781 remote_runtime.go:332] ContainerStatus "d0066197d436e24e8fd66ee69e73e7346b2bfad036f276a8df201c0936454799" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.709568    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "d0066197d436e24e8fd66ee69e73e7346b2bfad036f276a8df201c0936454799": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.709736    4781 remote_runtime.go:332] ContainerStatus "e31e7b04599a3f46baa4c370f1b770c07e76da36a35d9e27f7b942f66e18cb3d" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.709770    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "e31e7b04599a3f46baa4c370f1b770c07e76da36a35d9e27f7b942f66e18cb3d": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.709862    4781 remote_runtime.go:332] ContainerStatus "c9bcd608830736e0e7993641d90ed864ecda6a94a0a877df3e3d1d1bd3901400" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.709886    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "c9bcd608830736e0e7993641d90ed864ecda6a94a0a877df3e3d1d1bd3901400": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710069    4781 remote_runtime.go:332] ContainerStatus "21f404f005f9357190dcee8b119189bcf76df302816901ea8e7bac916e010ac6" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710097    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "21f404f005f9357190dcee8b119189bcf76df302816901ea8e7bac916e010ac6": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710182    4781 remote_runtime.go:332] ContainerStatus "f4ebec9fa110f941e87d599b9df0584bc217928b5148e686332ace8458f9549d" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710206    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "f4ebec9fa110f941e87d599b9df0584bc217928b5148e686332ace8458f9549d": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710287    4781 remote_runtime.go:332] ContainerStatus "d8b91bfa949cd3989d7c55e95365e48c9a3c24257eecf4511febb8ef824fc7b2" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710311    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "d8b91bfa949cd3989d7c55e95365e48c9a3c24257eecf4511febb8ef824fc7b2": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710391    4781 remote_runtime.go:332] ContainerStatus "f8226205ea0b5d381cae8d3e6d2753774c0bdac5082f3acbe9d16351a598255c" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710415    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "f8226205ea0b5d381cae8d3e6d2753774c0bdac5082f3acbe9d16351a598255c": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710493    4781 remote_runtime.go:332] ContainerStatus "dbf3cc8aee249a0b850398bb5ef5fc00589ece120543eae9a3ecf217399fc7c1" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710517    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "dbf3cc8aee249a0b850398bb5ef5fc00589ece120543eae9a3ecf217399fc7c1": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710605    4781 remote_runtime.go:332] ContainerStatus "e76020af15ef089ab612ab9fc9215859ac4d16d8b59ebf9d0469b5eb2aa203c6" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710629    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "e76020af15ef089ab612ab9fc9215859ac4d16d8b59ebf9d0469b5eb2aa203c6": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710876    4781 remote_runtime.go:332] ContainerStatus "b4d3f70134663f5bc070e327ad9fcc97d2d7d856674fae2f1f906339cd393264" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.710906    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "b4d3f70134663f5bc070e327ad9fcc97d2d7d856674fae2f1f906339cd393264": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.710992    4781 remote_runtime.go:332] ContainerStatus "4b0ed189bf5dfbd318bd316a7602b26332ee6d89111beeb21a05f07bd31ae5c9" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.711017    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "4b0ed189bf5dfbd318bd316a7602b26332ee6d89111beeb21a05f07bd31ae5c9": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.711097    4781 remote_runtime.go:332] ContainerStatus "092b1b2504bce39b57cd8a63cd8699436a1a3e76f6d91d26a58597b37087010d" from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: I0402 11:11:09.711120    4781 kuberuntime_gc.go:360] Error getting ContainerStatus for containerID "092b1b2504bce39b57cd8a63cd8699436a1a3e76f6d91d26a58597b37087010d": rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.711205    4781 kubelet.go:1264] Container garbage collection failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.823963    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.824013    4781 remote_runtime.go:510] Status from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.824057    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.824059    4781 kubelet.go:2173] Container runtime sanity check failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.824082    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.824161    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.868434    4781 remote_runtime.go:312] ListContainers with filter &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:09 node-9.domain.tld kubelet[4781]: E0402 11:11:09.868492    4781 container_log_manager.go:183] Failed to rotate container logs: failed to list containers: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.184768    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.184825    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.184860    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.323276    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.323360    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.323406    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.719154    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.719265    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.719319    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.867151    4781 remote_image.go:151] ImageFsInfo from image service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:10 node-9.domain.tld kubelet[4781]: E0402 11:11:10.867230    4781 eviction_manager.go:260] eviction manager: failed to get summary stats: failed to get imageFs stats: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.185062    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.185130    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.185164    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.406013    4781 remote_runtime.go:86] Version from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.822799    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.822891    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.822915    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:11 node-9.domain.tld kubelet[4781]: E0402 11:11:11.822986    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:12 node-9.domain.tld kubelet[4781]: E0402 11:11:12.185362    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:12 node-9.domain.tld kubelet[4781]: E0402 11:11:12.185424    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:12 node-9.domain.tld kubelet[4781]: E0402 11:11:12.185461    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.185647    4781 remote_runtime.go:206] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.185715    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.185763    4781 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.719175    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.719275    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.719328    4781 remote_runtime.go:392] ExecSync 677b4e8138791447db3af2c5e3f1497864976477cad900c2506b217f03d3cb25 'nc -z -w3 127.0.0.1 10665' from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.822666    4781 remote_runtime.go:206] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.822756    4781 kuberuntime_sandbox.go:227] ListPodSandbox failed: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.822778    4781 kubelet_pods.go:1092] Error listing containers: &status.statusError{state:impl.MessageState{NoUnkeyedLiterals:pragma.NoUnkeyedLiterals{}, DoNotCompare:pragma.DoNotCompare{}, DoNotCopy:pragma.DoNotCopy{}, atomicMessageInfo:(*impl.MessageInfo)(nil)}, sizeCache:0, unknownFields:[]uint8(nil), Code:14, Message:"connection error: desc = \"transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused\"", Details:[]*anypb.Any(nil)}
Apr 02 11:11:13 node-9.domain.tld kubelet[4781]: E0402 11:11:13.822847    4781 kubelet.go:1997] Failed cleaning pods: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: connection refused"
Apr 02 11:11:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:14.194322262+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\""
Apr 02 11:11:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:14.194492532+08:00" level=info msg="Container to stop \"c2fdfb69a7778a307702ed14eb829ee2ae564440efc1234a51853228ad673d7a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:11:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:14.195043540+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,}"
Apr 02 11:11:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:14.250040427+08:00" level=info msg="TearDown network for sandbox \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" successfully"
Apr 02 11:11:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:14.250111867+08:00" level=info msg="StopPodSandbox for \"caba7ddc1251f142dab2ec151db56c97e690cc5f8edfd4a9a0135fb33fe724dc\" returns successfully"
Apr 02 11:11:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:14.251078338+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,}"
Apr 02 11:11:14 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:11:14 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 2.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:11:14 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:11:14 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld sshd[30386]: Accepted publickey for root from 127.0.0.1 port 45326 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:11:15 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:11:15 node-9.domain.tld systemd-logind[2408]: New session 59 of user root.
-- Subject: A new session 59 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 59 has been created for the user root.
-- 
-- The leading process of the session is 30386.
Apr 02 11:11:15 node-9.domain.tld systemd[1]: Started Session 59 of user root.
-- Subject: Unit session-59.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-59.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld systemd[30424]: Startup finished in 119ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 119098 microseconds.
Apr 02 11:11:15 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:15 node-9.domain.tld sshd[30386]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:11:15 node-9.domain.tld /etc/sysconfig/network-scripts/ifup-ipv6[30616]: Global IPv6 forwarding is disabled in configuration, but not currently disabled in kernel
Apr 02 11:11:15 node-9.domain.tld /etc/sysconfig/network-scripts/ifup-ipv6[30617]: Please restart network with '/sbin/service network restart'
Apr 02 11:11:15 node-9.domain.tld sshd[30463]: Received disconnect from 127.0.0.1 port 45326:11: disconnected by user
Apr 02 11:11:15 node-9.domain.tld sshd[30463]: Disconnected from user root 127.0.0.1 port 45326
Apr 02 11:11:15 node-9.domain.tld sshd[30386]: pam_unix(sshd:session): session closed for user root
Apr 02 11:11:15 node-9.domain.tld systemd-logind[2408]: Session 59 logged out. Waiting for processes to exit.
Apr 02 11:11:15 node-9.domain.tld kubelet[4781]: I0402 11:11:15.815300    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:11:15 node-9.domain.tld kubelet[4781]: E0402 11:11:15.815871    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:16 node-9.domain.tld coaster-startup-checker[30277]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]: Start to check OSD by-path after node start up.
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]: Node-9 is ready in EOS.
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]: Traceback (most recent call last):
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]:     sys.exit(main())
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:11:17 node-9.domain.tld coaster-startup-checker[30277]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:11:17 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:11:17 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: session-59.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-59.scope has successfully entered the 'dead' state.
Apr 02 11:11:17 node-9.domain.tld systemd-logind[2408]: Removed session 59.
-- Subject: Session 59 has been terminated
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A session with the ID 59 has been terminated.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:17 node-9.domain.tld systemd[30424]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:11:17 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:11:18 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:18.816115534+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:11:19 node-9.domain.tld systemd-udevd[31753]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:11:19 node-9.domain.tld systemd-udevd[31753]: Could not generate persistent MAC address for 9998900196eb_c: No such file or directory
Apr 02 11:11:19 node-9.domain.tld systemd-udevd[31754]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:11:19 node-9.domain.tld systemd-udevd[31754]: Could not generate persistent MAC address for 9998900196eb_h: No such file or directory
Apr 02 11:11:19 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:11:19 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:11:19 node-9.domain.tld kernel: cni0: port 6(vethe4a44020) entered blocking state
Apr 02 11:11:19 node-9.domain.tld kernel: cni0: port 6(vethe4a44020) entered disabled state
Apr 02 11:11:19 node-9.domain.tld kernel: device vethe4a44020 entered promiscuous mode
Apr 02 11:11:19 node-9.domain.tld kernel: cni0: port 6(vethe4a44020) entered blocking state
Apr 02 11:11:19 node-9.domain.tld kernel: cni0: port 6(vethe4a44020) entered forwarding state
Apr 02 11:11:20 node-9.domain.tld systemd-udevd[31780]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:11:20 node-9.domain.tld systemd-udevd[31780]: Could not generate persistent MAC address for vethe4a44020: No such file or directory
Apr 02 11:11:20 node-9.domain.tld kernel: device 9998900196eb_h entered promiscuous mode
Apr 02 11:11:20 node-9.domain.tld kernel: eth0: renamed from 9998900196eb_c
Apr 02 11:11:20 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:11:20 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.470015575+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be pid=32204
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.582170320+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/5c34207d8e1ef81096ac5df9a7a126dce9af22e908762dcdf79275c666b3af71 pid=32293
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.694655234+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:alluxio-fuse-hjvv5,Uid:e74039f9-9bee-4e6a-ade7-535f5d7a309e,Namespace:aipaas-public-alluxio,Attempt:0,} returns sandbox id \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.697829546+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.763918787+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.766973428+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.772446681+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.776935523+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise@sha256:13bfc4ff955c6ee924e04360a6f167f958df38ff1c2623b3ded7529c93dc62b6,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.779393851+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\" returns image reference \"sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.783321751+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for container &ContainerMetadata{Name:umount-path,Attempt:0,}"
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.790651573+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:gpu-feature-discovery-9bc7l,Uid:a5c9f638-c13f-4618-9c2a-9d56ce52f483,Namespace:imp-qgpu,Attempt:1,} returns sandbox id \"5c34207d8e1ef81096ac5df9a7a126dce9af22e908762dcdf79275c666b3af71\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.794467462+08:00" level=info msg="CreateContainer within sandbox \"5c34207d8e1ef81096ac5df9a7a126dce9af22e908762dcdf79275c666b3af71\" for container &ContainerMetadata{Name:toolkit-validation,Attempt:0,}"
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.829914747+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for &ContainerMetadata{Name:umount-path,Attempt:0,} returns container id \"83de256a48d66bd6e115947cbf204fdb9ce6954c504a08d5189843d8cfb40bfc\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.830461239+08:00" level=info msg="StartContainer for \"83de256a48d66bd6e115947cbf204fdb9ce6954c504a08d5189843d8cfb40bfc\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.860936973+08:00" level=info msg="CreateContainer within sandbox \"5c34207d8e1ef81096ac5df9a7a126dce9af22e908762dcdf79275c666b3af71\" for &ContainerMetadata{Name:toolkit-validation,Attempt:0,} returns container id \"f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99\""
Apr 02 11:11:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:21.861495004+08:00" level=info msg="StartContainer for \"f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99\""
Apr 02 11:11:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:22.065176716+08:00" level=info msg="StartContainer for \"83de256a48d66bd6e115947cbf204fdb9ce6954c504a08d5189843d8cfb40bfc\" returns successfully"
Apr 02 11:11:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:22.219001775+08:00" level=info msg="shim disconnected" id=83de256a48d66bd6e115947cbf204fdb9ce6954c504a08d5189843d8cfb40bfc
Apr 02 11:11:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:22.219135147+08:00" level=warning msg="cleaning up after shim disconnected" id=83de256a48d66bd6e115947cbf204fdb9ce6954c504a08d5189843d8cfb40bfc namespace=k8s.io
Apr 02 11:11:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:22.219177203+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:22.255673576+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:22+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=32643\n"
Apr 02 11:11:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:22.422446788+08:00" level=info msg="StartContainer for \"f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99\" returns successfully"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.234302128+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\""
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.303887967+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.307183719+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.313328620+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.318973008+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise@sha256:13bfc4ff955c6ee924e04360a6f167f958df38ff1c2623b3ded7529c93dc62b6,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.321437036+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\" returns image reference \"sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b\""
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.326735936+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for container &ContainerMetadata{Name:path-permission,Attempt:0,}"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.359952732+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for &ContainerMetadata{Name:path-permission,Attempt:0,} returns container id \"c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922\""
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.360500170+08:00" level=info msg="StartContainer for \"c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922\""
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.590073784+08:00" level=info msg="StartContainer for \"c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922\" returns successfully"
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.690146625+08:00" level=info msg="shim disconnected" id=c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.690241228+08:00" level=warning msg="cleaning up after shim disconnected" id=c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922 namespace=k8s.io
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.690264953+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:23 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:23.725673681+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:23+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=33191\n"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.243330078+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\""
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.290011400+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.292175381+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.296633484+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.300295614+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise@sha256:13bfc4ff955c6ee924e04360a6f167f958df38ff1c2623b3ded7529c93dc62b6,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.302998609+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\" returns image reference \"sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b\""
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.306419862+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for container &ContainerMetadata{Name:create-alluxio-fuse-dir,Attempt:0,}"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.332168037+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for &ContainerMetadata{Name:create-alluxio-fuse-dir,Attempt:0,} returns container id \"8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649\""
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.332672450+08:00" level=info msg="StartContainer for \"8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649\""
Apr 02 11:11:24 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:11:24 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:11:24 node-9.domain.tld kernel: cni0: port 7(vethe4558861) entered blocking state
Apr 02 11:11:24 node-9.domain.tld kernel: cni0: port 7(vethe4558861) entered disabled state
Apr 02 11:11:24 node-9.domain.tld kernel: device vethe4558861 entered promiscuous mode
Apr 02 11:11:24 node-9.domain.tld kernel: cni0: port 7(vethe4558861) entered blocking state
Apr 02 11:11:24 node-9.domain.tld kernel: cni0: port 7(vethe4558861) entered forwarding state
Apr 02 11:11:24 node-9.domain.tld systemd-udevd[33469]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:11:24 node-9.domain.tld systemd-udevd[33469]: Could not generate persistent MAC address for vethe4558861: No such file or directory
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.621789034+08:00" level=info msg="StartContainer for \"8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649\" returns successfully"
Apr 02 11:11:24 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.726047209+08:00" level=info msg="shim disconnected" id=8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.726161089+08:00" level=warning msg="cleaning up after shim disconnected" id=8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649 namespace=k8s.io
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.726188952+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:24.761175716+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:24+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=33550\n"
Apr 02 11:11:24 node-9.domain.tld kubelet[4781]: W0402 11:11:24.910651    4781 manager.go:1176] Failed to process watch event {EventType:0 Name:/kubepods/burstable/pode74039f9-9bee-4e6a-ade7-535f5d7a309e/c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922 WatchSource:0}: task c06e769f7bf73a7055d776c24b9277083340c939c87e3990224813d576ce7922 not found: not found
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.251581119+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for container &ContainerMetadata{Name:wait-master,Attempt:0,}"
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.283069434+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for &ContainerMetadata{Name:wait-master,Attempt:0,} returns container id \"e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76\""
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.283569319+08:00" level=info msg="StartContainer for \"e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76\""
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.510943782+08:00" level=info msg="StartContainer for \"e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76\" returns successfully"
Apr 02 11:11:25 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.684257317+08:00" level=info msg="shim disconnected" id=e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.684351876+08:00" level=warning msg="cleaning up after shim disconnected" id=e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76 namespace=k8s.io
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.684377342+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:25 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:25.719045980+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:25+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=33862\n"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.256079799+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\""
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.305158494+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.308064339+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.313368870+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.317768805+08:00" level=info msg="ImageUpdate event &ImageUpdate{Name:dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise@sha256:13bfc4ff955c6ee924e04360a6f167f958df38ff1c2623b3ded7529c93dc62b6,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.320035290+08:00" level=info msg="PullImage \"dockerhub.citicsinfo.com/aipaas-public-alluxio/alluxio-enterprise:zhongxin-AI-3.0-1.1.0-v1\" returns image reference \"sha256:0beda48084f5f705bc6d061080fe9c9638acf9a8f54d7749022f4037684df44b\""
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.324186909+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for container &ContainerMetadata{Name:alluxio-fuse,Attempt:0,}"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.368007665+08:00" level=info msg="CreateContainer within sandbox \"9998900196eb9862c9417ee1047371ec104ae5f522f4835f4f007376580691be\" for &ContainerMetadata{Name:alluxio-fuse,Attempt:0,} returns container id \"737b1b0d97c45b814fe38d67c50cdcc8891c100ae08f8812fd73967454cf38d5\""
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.368497539+08:00" level=info msg="StartContainer for \"737b1b0d97c45b814fe38d67c50cdcc8891c100ae08f8812fd73967454cf38d5\""
Apr 02 11:11:26 node-9.domain.tld kubelet[4781]: W0402 11:11:26.416117    4781 manager.go:1176] Failed to process watch event {EventType:0 Name:/kubepods/burstable/pode74039f9-9bee-4e6a-ade7-535f5d7a309e/8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649 WatchSource:0}: task 8927fbc0447d18c10841906247cf5d6654f820bcce1169d1f8e9ad3a2001b649 not found: not found
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.583115490+08:00" level=info msg="StartContainer for \"737b1b0d97c45b814fe38d67c50cdcc8891c100ae08f8812fd73967454cf38d5\" returns successfully"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.615078443+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975 pid=34277
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.835776505+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-operator-validator-zlljs,Uid:cd15a183-f8fb-4b52-a26d-5fe952a47e48,Namespace:imp-qgpu,Attempt:0,} returns sandbox id \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\""
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.840779782+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for container &ContainerMetadata{Name:driver-validation,Attempt:0,}"
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.914178240+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for &ContainerMetadata{Name:driver-validation,Attempt:0,} returns container id \"de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de\""
Apr 02 11:11:26 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:26.914780354+08:00" level=info msg="StartContainer for \"de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de\""
Apr 02 11:11:27 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:27.262684334+08:00" level=info msg="StartContainer for \"de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de\" returns successfully"
Apr 02 11:11:27 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:11:27 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 3.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:11:27 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:11:27 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:27 node-9.domain.tld kubelet[4781]: W0402 11:11:27.922705    4781 manager.go:1176] Failed to process watch event {EventType:0 Name:/kubepods/burstable/pode74039f9-9bee-4e6a-ade7-535f5d7a309e/e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76 WatchSource:0}: task e6f8a47ab8c8ec26a5d8537a50159fd0c6eb41a29ae7acc090772f1048bebb76 not found: not found
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:28 node-9.domain.tld kernel: perf: interrupt took too long (2693 > 2500), lowering kernel.perf_event_max_sample_rate to 74000
Apr 02 11:11:28 node-9.domain.tld coaster-startup-checker[34672]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:11:28 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:28.937572240+08:00" level=info msg="shim disconnected" id=de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de
Apr 02 11:11:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:28.937708695+08:00" level=warning msg="cleaning up after shim disconnected" id=de2dcdf9f3ec15f00f40d436de99628029d8c9c75e7cf51456e79ebad4c197de namespace=k8s.io
Apr 02 11:11:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:28.937746455+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:28.974025689+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:28+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=35078\n"
Apr 02 11:11:29 node-9.domain.tld kernel: fuse init (API version 7.27)
Apr 02 11:11:29 node-9.domain.tld systemd[1]: Mounting FUSE Control File System...
-- Subject: Unit sys-fs-fuse-connections.mount has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sys-fs-fuse-connections.mount has begun starting up.
Apr 02 11:11:29 node-9.domain.tld systemd[1]: Mounted FUSE Control File System.
-- Subject: Unit sys-fs-fuse-connections.mount has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit sys-fs-fuse-connections.mount has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:29 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:29.289594019+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for container &ContainerMetadata{Name:toolkit-validation,Attempt:0,}"
Apr 02 11:11:29 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:29.353982908+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for &ContainerMetadata{Name:toolkit-validation,Attempt:0,} returns container id \"c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd\""
Apr 02 11:11:29 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:29.354478589+08:00" level=info msg="StartContainer for \"c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd\""
Apr 02 11:11:29 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:29.722351679+08:00" level=info msg="StartContainer for \"c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd\" returns successfully"
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]: Start to check OSD by-path after node start up.
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]: Node-9 is ready in EOS.
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]: Traceback (most recent call last):
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]:     sys.exit(main())
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:11:29 node-9.domain.tld coaster-startup-checker[34672]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:11:29 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:11:29 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:11:30 node-9.domain.tld kubelet[4781]: I0402 11:11:30.815212    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:11:30 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:30.818717954+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for container &ContainerMetadata{Name:qgpu-exporter,Attempt:3,}"
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.283396437+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for &ContainerMetadata{Name:qgpu-exporter,Attempt:3,} returns container id \"3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4\""
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.284118397+08:00" level=info msg="StartContainer for \"3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4\""
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.374434528+08:00" level=info msg="shim disconnected" id=c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.374546373+08:00" level=warning msg="cleaning up after shim disconnected" id=c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd namespace=k8s.io
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.374583703+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.410795074+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:31+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=35889\n"
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.457459045+08:00" level=info msg="StartContainer for \"3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4\" returns successfully"
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.604185539+08:00" level=info msg="shim disconnected" id=3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.604298049+08:00" level=warning msg="cleaning up after shim disconnected" id=3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4 namespace=k8s.io
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.604325126+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:31 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:31.630690672+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:31+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=36001\n"
Apr 02 11:11:31 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:31 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-c748aa8ac1fe35ecc5c9dc39235027f972a20a0accc09a79e1f0338b81ea61fd-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:32 node-9.domain.tld kubelet[4781]: I0402 11:11:32.305231    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f
Apr 02 11:11:32 node-9.domain.tld kubelet[4781]: I0402 11:11:32.305782    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:11:32 node-9.domain.tld kubelet[4781]: E0402 11:11:32.306377    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.306452731+08:00" level=info msg="RemoveContainer for \"bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f\""
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.306969148+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for container &ContainerMetadata{Name:cuda-validation,Attempt:0,}"
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.316223367+08:00" level=info msg="RemoveContainer for \"bbc4dcbce038cdefb5edd804e232248757cdbaf69787ea51e49dd9e87e56ab4f\" returns successfully"
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.374996164+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for &ContainerMetadata{Name:cuda-validation,Attempt:0,} returns container id \"0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee\""
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.375547351+08:00" level=info msg="StartContainer for \"0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee\""
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.550382583+08:00" level=info msg="shim disconnected" id=f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.550506514+08:00" level=warning msg="cleaning up after shim disconnected" id=f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99 namespace=k8s.io
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.550532095+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.591384213+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:32+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=36332\n"
Apr 02 11:11:32 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:32.747959058+08:00" level=info msg="StartContainer for \"0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee\" returns successfully"
Apr 02 11:11:32 node-9.domain.tld kubelet[4781]: I0402 11:11:32.841466    4781 topology_manager.go:187] [topologymanager] Topology Admit Handler
Apr 02 11:11:32 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-f224597d01ad4fbd65d6c68a21697c3eda207278643d638cb70edb36bff11e99-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:32 node-9.domain.tld kubelet[4781]: I0402 11:11:32.925523    4781 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "nvidia-operator-validator-token-7vtbt" (UniqueName: "kubernetes.io/secret/736fe247-efac-4719-bb68-139e52ac9cc8-nvidia-operator-validator-token-7vtbt") pod "nvidia-cuda-validator-qn4dd" (UID: "736fe247-efac-4719-bb68-139e52ac9cc8")
Apr 02 11:11:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:33.152422024+08:00" level=info msg="RunPodsandbox for &PodSandboxMetadata{Name:nvidia-cuda-validator-qn4dd,Uid:736fe247-efac-4719-bb68-139e52ac9cc8,Namespace:imp-qgpu,Attempt:0,}"
Apr 02 11:11:33 node-9.domain.tld kubelet[4781]: I0402 11:11:33.316714    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: c2fdfb69a7778a307702ed14eb829ee2ae564440efc1234a51853228ad673d7a
Apr 02 11:11:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:33.320117782+08:00" level=info msg="CreateContainer within sandbox \"5c34207d8e1ef81096ac5df9a7a126dce9af22e908762dcdf79275c666b3af71\" for container &ContainerMetadata{Name:gpu-feature-discovery,Attempt:1,}"
Apr 02 11:11:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:33.396102778+08:00" level=info msg="CreateContainer within sandbox \"5c34207d8e1ef81096ac5df9a7a126dce9af22e908762dcdf79275c666b3af71\" for &ContainerMetadata{Name:gpu-feature-discovery,Attempt:1,} returns container id \"2faede492064d57481886c7f02c30641b837cc4dd50266677b750fc8c70026d8\""
Apr 02 11:11:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:33.396703877+08:00" level=info msg="StartContainer for \"2faede492064d57481886c7f02c30641b837cc4dd50266677b750fc8c70026d8\""
Apr 02 11:11:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:33.980649764+08:00" level=info msg="StartContainer for \"2faede492064d57481886c7f02c30641b837cc4dd50266677b750fc8c70026d8\" returns successfully"
Apr 02 11:11:34 node-9.domain.tld systemd[1]: Starting titanagent check exception...
-- Subject: Unit titanagent_check_exception.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has begun starting up.
Apr 02 11:11:38 node-9.domain.tld systemd[1]: titanagent_check_exception.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check_exception.service has successfully entered the 'dead' state.
Apr 02 11:11:38 node-9.domain.tld systemd[1]: Started titanagent check exception.
-- Subject: Unit titanagent_check_exception.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:38 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready
Apr 02 11:11:38 node-9.domain.tld kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Apr 02 11:11:38 node-9.domain.tld kernel: cni0: port 8(veth2f3d33d4) entered blocking state
Apr 02 11:11:38 node-9.domain.tld kernel: cni0: port 8(veth2f3d33d4) entered disabled state
Apr 02 11:11:38 node-9.domain.tld kernel: device veth2f3d33d4 entered promiscuous mode
Apr 02 11:11:38 node-9.domain.tld kernel: cni0: port 8(veth2f3d33d4) entered blocking state
Apr 02 11:11:38 node-9.domain.tld kernel: cni0: port 8(veth2f3d33d4) entered forwarding state
Apr 02 11:11:38 node-9.domain.tld systemd-udevd[38577]: link_config: autonegotiation is unset or enabled, the speed and duplex are not writable.
Apr 02 11:11:38 node-9.domain.tld systemd-udevd[38577]: Could not generate persistent MAC address for veth2f3d33d4: No such file or directory
Apr 02 11:11:40 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:11:40 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 4.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:11:40 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:11:40 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:40 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:40.092710654+08:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d pid=38968
Apr 02 11:11:40 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:40.355613673+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nvidia-cuda-validator-qn4dd,Uid:736fe247-efac-4719-bb68-139e52ac9cc8,Namespace:imp-qgpu,Attempt:0,} returns sandbox id \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\""
Apr 02 11:11:40 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:40.359540793+08:00" level=info msg="CreateContainer within sandbox \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" for container &ContainerMetadata{Name:cuda-validation,Attempt:0,}"
Apr 02 11:11:40 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:40.420944622+08:00" level=info msg="CreateContainer within sandbox \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" for &ContainerMetadata{Name:cuda-validation,Attempt:0,} returns container id \"472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903\""
Apr 02 11:11:40 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:40.421517755+08:00" level=info msg="StartContainer for \"472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903\""
Apr 02 11:11:40 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:40.798631990+08:00" level=info msg="StartContainer for \"472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903\" returns successfully"
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:41 node-9.domain.tld coaster-startup-checker[38942]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:11:41 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:41 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:41.854324571+08:00" level=info msg="shim disconnected" id=472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903
Apr 02 11:11:41 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:41.854449286+08:00" level=warning msg="cleaning up after shim disconnected" id=472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903 namespace=k8s.io
Apr 02 11:11:41 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:41.854475187+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:41 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:41.891889738+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:41+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=39675\n"
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.363570777+08:00" level=info msg="CreateContainer within sandbox \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" for container &ContainerMetadata{Name:nvidia-cuda-validator,Attempt:0,}"
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.420902074+08:00" level=info msg="CreateContainer within sandbox \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" for &ContainerMetadata{Name:nvidia-cuda-validator,Attempt:0,} returns container id \"82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36\""
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.421614757+08:00" level=info msg="StartContainer for \"82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36\""
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]: Start to check OSD by-path after node start up.
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]: Node-9 is ready in EOS.
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]: Traceback (most recent call last):
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]:     sys.exit(main())
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:11:42 node-9.domain.tld coaster-startup-checker[38942]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:11:42 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:11:42 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.847368260+08:00" level=info msg="StartContainer for \"82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36\" returns successfully"
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.946644538+08:00" level=info msg="shim disconnected" id=82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.946768034+08:00" level=warning msg="cleaning up after shim disconnected" id=82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36 namespace=k8s.io
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.946799809+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:42 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:42.983015998+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:42+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=40077\n"
Apr 02 11:11:43 node-9.domain.tld kubelet[4781]: I0402 11:11:43.366333    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.366559575+08:00" level=info msg="StopPodSandbox for \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\""
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.366712692+08:00" level=info msg="Container to stop \"472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.366809298+08:00" level=info msg="Container to stop \"82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:11:43 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:43 node-9.domain.tld systemd[1]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d-shm.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.grpc.v1.cri-sandboxes-039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d-shm.mount has successfully entered the 'dead' state.
Apr 02 11:11:43 node-9.domain.tld kubelet[4781]: I0402 11:11:43.482757    4781 reconciler.go:196] operationExecutor.UnmountVolume started for volume "nvidia-operator-validator-token-7vtbt" (UniqueName: "kubernetes.io/secret/736fe247-efac-4719-bb68-139e52ac9cc8-nvidia-operator-validator-token-7vtbt") pod "736fe247-efac-4719-bb68-139e52ac9cc8" (UID: "736fe247-efac-4719-bb68-139e52ac9cc8")
Apr 02 11:11:43 node-9.domain.tld systemd[1]: var-lib-kubelet-pods-736fe247\x2defac\x2d4719\x2dbb68\x2d139e52ac9cc8-volumes-kubernetes.io\x7esecret-nvidia\x2doperator\x2dvalidator\x2dtoken\x2d7vtbt.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit var-lib-kubelet-pods-736fe247\x2defac\x2d4719\x2dbb68\x2d139e52ac9cc8-volumes-kubernetes.io\x7esecret-nvidia\x2doperator\x2dvalidator\x2dtoken\x2d7vtbt.mount has successfully entered the 'dead' state.
Apr 02 11:11:43 node-9.domain.tld kubelet[4781]: I0402 11:11:43.506461    4781 operation_generator.go:813] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/736fe247-efac-4719-bb68-139e52ac9cc8-nvidia-operator-validator-token-7vtbt" (OuterVolumeSpecName: "nvidia-operator-validator-token-7vtbt") pod "736fe247-efac-4719-bb68-139e52ac9cc8" (UID: "736fe247-efac-4719-bb68-139e52ac9cc8"). InnerVolumeSpecName "nvidia-operator-validator-token-7vtbt". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 02 11:11:43 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.532108362+08:00" level=info msg="shim disconnected" id=039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.532194504+08:00" level=warning msg="cleaning up after shim disconnected" id=039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d namespace=k8s.io
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.532219959+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.570888618+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:43+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=40315\n"
Apr 02 11:11:43 node-9.domain.tld kubelet[4781]: I0402 11:11:43.584039    4781 reconciler.go:319] Volume detached for volume "nvidia-operator-validator-token-7vtbt" (UniqueName: "kubernetes.io/secret/736fe247-efac-4719-bb68-139e52ac9cc8-nvidia-operator-validator-token-7vtbt") on node "node-9" DevicePath ""
Apr 02 11:11:43 node-9.domain.tld kernel: cni0: port 8(veth2f3d33d4) entered disabled state
Apr 02 11:11:43 node-9.domain.tld kernel: device veth2f3d33d4 left promiscuous mode
Apr 02 11:11:43 node-9.domain.tld kernel: cni0: port 8(veth2f3d33d4) entered disabled state
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.744914857+08:00" level=info msg="TearDown network for sandbox \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" successfully"
Apr 02 11:11:43 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:43.744990876+08:00" level=info msg="StopPodSandbox for \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" returns successfully"
Apr 02 11:11:43 node-9.domain.tld systemd[1]: run-netns-cni\x2d68c7d882\x2d236d\x2d43a8\x2dbbbe\x2db8f621eac801.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-netns-cni\x2d68c7d882\x2d236d\x2d43a8\x2dbbbe\x2db8f621eac801.mount has successfully entered the 'dead' state.
Apr 02 11:11:43 node-9.domain.tld kubelet[4781]: W0402 11:11:43.974307    4781 manager.go:1176] Failed to process watch event {EventType:0 Name:/kubepods/besteffort/pod736fe247-efac-4719-bb68-139e52ac9cc8/82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36 WatchSource:0}: task 82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36 not found: not found
Apr 02 11:11:44 node-9.domain.tld kubelet[4781]: W0402 11:11:44.372379    4781 pod_container_deletor.go:79] Container "039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d" not found in pod's containers
Apr 02 11:11:44 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:44.372875251+08:00" level=info msg="StopPodSandbox for \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\""
Apr 02 11:11:44 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:44.373037512+08:00" level=info msg="Container to stop \"472833a9b25eb90e89bb38b82c88654fcf8d1eabbf3da3b0b142f5e1c6d79903\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:11:44 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:44.373126737+08:00" level=info msg="Container to stop \"82895cf0cc7ce90dcf02e32e3aba1080efa5152adce0bd89d88db20c5f7cbf36\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Apr 02 11:11:44 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:44.428065780+08:00" level=info msg="TearDown network for sandbox \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" successfully"
Apr 02 11:11:44 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:44.428135749+08:00" level=info msg="StopPodSandbox for \"039150cceeb7c3a5c25dbdd0a2188c675ebb72b83862e1b08a018c64246d889d\" returns successfully"
Apr 02 11:11:45 node-9.domain.tld kubelet[4781]: I0402 11:11:45.815329    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:11:45 node-9.domain.tld kubelet[4781]: E0402 11:11:45.815896    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:11:47 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:47 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:47.978456104+08:00" level=info msg="shim disconnected" id=0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee
Apr 02 11:11:47 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:47.978587339+08:00" level=warning msg="cleaning up after shim disconnected" id=0d3811aa17b367ea440fc6978640ace8161a20cd55f4e2eb44f7aa16cf918cee namespace=k8s.io
Apr 02 11:11:47 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:47.978618715+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:48 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:48.013586432+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:47+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=41503\n"
Apr 02 11:11:48 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:48.394107852+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for container &ContainerMetadata{Name:nvidia-operator-validator,Attempt:0,}"
Apr 02 11:11:48 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:48.457398817+08:00" level=info msg="CreateContainer within sandbox \"df2ea2a4e5b29bd6c40e0bf8ddc97f6a7ce983dd4f291eae030289427c4ff975\" for &ContainerMetadata{Name:nvidia-operator-validator,Attempt:0,} returns container id \"990f3686aa0f243e4a20679c1d0e6ffcec72733d884bd830c02caf2e4087f1b1\""
Apr 02 11:11:48 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:48.457972768+08:00" level=info msg="StartContainer for \"990f3686aa0f243e4a20679c1d0e6ffcec72733d884bd830c02caf2e4087f1b1\""
Apr 02 11:11:48 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:48.834997734+08:00" level=info msg="StartContainer for \"990f3686aa0f243e4a20679c1d0e6ffcec72733d884bd830c02caf2e4087f1b1\" returns successfully"
Apr 02 11:11:52 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:11:52 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 5.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:11:52 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:11:52 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:11:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:52.914332481+08:00" level=info msg="shim disconnected" id=14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3
Apr 02 11:11:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:52.914458430+08:00" level=warning msg="cleaning up after shim disconnected" id=14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3 namespace=k8s.io
Apr 02 11:11:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:52.914486857+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:11:52 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:11:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:52.941735369+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:11:52+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=42791\n"
Apr 02 11:11:53 node-9.domain.tld kernel: pgpu:0, memleft:40460, pgpu_mem:40960
Apr 02 11:11:53 node-9.domain.tld kernel: pgpu:1, memleft:40460, pgpu_mem:40960
Apr 02 11:11:53 node-9.domain.tld kernel: pgpu:2, memleft:40460, pgpu_mem:40960
Apr 02 11:11:53 node-9.domain.tld kernel: pgpu:3, memleft:40460, pgpu_mem:40960
Apr 02 11:11:53 node-9.domain.tld kernel: Initialized qGPU with parameter: Device ID: 20f1, Video Memory: 40960M, attached pGPU number: 4.
Apr 02 11:11:53 node-9.domain.tld kubelet[4781]: I0402 11:11:53.418070    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3
Apr 02 11:11:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:53.422177371+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for container &ContainerMetadata{Name:worker,Attempt:1,}"
Apr 02 11:11:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:53.463712306+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for &ContainerMetadata{Name:worker,Attempt:1,} returns container id \"6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23\""
Apr 02 11:11:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:53.464305130+08:00" level=info msg="StartContainer for \"6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23\""
Apr 02 11:11:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:11:53.674521170+08:00" level=info msg="StartContainer for \"6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23\" returns successfully"
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:11:54 node-9.domain.tld coaster-startup-checker[42754]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]: Start to check OSD by-path after node start up.
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]: Node-9 is ready in EOS.
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]: Traceback (most recent call last):
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]:     sys.exit(main())
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:11:55 node-9.domain.tld coaster-startup-checker[42754]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:11:55 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:11:55 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:11:57 node-9.domain.tld kubelet[4781]: I0402 11:11:57.815467    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:11:57 node-9.domain.tld kubelet[4781]: E0402 11:11:57.816099    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:12:05 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:12:05 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 6.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:12:05 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:12:05 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:06 node-9.domain.tld coaster-startup-checker[45759]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]: Start to check OSD by-path after node start up.
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]: Node-9 is ready in EOS.
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]: Traceback (most recent call last):
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]:     sys.exit(main())
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:12:07 node-9.domain.tld coaster-startup-checker[45759]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:12:07 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:12:07 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.712937    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: c9bcd608830736e0e7993641d90ed864ecda6a94a0a877df3e3d1d1bd3901400
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.714215854+08:00" level=info msg="RemoveContainer for \"c9bcd608830736e0e7993641d90ed864ecda6a94a0a877df3e3d1d1bd3901400\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.723938341+08:00" level=info msg="RemoveContainer for \"c9bcd608830736e0e7993641d90ed864ecda6a94a0a877df3e3d1d1bd3901400\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.724192    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 092b1b2504bce39b57cd8a63cd8699436a1a3e76f6d91d26a58597b37087010d
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.725759450+08:00" level=info msg="RemoveContainer for \"092b1b2504bce39b57cd8a63cd8699436a1a3e76f6d91d26a58597b37087010d\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.734173727+08:00" level=info msg="RemoveContainer for \"092b1b2504bce39b57cd8a63cd8699436a1a3e76f6d91d26a58597b37087010d\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.734381    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: dbf3cc8aee249a0b850398bb5ef5fc00589ece120543eae9a3ecf217399fc7c1
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.735595021+08:00" level=info msg="RemoveContainer for \"dbf3cc8aee249a0b850398bb5ef5fc00589ece120543eae9a3ecf217399fc7c1\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.744812980+08:00" level=info msg="RemoveContainer for \"dbf3cc8aee249a0b850398bb5ef5fc00589ece120543eae9a3ecf217399fc7c1\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.745027    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: e76020af15ef089ab612ab9fc9215859ac4d16d8b59ebf9d0469b5eb2aa203c6
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.746057066+08:00" level=info msg="RemoveContainer for \"e76020af15ef089ab612ab9fc9215859ac4d16d8b59ebf9d0469b5eb2aa203c6\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.751989925+08:00" level=info msg="RemoveContainer for \"e76020af15ef089ab612ab9fc9215859ac4d16d8b59ebf9d0469b5eb2aa203c6\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.752192    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d0066197d436e24e8fd66ee69e73e7346b2bfad036f276a8df201c0936454799
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.753133970+08:00" level=info msg="RemoveContainer for \"d0066197d436e24e8fd66ee69e73e7346b2bfad036f276a8df201c0936454799\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.762189380+08:00" level=info msg="RemoveContainer for \"d0066197d436e24e8fd66ee69e73e7346b2bfad036f276a8df201c0936454799\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.762400    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d8b91bfa949cd3989d7c55e95365e48c9a3c24257eecf4511febb8ef824fc7b2
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.763350848+08:00" level=info msg="RemoveContainer for \"d8b91bfa949cd3989d7c55e95365e48c9a3c24257eecf4511febb8ef824fc7b2\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.770817191+08:00" level=info msg="RemoveContainer for \"d8b91bfa949cd3989d7c55e95365e48c9a3c24257eecf4511febb8ef824fc7b2\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.771012    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: b4d3f70134663f5bc070e327ad9fcc97d2d7d856674fae2f1f906339cd393264
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.772027987+08:00" level=info msg="RemoveContainer for \"b4d3f70134663f5bc070e327ad9fcc97d2d7d856674fae2f1f906339cd393264\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.778463676+08:00" level=info msg="RemoveContainer for \"b4d3f70134663f5bc070e327ad9fcc97d2d7d856674fae2f1f906339cd393264\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.778658    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: e31e7b04599a3f46baa4c370f1b770c07e76da36a35d9e27f7b942f66e18cb3d
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.779408960+08:00" level=info msg="RemoveContainer for \"e31e7b04599a3f46baa4c370f1b770c07e76da36a35d9e27f7b942f66e18cb3d\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.784244503+08:00" level=info msg="RemoveContainer for \"e31e7b04599a3f46baa4c370f1b770c07e76da36a35d9e27f7b942f66e18cb3d\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.784469    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: f8226205ea0b5d381cae8d3e6d2753774c0bdac5082f3acbe9d16351a598255c
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.785669040+08:00" level=info msg="RemoveContainer for \"f8226205ea0b5d381cae8d3e6d2753774c0bdac5082f3acbe9d16351a598255c\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.790387603+08:00" level=info msg="RemoveContainer for \"f8226205ea0b5d381cae8d3e6d2753774c0bdac5082f3acbe9d16351a598255c\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.790608    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: f4ebec9fa110f941e87d599b9df0584bc217928b5148e686332ace8458f9549d
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.791759585+08:00" level=info msg="RemoveContainer for \"f4ebec9fa110f941e87d599b9df0584bc217928b5148e686332ace8458f9549d\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.802472603+08:00" level=info msg="RemoveContainer for \"f4ebec9fa110f941e87d599b9df0584bc217928b5148e686332ace8458f9549d\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.802960    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 4b0ed189bf5dfbd318bd316a7602b26332ee6d89111beeb21a05f07bd31ae5c9
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.806801647+08:00" level=info msg="RemoveContainer for \"4b0ed189bf5dfbd318bd316a7602b26332ee6d89111beeb21a05f07bd31ae5c9\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.824351454+08:00" level=info msg="RemoveContainer for \"4b0ed189bf5dfbd318bd316a7602b26332ee6d89111beeb21a05f07bd31ae5c9\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld kubelet[4781]: I0402 11:12:09.824562    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 21f404f005f9357190dcee8b119189bcf76df302816901ea8e7bac916e010ac6
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.825710866+08:00" level=info msg="RemoveContainer for \"21f404f005f9357190dcee8b119189bcf76df302816901ea8e7bac916e010ac6\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.851066475+08:00" level=info msg="RemoveContainer for \"21f404f005f9357190dcee8b119189bcf76df302816901ea8e7bac916e010ac6\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.853034905+08:00" level=info msg="StopPodSandbox for \"21cf4877a78a0e7b23b59b3d5f43b268d770d1e08cae40cd881720d71b9bf659\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.853278927+08:00" level=info msg="TearDown network for sandbox \"21cf4877a78a0e7b23b59b3d5f43b268d770d1e08cae40cd881720d71b9bf659\" successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.853312213+08:00" level=info msg="StopPodSandbox for \"21cf4877a78a0e7b23b59b3d5f43b268d770d1e08cae40cd881720d71b9bf659\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.853643319+08:00" level=info msg="RemovePodSandbox for \"21cf4877a78a0e7b23b59b3d5f43b268d770d1e08cae40cd881720d71b9bf659\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.863715374+08:00" level=info msg="RemovePodSandbox \"21cf4877a78a0e7b23b59b3d5f43b268d770d1e08cae40cd881720d71b9bf659\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.864023489+08:00" level=info msg="StopPodSandbox for \"46c369de0cfef66f30487fc0ca273b2a4137a63435590c9615419e1516bf4d8e\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.864236878+08:00" level=info msg="TearDown network for sandbox \"46c369de0cfef66f30487fc0ca273b2a4137a63435590c9615419e1516bf4d8e\" successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.864265310+08:00" level=info msg="StopPodSandbox for \"46c369de0cfef66f30487fc0ca273b2a4137a63435590c9615419e1516bf4d8e\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.864567696+08:00" level=info msg="RemovePodSandbox for \"46c369de0cfef66f30487fc0ca273b2a4137a63435590c9615419e1516bf4d8e\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.874003247+08:00" level=info msg="RemovePodSandbox \"46c369de0cfef66f30487fc0ca273b2a4137a63435590c9615419e1516bf4d8e\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.874333749+08:00" level=info msg="StopPodSandbox for \"6096f6c8972a415a61209197572cbb67a73d74a11fb51a61bdb9e6e869676fe6\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.919718553+08:00" level=info msg="TearDown network for sandbox \"6096f6c8972a415a61209197572cbb67a73d74a11fb51a61bdb9e6e869676fe6\" successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.919852369+08:00" level=info msg="StopPodSandbox for \"6096f6c8972a415a61209197572cbb67a73d74a11fb51a61bdb9e6e869676fe6\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.920319434+08:00" level=info msg="RemovePodSandbox for \"6096f6c8972a415a61209197572cbb67a73d74a11fb51a61bdb9e6e869676fe6\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.929927671+08:00" level=info msg="RemovePodSandbox \"6096f6c8972a415a61209197572cbb67a73d74a11fb51a61bdb9e6e869676fe6\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.930301412+08:00" level=info msg="StopPodSandbox for \"89bb258cbebc7e7dac3128e4f39e7d98fa8651001fc7b4de7678a84288139d7b\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.930532358+08:00" level=info msg="TearDown network for sandbox \"89bb258cbebc7e7dac3128e4f39e7d98fa8651001fc7b4de7678a84288139d7b\" successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.930562234+08:00" level=info msg="StopPodSandbox for \"89bb258cbebc7e7dac3128e4f39e7d98fa8651001fc7b4de7678a84288139d7b\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.931015495+08:00" level=info msg="RemovePodSandbox for \"89bb258cbebc7e7dac3128e4f39e7d98fa8651001fc7b4de7678a84288139d7b\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.940118367+08:00" level=info msg="RemovePodSandbox \"89bb258cbebc7e7dac3128e4f39e7d98fa8651001fc7b4de7678a84288139d7b\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.940483901+08:00" level=info msg="StopPodSandbox for \"7d8fe50352278acebd691c918af9a09113c7a391651dd28a021d85a0d0ab54f0\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.987190839+08:00" level=info msg="TearDown network for sandbox \"7d8fe50352278acebd691c918af9a09113c7a391651dd28a021d85a0d0ab54f0\" successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.987239513+08:00" level=info msg="StopPodSandbox for \"7d8fe50352278acebd691c918af9a09113c7a391651dd28a021d85a0d0ab54f0\" returns successfully"
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.987677599+08:00" level=info msg="RemovePodSandbox for \"7d8fe50352278acebd691c918af9a09113c7a391651dd28a021d85a0d0ab54f0\""
Apr 02 11:12:09 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:09.997829043+08:00" level=info msg="RemovePodSandbox \"7d8fe50352278acebd691c918af9a09113c7a391651dd28a021d85a0d0ab54f0\" returns successfully"
Apr 02 11:12:12 node-9.domain.tld kubelet[4781]: I0402 11:12:12.815322    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:12:12 node-9.domain.tld kubelet[4781]: E0402 11:12:12.815979    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:12:17 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:12:17 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 7.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:12:17 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:12:17 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:19 node-9.domain.tld coaster-startup-checker[48716]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]: Start to check OSD by-path after node start up.
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]: Node-9 is ready in EOS.
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]: Traceback (most recent call last):
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]:     sys.exit(main())
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:12:20 node-9.domain.tld coaster-startup-checker[48716]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:12:20 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:12:20 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:12:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:21.890333724+08:00" level=info msg="shim disconnected" id=432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434
Apr 02 11:12:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:21.890444665+08:00" level=warning msg="cleaning up after shim disconnected" id=432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434 namespace=k8s.io
Apr 02 11:12:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:21.890471672+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:12:21 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-432dcf80fab424f3bb19a2c7739dae2a7b4650123f69442f4d47b1e8277c1434-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:12:21 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:21.917260714+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:12:21+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=49705\n"
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.542571595+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:0,}"
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.608904500+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:0,} returns container id \"124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64\""
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.609509325+08:00" level=info msg="StartContainer for \"124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64\""
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.898529954+08:00" level=info msg="StartContainer for \"124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64\" returns successfully"
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.902443329+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-controller,Attempt:0,}"
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.956571145+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-controller,Attempt:0,} returns container id \"19f4df719170c28246c23919876a74274cc0b51bcf69effdc253715c53243965\""
Apr 02 11:12:22 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:22.957066763+08:00" level=info msg="StartContainer for \"19f4df719170c28246c23919876a74274cc0b51bcf69effdc253715c53243965\""
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.281457297+08:00" level=info msg="StartContainer for \"19f4df719170c28246c23919876a74274cc0b51bcf69effdc253715c53243965\" returns successfully"
Apr 02 11:12:23 node-9.domain.tld kubelet[4781]: E0402 11:12:23.300274    4781 cadvisor_stats_provider.go:401] Partial failure issuing cadvisor.ContainerInfoV2: partial failures: ["/kubepods/burstable/podd56ec515-f683-49ae-af58-15c5ec1b769b/19f4df719170c28246c23919876a74274cc0b51bcf69effdc253715c53243965": RecentStats: unable to find data in memory cache]
Apr 02 11:12:23 node-9.domain.tld kubelet[4781]: E0402 11:12:23.300349    4781 cadvisor_stats_provider.go:401] Partial failure issuing cadvisor.ContainerInfoV2: partial failures: ["/kubepods/burstable/podd56ec515-f683-49ae-af58-15c5ec1b769b/19f4df719170c28246c23919876a74274cc0b51bcf69effdc253715c53243965": RecentStats: unable to find data in memory cache]
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.374617893+08:00" level=info msg="shim disconnected" id=124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.374740298+08:00" level=warning msg="cleaning up after shim disconnected" id=124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64 namespace=k8s.io
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.374767352+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.402148044+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:12:23+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=50217\n"
Apr 02 11:12:23 node-9.domain.tld kubelet[4781]: I0402 11:12:23.545183    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.549368330+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:1,}"
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.615808267+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:1,} returns container id \"afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7\""
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.616396352+08:00" level=info msg="StartContainer for \"afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7\""
Apr 02 11:12:23 node-9.domain.tld kubelet[4781]: I0402 11:12:23.815516    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:12:23 node-9.domain.tld kubelet[4781]: E0402 11:12:23.816124    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:12:23 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:12:23 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:23.920256120+08:00" level=info msg="StartContainer for \"afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7\" returns successfully"
Apr 02 11:12:24 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:12:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:24.151224387+08:00" level=info msg="shim disconnected" id=afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7
Apr 02 11:12:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:24.151325715+08:00" level=warning msg="cleaning up after shim disconnected" id=afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7 namespace=k8s.io
Apr 02 11:12:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:24.151355039+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:12:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:24.178186157+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:12:24+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=50512\n"
Apr 02 11:12:24 node-9.domain.tld kubelet[4781]: I0402 11:12:24.551067    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64
Apr 02 11:12:24 node-9.domain.tld kubelet[4781]: I0402 11:12:24.551913    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7
Apr 02 11:12:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:24.553057011+08:00" level=info msg="RemoveContainer for \"124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64\""
Apr 02 11:12:24 node-9.domain.tld kubelet[4781]: E0402 11:12:24.553451    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 10s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:12:24 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:24.561174217+08:00" level=info msg="RemoveContainer for \"124f75057b074b6e0a039f7fa2794ed2469362b45772cdee19c18b799c38ff64\" returns successfully"
Apr 02 11:12:25 node-9.domain.tld kubelet[4781]: I0402 11:12:25.557889    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7
Apr 02 11:12:25 node-9.domain.tld kubelet[4781]: E0402 11:12:25.558969    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 10s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:12:30 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:12:30 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 8.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:12:30 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:12:30 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:31 node-9.domain.tld coaster-startup-checker[52022]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]: Start to check OSD by-path after node start up.
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]: Node-9 is ready in EOS.
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]: Traceback (most recent call last):
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]:     sys.exit(main())
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:12:33 node-9.domain.tld coaster-startup-checker[52022]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:12:33 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:12:33 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:12:37 node-9.domain.tld kubelet[4781]: I0402 11:12:37.816088    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7
Apr 02 11:12:37 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:37.821778056+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:2,}"
Apr 02 11:12:37 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:37.887980791+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:2,} returns container id \"2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb\""
Apr 02 11:12:37 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:37.888526319+08:00" level=info msg="StartContainer for \"2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb\""
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.140960644+08:00" level=info msg="StartContainer for \"2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb\" returns successfully"
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.358853559+08:00" level=info msg="shim disconnected" id=2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.358970322+08:00" level=warning msg="cleaning up after shim disconnected" id=2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb namespace=k8s.io
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.358997664+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.386214257+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:12:38+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=54287\n"
Apr 02 11:12:38 node-9.domain.tld kubelet[4781]: I0402 11:12:38.605331    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7
Apr 02 11:12:38 node-9.domain.tld kubelet[4781]: I0402 11:12:38.606057    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.606815256+08:00" level=info msg="RemoveContainer for \"afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7\""
Apr 02 11:12:38 node-9.domain.tld kubelet[4781]: E0402 11:12:38.607139    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:12:38 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:38.616494647+08:00" level=info msg="RemoveContainer for \"afb61e366b8a46a08538f27ab807ed2b7ff9d01a5d3b331800c968ed4415c2c7\" returns successfully"
Apr 02 11:12:38 node-9.domain.tld kubelet[4781]: I0402 11:12:38.815336    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:12:38 node-9.domain.tld kubelet[4781]: E0402 11:12:38.815901    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:12:40 node-9.domain.tld ovs-vswitchd[3123]: ovs|00441|rconn|ERR|br-int<->unix#41: no response to inactivity probe after 60 seconds, disconnecting
Apr 02 11:12:43 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:12:43 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 9.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:12:43 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:12:43 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:44 node-9.domain.tld coaster-startup-checker[55411]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]: Start to check OSD by-path after node start up.
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]: Node-9 is ready in EOS.
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]: Traceback (most recent call last):
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]:     sys.exit(main())
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:12:45 node-9.domain.tld coaster-startup-checker[55411]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:12:45 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:12:45 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:12:48 node-9.domain.tld kubelet[4781]: I0402 11:12:48.815956    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb
Apr 02 11:12:48 node-9.domain.tld kubelet[4781]: E0402 11:12:48.817042    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:12:52 node-9.domain.tld kubelet[4781]: I0402 11:12:52.815310    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:12:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:52.818781738+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for container &ContainerMetadata{Name:qgpu-exporter,Attempt:4,}"
Apr 02 11:12:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:52.881758704+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for &ContainerMetadata{Name:qgpu-exporter,Attempt:4,} returns container id \"af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e\""
Apr 02 11:12:52 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:52.882309990+08:00" level=info msg="StartContainer for \"af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e\""
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.055932923+08:00" level=info msg="StartContainer for \"af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e\" returns successfully"
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.180104196+08:00" level=info msg="shim disconnected" id=af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.180209611+08:00" level=warning msg="cleaning up after shim disconnected" id=af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e namespace=k8s.io
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.180238161+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.206852475+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:12:53+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=1464\n"
Apr 02 11:12:53 node-9.domain.tld kubelet[4781]: I0402 11:12:53.662218    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4
Apr 02 11:12:53 node-9.domain.tld kubelet[4781]: I0402 11:12:53.662779    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:12:53 node-9.domain.tld kubelet[4781]: E0402 11:12:53.663331    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.663889403+08:00" level=info msg="RemoveContainer for \"3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4\""
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.674063572+08:00" level=info msg="RemoveContainer for \"3e701b1ddb5a0dad18b08fe6a3f47f8189922a1e477c89c855b70ffe2e2fccf4\" returns successfully"
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.792326840+08:00" level=info msg="shim disconnected" id=6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.792434677+08:00" level=warning msg="cleaning up after shim disconnected" id=6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23 namespace=k8s.io
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.792461562+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:12:53 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:12:53 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:53.819750402+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:12:53+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2322\n"
Apr 02 11:12:54 node-9.domain.tld kubelet[4781]: I0402 11:12:54.668658    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3
Apr 02 11:12:54 node-9.domain.tld kubelet[4781]: I0402 11:12:54.669228    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23
Apr 02 11:12:54 node-9.domain.tld kubelet[4781]: E0402 11:12:54.670084    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 10s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:12:54 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:54.670232916+08:00" level=info msg="RemoveContainer for \"14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3\""
Apr 02 11:12:54 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:54.680507855+08:00" level=info msg="RemoveContainer for \"14900b920689f1be0e0132adb4a9ad607e235ec7250dcc3004e3f0f2a0b833f3\" returns successfully"
Apr 02 11:12:56 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:12:56 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 10.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:12:56 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:12:56 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:12:57 node-9.domain.tld coaster-startup-checker[5414]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]: Start to check OSD by-path after node start up.
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]: Node-9 is ready in EOS.
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]: Traceback (most recent call last):
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]:     sys.exit(main())
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:12:58 node-9.domain.tld coaster-startup-checker[5414]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:12:58 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:12:58 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:12:59 node-9.domain.tld kubelet[4781]: I0402 11:12:59.818076    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb
Apr 02 11:12:59 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:59.822852944+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:3,}"
Apr 02 11:12:59 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:59.883398654+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:3,} returns container id \"d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6\""
Apr 02 11:12:59 node-9.domain.tld containerd[28543]: time="2024-04-02T11:12:59.883990408+08:00" level=info msg="StartContainer for \"d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6\""
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.160578209+08:00" level=info msg="StartContainer for \"d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6\" returns successfully"
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.368205883+08:00" level=info msg="shim disconnected" id=d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.368318491+08:00" level=warning msg="cleaning up after shim disconnected" id=d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6 namespace=k8s.io
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.368349541+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.395069255+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:13:00+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=11119\n"
Apr 02 11:13:00 node-9.domain.tld kubelet[4781]: I0402 11:13:00.705468    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb
Apr 02 11:13:00 node-9.domain.tld kubelet[4781]: I0402 11:13:00.706288    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.706857740+08:00" level=info msg="RemoveContainer for \"2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb\""
Apr 02 11:13:00 node-9.domain.tld kubelet[4781]: E0402 11:13:00.707397    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:13:00 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:00.716345487+08:00" level=info msg="RemoveContainer for \"2193cbc8445854d3e5aafdec74f3d5042e7c7ce0d23b3d3384f21f95656b7efb\" returns successfully"
Apr 02 11:13:00 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:13:06 node-9.domain.tld kubelet[4781]: I0402 11:13:06.815255    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23
Apr 02 11:13:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:06.819559407+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for container &ContainerMetadata{Name:worker,Attempt:2,}"
Apr 02 11:13:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:06.864495856+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for &ContainerMetadata{Name:worker,Attempt:2,} returns container id \"165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210\""
Apr 02 11:13:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:06.865011125+08:00" level=info msg="StartContainer for \"165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210\""
Apr 02 11:13:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:07.084674652+08:00" level=info msg="StartContainer for \"165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210\" returns successfully"
Apr 02 11:13:08 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:13:08 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 11.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:13:08 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:13:08 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:13:08 node-9.domain.tld kubelet[4781]: I0402 11:13:08.816044    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:13:08 node-9.domain.tld kubelet[4781]: E0402 11:13:08.816603    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:10 node-9.domain.tld coaster-startup-checker[20406]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:13:10 node-9.domain.tld systemd[1]: man-db-cache-update.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit man-db-cache-update.service has successfully entered the 'dead' state.
Apr 02 11:13:10 node-9.domain.tld systemd[1]: Started man-db-cache-update.service.
-- Subject: Unit man-db-cache-update.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit man-db-cache-update.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:13:10 node-9.domain.tld systemd[1]: run-r573e411908e946edab0dfc51b47915e0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-r573e411908e946edab0dfc51b47915e0.service has successfully entered the 'dead' state.
Apr 02 11:13:10 node-9.domain.tld systemd[1]: run-r5a4f812a675c423d80600db5baa704ae.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-r5a4f812a675c423d80600db5baa704ae.service has successfully entered the 'dead' state.
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]: Start to check OSD by-path after node start up.
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]: Node-9 is ready in EOS.
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]: Traceback (most recent call last):
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]:     sys.exit(main())
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:13:11 node-9.domain.tld coaster-startup-checker[20406]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:13:11 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:13:11 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:13:13 node-9.domain.tld kubelet[4781]: I0402 11:13:13.815736    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:13 node-9.domain.tld kubelet[4781]: E0402 11:13:13.816888    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:13:21 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:13:21 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 12.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:13:21 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:13:21 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kubelet[4781]: I0402 11:13:22.815271    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:13:22 node-9.domain.tld kubelet[4781]: E0402 11:13:22.815891    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:22 node-9.domain.tld coaster-startup-checker[22773]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]: Start to check OSD by-path after node start up.
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]: Node-9 is ready in EOS.
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]: Traceback (most recent call last):
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]:     sys.exit(main())
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:13:24 node-9.domain.tld coaster-startup-checker[22773]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:13:24 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:13:24 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:13:24 node-9.domain.tld kubelet[4781]: I0402 11:13:24.815477    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:24 node-9.domain.tld kubelet[4781]: E0402 11:13:24.816606    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:13:34 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:13:34 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 13.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:13:34 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:13:34 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:13:34 node-9.domain.tld systemd[1]: Starting titanagent check exception...
-- Subject: Unit titanagent_check_exception.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has begun starting up.
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:35 node-9.domain.tld coaster-startup-checker[23446]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]: Start to check OSD by-path after node start up.
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]: Node-9 is ready in EOS.
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]: Traceback (most recent call last):
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]:     sys.exit(main())
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:13:36 node-9.domain.tld coaster-startup-checker[23446]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:13:36 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:13:36 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:13:36 node-9.domain.tld kubelet[4781]: I0402 11:13:36.815289    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:13:36 node-9.domain.tld kubelet[4781]: E0402 11:13:36.815925    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:13:37 node-9.domain.tld kubelet[4781]: I0402 11:13:37.815400    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:37 node-9.domain.tld kubelet[4781]: E0402 11:13:37.816430    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:13:38 node-9.domain.tld systemd[1]: titanagent_check_exception.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check_exception.service has successfully entered the 'dead' state.
Apr 02 11:13:38 node-9.domain.tld systemd[1]: Started titanagent check exception.
-- Subject: Unit titanagent_check_exception.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:13:46 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:13:46 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 14.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:13:46 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:13:46 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:13:48 node-9.domain.tld coaster-startup-checker[24518]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]: Start to check OSD by-path after node start up.
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]: Node-9 is ready in EOS.
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]: Traceback (most recent call last):
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]:     sys.exit(main())
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:13:49 node-9.domain.tld coaster-startup-checker[24518]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:13:49 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:13:49 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:13:49 node-9.domain.tld kubelet[4781]: I0402 11:13:49.817796    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:13:49 node-9.domain.tld kubelet[4781]: E0402 11:13:49.818413    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:13:50 node-9.domain.tld kubelet[4781]: I0402 11:13:50.815322    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:50 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:50.819982849+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:4,}"
Apr 02 11:13:50 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:50.867051852+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:4,} returns container id \"11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422\""
Apr 02 11:13:50 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:50.867537517+08:00" level=info msg="StartContainer for \"11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422\""
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.031637305+08:00" level=info msg="StartContainer for \"11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422\" returns successfully"
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.325216848+08:00" level=info msg="shim disconnected" id=11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.325333216+08:00" level=warning msg="cleaning up after shim disconnected" id=11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422 namespace=k8s.io
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.325363182+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.353094691+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:13:51+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=24752\n"
Apr 02 11:13:51 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:13:51 node-9.domain.tld kubelet[4781]: I0402 11:13:51.882228    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6
Apr 02 11:13:51 node-9.domain.tld kubelet[4781]: I0402 11:13:51.882846    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.883554929+08:00" level=info msg="RemoveContainer for \"d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6\""
Apr 02 11:13:51 node-9.domain.tld kubelet[4781]: E0402 11:13:51.883934    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:13:51 node-9.domain.tld containerd[28543]: time="2024-04-02T11:13:51.893318492+08:00" level=info msg="RemoveContainer for \"d4ca3d5d6254f5dc2debe777e9a28c9e11732a8b147810baf9a9836a9ee7c9a6\" returns successfully"
Apr 02 11:13:59 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:13:59 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 15.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:13:59 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:13:59 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:00 node-9.domain.tld coaster-startup-checker[24990]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]: Start to check OSD by-path after node start up.
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]: Node-9 is ready in EOS.
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]: Traceback (most recent call last):
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]:     sys.exit(main())
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:14:01 node-9.domain.tld coaster-startup-checker[24990]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:14:01 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:14:01 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:14:01 node-9.domain.tld kubelet[4781]: I0402 11:14:01.815299    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:14:01 node-9.domain.tld kubelet[4781]: E0402 11:14:01.815874    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:14:05 node-9.domain.tld kubelet[4781]: I0402 11:14:05.815442    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:14:05 node-9.domain.tld kubelet[4781]: E0402 11:14:05.816546    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:14:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:07.195810376+08:00" level=info msg="shim disconnected" id=165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210
Apr 02 11:14:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:07.195925183+08:00" level=warning msg="cleaning up after shim disconnected" id=165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210 namespace=k8s.io
Apr 02 11:14:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:07.195951348+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:14:07 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:14:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:07.223685118+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:14:07+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=25291\n"
Apr 02 11:14:07 node-9.domain.tld kubelet[4781]: I0402 11:14:07.934596    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23
Apr 02 11:14:07 node-9.domain.tld kubelet[4781]: I0402 11:14:07.935190    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210
Apr 02 11:14:07 node-9.domain.tld kubelet[4781]: E0402 11:14:07.936026    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 20s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:14:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:07.936013211+08:00" level=info msg="RemoveContainer for \"6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23\""
Apr 02 11:14:07 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:07.945634292+08:00" level=info msg="RemoveContainer for \"6298234ffc4f4bd8fc10dc24c1a1187a8fda18e5cb7da894fe42be52b8d2fa23\" returns successfully"
Apr 02 11:14:11 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:14:11 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 16.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:14:11 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:14:11 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:14:12 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:12 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:13 node-9.domain.tld coaster-startup-checker[25416]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]: Start to check OSD by-path after node start up.
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]: Node-9 is ready in EOS.
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]: Traceback (most recent call last):
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]:     sys.exit(main())
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:14:14 node-9.domain.tld coaster-startup-checker[25416]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:14:14 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:14:14 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:14:15 node-9.domain.tld kubelet[4781]: I0402 11:14:15.815287    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:14:15 node-9.domain.tld kubelet[4781]: E0402 11:14:15.815881    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:14:18 node-9.domain.tld kubelet[4781]: I0402 11:14:18.815501    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210
Apr 02 11:14:18 node-9.domain.tld kubelet[4781]: I0402 11:14:18.815673    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:14:18 node-9.domain.tld kubelet[4781]: E0402 11:14:18.816366    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 20s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:14:18 node-9.domain.tld kubelet[4781]: E0402 11:14:18.816777    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:14:20 node-9.domain.tld systemd[1]: Starting titanagent check exception...
-- Subject: Unit titanagent_check_exception.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has begun starting up.
Apr 02 11:14:24 node-9.domain.tld systemd[1]: titanagent_check_exception.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check_exception.service has successfully entered the 'dead' state.
Apr 02 11:14:24 node-9.domain.tld systemd[1]: Started titanagent check exception.
-- Subject: Unit titanagent_check_exception.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:14:24 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:14:24 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 17.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:14:24 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:14:24 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:25 node-9.domain.tld coaster-startup-checker[26043]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]: Start to check OSD by-path after node start up.
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]: Node-9 is ready in EOS.
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]: Traceback (most recent call last):
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]:     sys.exit(main())
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:14:26 node-9.domain.tld coaster-startup-checker[26043]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:14:26 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:14:26 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:14:26 node-9.domain.tld kubelet[4781]: I0402 11:14:26.815607    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:14:26 node-9.domain.tld kubelet[4781]: E0402 11:14:26.816225    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:14:29 node-9.domain.tld kubelet[4781]: I0402 11:14:29.817749    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:14:29 node-9.domain.tld kubelet[4781]: E0402 11:14:29.818809    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:14:33 node-9.domain.tld kubelet[4781]: I0402 11:14:33.815357    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210
Apr 02 11:14:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:33.819808906+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for container &ContainerMetadata{Name:worker,Attempt:3,}"
Apr 02 11:14:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:33.873873085+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for &ContainerMetadata{Name:worker,Attempt:3,} returns container id \"70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49\""
Apr 02 11:14:33 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:33.874537754+08:00" level=info msg="StartContainer for \"70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49\""
Apr 02 11:14:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:14:34.047926244+08:00" level=info msg="StartContainer for \"70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49\" returns successfully"
Apr 02 11:14:36 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:14:36 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 18.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:14:36 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:14:36 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:38 node-9.domain.tld coaster-startup-checker[26744]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]: Start to check OSD by-path after node start up.
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]: Node-9 is ready in EOS.
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]: Traceback (most recent call last):
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]:     sys.exit(main())
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:14:39 node-9.domain.tld coaster-startup-checker[26744]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:14:39 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:14:39 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:14:41 node-9.domain.tld kubelet[4781]: I0402 11:14:41.815257    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:14:41 node-9.domain.tld kubelet[4781]: E0402 11:14:41.815840    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:14:44 node-9.domain.tld kubelet[4781]: I0402 11:14:44.820535    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:14:44 node-9.domain.tld kubelet[4781]: E0402 11:14:44.823380    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:14:49 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:14:49 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 19.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:14:49 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:14:49 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:14:50 node-9.domain.tld coaster-startup-checker[27567]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:14:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:51.222 4602 WARNING coaster_agent.iface [-] Get nic slot: 00:14.0-1.6 device info with exception: Command 'timeout 5s /usr/sbin/lspci|grep '00:14.0-1.6'' returned non-zero exit status 1
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]: Start to check OSD by-path after node start up.
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]: Node-9 is ready in EOS.
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]: Traceback (most recent call last):
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]:     sys.exit(main())
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:14:51 node-9.domain.tld coaster-startup-checker[27567]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:14:51 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:14:51 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:14:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:52.334 4602 WARNING coaster_agent.iface [-] Get nic slot: 00:14.0-1.6 device info with exception: Command 'timeout 5s /usr/sbin/lspci|grep '00:14.0-1.6'' returned non-zero exit status 1
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:14:53 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:14:53.811 4602 ERROR coaster_agent.agent
Apr 02 11:14:55 node-9.domain.tld kubelet[4781]: I0402 11:14:55.815307    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:14:55 node-9.domain.tld kubelet[4781]: E0402 11:14:55.815886    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:14:59 node-9.domain.tld kubelet[4781]: I0402 11:14:59.817713    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:14:59 node-9.domain.tld kubelet[4781]: E0402 11:14:59.818888    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 1m20s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Started Session 61 of user root.
-- Subject: Unit session-61.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-61.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 20.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Startup finished in 130ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 130553 microseconds.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld CROND[28495]: (root) CMD (flock -xn /tmp/drop_cache.lock /usr/bin/drop_cache.sh >> /var/log/drop_cache.log)
Apr 02 11:15:01 node-9.domain.tld systemd[1]: session-61.scope: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit session-61.scope has successfully entered the 'dead' state.
Apr 02 11:15:01 node-9.domain.tld systemd[1]: Stopping User Manager for UID 0...
-- Subject: Unit user@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Stopped target Default.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Stopped target Basic System.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Stopped target Timers.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Stopped target Paths.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Stopped target Sockets.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: dbus.socket: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Closed D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished shutting down.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Reached target Shutdown.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:01 node-9.domain.tld systemd[28475]: Starting Exit the Session...
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: user@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user@0.service has successfully entered the 'dead' state.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: Stopped User Manager for UID 0.
-- Subject: Unit user@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished shutting down.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: Stopping /run/user/0 mount wrapper...
-- Subject: Unit user-runtime-dir@0.service has begun shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has begun shutting down.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: Removed slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished shutting down.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: run-user-0.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-user-0.mount has successfully entered the 'dead' state.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: user-runtime-dir@0.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit user-runtime-dir@0.service has successfully entered the 'dead' state.
Apr 02 11:15:02 node-9.domain.tld systemd[1]: Stopped /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished shutting down.
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:03 node-9.domain.tld coaster-startup-checker[28491]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]: Start to check OSD by-path after node start up.
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]: Node-9 is ready in EOS.
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]: Traceback (most recent call last):
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]:     sys.exit(main())
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:04 node-9.domain.tld coaster-startup-checker[28491]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:04 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:15:04 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:15:08 node-9.domain.tld kubelet[4781]: I0402 11:15:08.815539    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:15:08 node-9.domain.tld kubelet[4781]: E0402 11:15:08.816137    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:15:09 node-9.domain.tld kubelet[4781]: E0402 11:15:09.786905    4781 info.go:99] Failed to get disk map: open /sys/block/nvme4c4n1/dev: no such file or directory
Apr 02 11:15:11 node-9.domain.tld kubelet[4781]: E0402 11:15:11.645642    4781 manager.go:544] unexpected: unhealthyDevices and endpoints are out of sync
Apr 02 11:15:11 node-9.domain.tld kubelet[4781]: E0402 11:15:11.645686    4781 manager.go:544] unexpected: unhealthyDevices and endpoints are out of sync
Apr 02 11:15:13 node-9.domain.tld systemd[1]: Starting titanagent punch card...
-- Subject: Unit titanagent_check.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check.service has begun starting up.
Apr 02 11:15:13 node-9.domain.tld kubelet[4781]: I0402 11:15:13.815735    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:15:13 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:13.820508694+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:5,}"
Apr 02 11:15:13 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:13.876909653+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:5,} returns container id \"065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942\""
Apr 02 11:15:13 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:13.877464109+08:00" level=info msg="StartContainer for \"065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942\""
Apr 02 11:15:13 node-9.domain.tld systemd[1]: titanagent_check.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check.service has successfully entered the 'dead' state.
Apr 02 11:15:13 node-9.domain.tld systemd[1]: Started titanagent punch card.
-- Subject: Unit titanagent_check.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:14.035352809+08:00" level=info msg="StartContainer for \"065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942\" returns successfully"
Apr 02 11:15:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:14.245212548+08:00" level=info msg="shim disconnected" id=065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:15:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:14.245300716+08:00" level=warning msg="cleaning up after shim disconnected" id=065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942 namespace=k8s.io
Apr 02 11:15:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:14.245328215+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:15:14 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:14.273196185+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:15:14+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=29072\n"
Apr 02 11:15:14 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:15:14 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 21.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:15:14 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:15:14 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:14 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:15:15 node-9.domain.tld kubelet[4781]: I0402 11:15:15.164766    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422
Apr 02 11:15:15 node-9.domain.tld kubelet[4781]: I0402 11:15:15.165331    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:15:15 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:15.166157984+08:00" level=info msg="RemoveContainer for \"11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422\""
Apr 02 11:15:15 node-9.domain.tld kubelet[4781]: E0402 11:15:15.166395    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:15:15 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:15.181085391+08:00" level=info msg="RemoveContainer for \"11f54c3199b5396f8c55e6d38294dd6cd128877b6a03247e5ddee6f3694b9422\" returns successfully"
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:15 node-9.domain.tld coaster-startup-checker[29086]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]: Start to check OSD by-path after node start up.
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]: Node-9 is ready in EOS.
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]: Traceback (most recent call last):
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]:     sys.exit(main())
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:16 node-9.domain.tld coaster-startup-checker[29086]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:16 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:15:16 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:21.902 4602 ERROR coaster_agent.agent
Apr 02 11:15:22 node-9.domain.tld kubelet[4781]: I0402 11:15:22.815236    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:15:22 node-9.domain.tld kubelet[4781]: E0402 11:15:22.815835    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:15:26 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:15:26 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 22.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:15:26 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:15:26 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:28 node-9.domain.tld coaster-startup-checker[29803]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]: Start to check OSD by-path after node start up.
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]: Node-9 is ready in EOS.
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]: Traceback (most recent call last):
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]:     sys.exit(main())
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:29 node-9.domain.tld coaster-startup-checker[29803]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:29 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:15:29 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:15:29 node-9.domain.tld kubelet[4781]: I0402 11:15:29.818197    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:15:29 node-9.domain.tld kubelet[4781]: E0402 11:15:29.819307    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:15:34 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:15:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:34.171382523+08:00" level=info msg="shim disconnected" id=70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:15:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:34.171492394+08:00" level=warning msg="cleaning up after shim disconnected" id=70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49 namespace=k8s.io
Apr 02 11:15:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:34.171518954+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:15:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:34.199488505+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:15:34+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=30114\n"
Apr 02 11:15:34 node-9.domain.tld kubelet[4781]: I0402 11:15:34.230096    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210
Apr 02 11:15:34 node-9.domain.tld kubelet[4781]: I0402 11:15:34.230650    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:15:34 node-9.domain.tld kubelet[4781]: E0402 11:15:34.231454    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 40s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:15:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:34.231531207+08:00" level=info msg="RemoveContainer for \"165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210\""
Apr 02 11:15:34 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:34.240839710+08:00" level=info msg="RemoveContainer for \"165a8df7f2d60fd30595b3f31e86797cfc1e66cbdd1856c2c907f641b38d3210\" returns successfully"
Apr 02 11:15:35 node-9.domain.tld kubelet[4781]: I0402 11:15:35.815463    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:15:35 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:35.819698931+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for container &ContainerMetadata{Name:qgpu-exporter,Attempt:5,}"
Apr 02 11:15:35 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:35.885416354+08:00" level=info msg="CreateContainer within sandbox \"3b72b52f354ec2f3c8b60da29a1e821ced715188d3afc58e7c0db50ac3a439b9\" for &ContainerMetadata{Name:qgpu-exporter,Attempt:5,} returns container id \"fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b\""
Apr 02 11:15:35 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:35.885991090+08:00" level=info msg="StartContainer for \"fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b\""
Apr 02 11:15:36 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:36.079003498+08:00" level=info msg="StartContainer for \"fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b\" returns successfully"
Apr 02 11:15:36 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:36.230734356+08:00" level=info msg="shim disconnected" id=fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:15:36 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:36.230859941+08:00" level=warning msg="cleaning up after shim disconnected" id=fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b namespace=k8s.io
Apr 02 11:15:36 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:36.230899570+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:15:36 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:36.257821485+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:15:36+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=30378\n"
Apr 02 11:15:36 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:15:37 node-9.domain.tld kubelet[4781]: I0402 11:15:37.254410    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e
Apr 02 11:15:37 node-9.domain.tld kubelet[4781]: I0402 11:15:37.255038    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:15:37 node-9.domain.tld kubelet[4781]: E0402 11:15:37.255781    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:15:37 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:37.255864099+08:00" level=info msg="RemoveContainer for \"af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e\""
Apr 02 11:15:37 node-9.domain.tld containerd[28543]: time="2024-04-02T11:15:37.265230045+08:00" level=info msg="RemoveContainer for \"af31eefb54c037517c5505b819983d3195df97c1b1be75633959addcc258d38e\" returns successfully"
Apr 02 11:15:37 node-9.domain.tld kubelet[4781]: W0402 11:15:37.450521    4781 manager.go:1176] Failed to process watch event {EventType:0 Name:/kubepods/besteffort/pod7081080d-560e-4046-91d7-1d5f2b1f8230/fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b WatchSource:0}: task fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b not found: not found
Apr 02 11:15:39 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:15:39 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 23.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:15:39 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:15:39 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:40 node-9.domain.tld coaster-startup-checker[30518]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:15:40 node-9.domain.tld kubelet[4781]: I0402 11:15:40.815414    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:15:40 node-9.domain.tld kubelet[4781]: E0402 11:15:40.816463    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]: Start to check OSD by-path after node start up.
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]: Node-9 is ready in EOS.
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]: Traceback (most recent call last):
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]:     sys.exit(main())
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:41 node-9.domain.tld coaster-startup-checker[30518]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:41 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:15:41 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:15:48 node-9.domain.tld kubelet[4781]: I0402 11:15:48.814875    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:15:48 node-9.domain.tld kubelet[4781]: E0402 11:15:48.815241    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 40s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:15:49 node-9.domain.tld kubelet[4781]: I0402 11:15:49.817451    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:15:49 node-9.domain.tld kubelet[4781]: E0402 11:15:49.817968    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:15:51 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:15:51 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 24.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:15:51 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:15:51 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:15:51 node-9.domain.tld kubelet[4781]: I0402 11:15:51.816198    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:15:51 node-9.domain.tld kubelet[4781]: E0402 11:15:51.817226    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:51 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:15:51.857 4602 ERROR coaster_agent.agent
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:52 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:53 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:53 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:15:53 node-9.domain.tld coaster-startup-checker[31424]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]: Start to check OSD by-path after node start up.
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]: Node-9 is ready in EOS.
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]: Traceback (most recent call last):
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]:     sys.exit(main())
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:15:54 node-9.domain.tld coaster-startup-checker[31424]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:15:54 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:15:54 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:15:59 node-9.domain.tld kubelet[4781]: I0402 11:15:59.817598    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:15:59 node-9.domain.tld kubelet[4781]: E0402 11:15:59.818436    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 40s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:16:02 node-9.domain.tld kubelet[4781]: I0402 11:16:02.815284    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:16:02 node-9.domain.tld kubelet[4781]: E0402 11:16:02.815916    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:16:04 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:16:04 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 25.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:16:04 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:16:04 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:05 node-9.domain.tld coaster-startup-checker[31825]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]: Start to check OSD by-path after node start up.
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]: Node-9 is ready in EOS.
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]: Traceback (most recent call last):
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]:     sys.exit(main())
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:06 node-9.domain.tld coaster-startup-checker[31825]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:06 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:16:06 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:16:06 node-9.domain.tld kubelet[4781]: I0402 11:16:06.815356    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:16:06 node-9.domain.tld kubelet[4781]: E0402 11:16:06.816446    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:16:13 node-9.domain.tld sshd[32189]: Accepted publickey for root from 192.168.11.3 port 45476 ssh2: RSA SHA256:wlNSFqGlPO/3EW7aSQ+kM0SCQWgQmfSb7W6eIDnfRzg
Apr 02 11:16:13 node-9.domain.tld systemd[1]: Started /run/user/0 mount wrapper.
-- Subject: Unit user-runtime-dir@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-runtime-dir@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[1]: Created slice User Slice of UID 0.
-- Subject: Unit user-0.slice has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user-0.slice has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[1]: Starting User Manager for UID 0...
-- Subject: Unit user@0.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has begun starting up.
Apr 02 11:16:13 node-9.domain.tld systemd-logind[2408]: New session 63 of user root.
-- Subject: A new session 63 has been created for user root
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- Documentation: https://www.freedesktop.org/wiki/Software/systemd/multiseat
-- 
-- A new session with the ID 63 has been created for the user root.
-- 
-- The leading process of the session is 32189.
Apr 02 11:16:13 node-9.domain.tld systemd[1]: Started Session 63 of user root.
-- Subject: Unit session-63.scope has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit session-63.scope has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: pam_unix(systemd-user:session): session opened for user root by (uid=0)
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Starting D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has begun starting up.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Reached target Timers.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Reached target Paths.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Listening on D-Bus User Message Bus Socket.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Reached target Sockets.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Reached target Basic System.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Reached target Default.
-- Subject: Unit UNIT has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit UNIT has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld systemd[32192]: Startup finished in 125ms.
-- Subject: User manager start-up is now complete
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The user manager instance for user 0 has been started. All services queued
-- for starting have been started. Note that other services might still be starting
-- up or be started at any later time.
-- 
-- Startup of the manager took 125107 microseconds.
Apr 02 11:16:13 node-9.domain.tld systemd[1]: Started User Manager for UID 0.
-- Subject: Unit user@0.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit user@0.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:13 node-9.domain.tld sshd[32189]: pam_unix(sshd:session): session opened for user root by (uid=0)
Apr 02 11:16:13 node-9.domain.tld kubelet[4781]: I0402 11:16:13.815360    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:16:13 node-9.domain.tld kubelet[4781]: E0402 11:16:13.816165    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 40s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:16:16 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:16:16 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 26.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:16:16 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:16:16 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:17 node-9.domain.tld kubelet[4781]: I0402 11:16:17.815059    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:16:17 node-9.domain.tld kubelet[4781]: E0402 11:16:17.815383    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:18 node-9.domain.tld coaster-startup-checker[32338]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]: Start to check OSD by-path after node start up.
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]: Node-9 is ready in EOS.
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]: Traceback (most recent call last):
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]:     sys.exit(main())
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:19 node-9.domain.tld coaster-startup-checker[32338]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:19 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:16:19 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:16:20 node-9.domain.tld kubelet[4781]: I0402 11:16:20.815550    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:16:20 node-9.domain.tld kubelet[4781]: E0402 11:16:20.816235    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:21 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:21.934 4602 ERROR coaster_agent.agent
Apr 02 11:16:22 node-9.domain.tld systemd[1]: Starting titanagent check exception...
-- Subject: Unit titanagent_check_exception.service has begun start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has begun starting up.
Apr 02 11:16:26 node-9.domain.tld systemd[1]: titanagent_check_exception.service: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit titanagent_check_exception.service has successfully entered the 'dead' state.
Apr 02 11:16:26 node-9.domain.tld systemd[1]: Started titanagent check exception.
-- Subject: Unit titanagent_check_exception.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit titanagent_check_exception.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:27 node-9.domain.tld kubelet[4781]: I0402 11:16:27.815356    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:16:27 node-9.domain.tld containerd[28543]: time="2024-04-02T11:16:27.820004305+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for container &ContainerMetadata{Name:worker,Attempt:4,}"
Apr 02 11:16:27 node-9.domain.tld containerd[28543]: time="2024-04-02T11:16:27.874628406+08:00" level=info msg="CreateContainer within sandbox \"f8c6824d06b7162939e77c4d3c1000628b0e1ef58562e1e58cddac1538af117c\" for &ContainerMetadata{Name:worker,Attempt:4,} returns container id \"438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220\""
Apr 02 11:16:27 node-9.domain.tld containerd[28543]: time="2024-04-02T11:16:27.875116156+08:00" level=info msg="StartContainer for \"438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220\""
Apr 02 11:16:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:16:28.075909020+08:00" level=info msg="StartContainer for \"438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220\" returns successfully"
Apr 02 11:16:29 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:16:29 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 27.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:16:29 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:16:29 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:30 node-9.domain.tld coaster-startup-checker[33296]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]: Start to check OSD by-path after node start up.
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]: Node-9 is ready in EOS.
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]: Traceback (most recent call last):
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]:     sys.exit(main())
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:31 node-9.domain.tld coaster-startup-checker[33296]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:31 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:16:31 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:16:31 node-9.domain.tld kubelet[4781]: I0402 11:16:31.815554    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:16:31 node-9.domain.tld kubelet[4781]: E0402 11:16:31.816182    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:16:35 node-9.domain.tld kubelet[4781]: I0402 11:16:35.815447    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:16:35 node-9.domain.tld kubelet[4781]: E0402 11:16:35.816524    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:16:41 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:16:41 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 28.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:16:41 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:16:41 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:42 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:43 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:43 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:43 node-9.domain.tld coaster-startup-checker[33747]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]: Start to check OSD by-path after node start up.
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]: Node-9 is ready in EOS.
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]: Traceback (most recent call last):
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]:     sys.exit(main())
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:44 node-9.domain.tld coaster-startup-checker[33747]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:44 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:16:44 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:16:44 node-9.domain.tld kubelet[4781]: I0402 11:16:44.815335    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:16:44 node-9.domain.tld kubelet[4781]: E0402 11:16:44.815919    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:16:48 node-9.domain.tld kubelet[4781]: I0402 11:16:48.816059    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:16:48 node-9.domain.tld kubelet[4781]: E0402 11:16:48.817357    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:16:52.012 4602 ERROR coaster_agent.agent
Apr 02 11:16:54 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:16:54 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 29.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:16:54 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:16:54 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:16:55 node-9.domain.tld coaster-startup-checker[34666]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:16:55 node-9.domain.tld kubelet[4781]: I0402 11:16:55.815289    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:16:55 node-9.domain.tld kubelet[4781]: E0402 11:16:55.815916    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]: Start to check OSD by-path after node start up.
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]: Node-9 is ready in EOS.
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]: Traceback (most recent call last):
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]:     sys.exit(main())
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:16:56 node-9.domain.tld coaster-startup-checker[34666]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:16:56 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:16:56 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:17:03 node-9.domain.tld kubelet[4781]: I0402 11:17:03.815384    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:17:03 node-9.domain.tld kubelet[4781]: E0402 11:17:03.816476    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:17:06 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:17:06 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 30.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:17:06 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:17:06 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:17:07 node-9.domain.tld kubelet[4781]: I0402 11:17:07.815451    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:17:07 node-9.domain.tld kubelet[4781]: E0402 11:17:07.816043    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:17:07 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:07 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:07 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:07 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:08 node-9.domain.tld coaster-startup-checker[35077]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]: Start to check OSD by-path after node start up.
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]: Node-9 is ready in EOS.
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]: Traceback (most recent call last):
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]:     sys.exit(main())
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:09 node-9.domain.tld coaster-startup-checker[35077]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:09 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:17:09 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:17:15 node-9.domain.tld kubelet[4781]: I0402 11:17:15.815513    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:17:15 node-9.domain.tld kubelet[4781]: E0402 11:17:15.816643    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:17:19 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:17:19 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 31.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:17:19 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:17:19 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:20 node-9.domain.tld coaster-startup-checker[35499]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]: Start to check OSD by-path after node start up.
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]: Node-9 is ready in EOS.
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]: Traceback (most recent call last):
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]:     sys.exit(main())
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:21 node-9.domain.tld coaster-startup-checker[35499]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:21 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:17:21 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:22 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:22.068 4602 ERROR coaster_agent.agent
Apr 02 11:17:22 node-9.domain.tld kubelet[4781]: I0402 11:17:22.815404    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:17:22 node-9.domain.tld kubelet[4781]: E0402 11:17:22.816056    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:17:27 node-9.domain.tld kubelet[4781]: I0402 11:17:27.815465    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:17:27 node-9.domain.tld kubelet[4781]: E0402 11:17:27.816522    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:17:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:17:28.189153233+08:00" level=info msg="shim disconnected" id=438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220
Apr 02 11:17:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:17:28.189271624+08:00" level=warning msg="cleaning up after shim disconnected" id=438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220 namespace=k8s.io
Apr 02 11:17:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:17:28.189297349+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:17:28 node-9.domain.tld systemd[32192]: run-containerd-io.containerd.runtime.v2.task-k8s.io-438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:17:28 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:17:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:17:28.214766588+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:17:28+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=36146\n"
Apr 02 11:17:28 node-9.domain.tld kubelet[4781]: I0402 11:17:28.607885    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49
Apr 02 11:17:28 node-9.domain.tld kubelet[4781]: I0402 11:17:28.608528    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220
Apr 02 11:17:28 node-9.domain.tld kubelet[4781]: E0402 11:17:28.609349    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 1m20s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:17:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:17:28.609489594+08:00" level=info msg="RemoveContainer for \"70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49\""
Apr 02 11:17:28 node-9.domain.tld containerd[28543]: time="2024-04-02T11:17:28.618762584+08:00" level=info msg="RemoveContainer for \"70a5ad06d843256e618d187e4981ca01dfc49186fd5c18e18024b7f91630bd49\" returns successfully"
Apr 02 11:17:31 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:17:31 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 32.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:17:31 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:17:31 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:33 node-9.domain.tld coaster-startup-checker[36339]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]: Start to check OSD by-path after node start up.
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]: Node-9 is ready in EOS.
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]: Traceback (most recent call last):
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]:     sys.exit(main())
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:34 node-9.domain.tld coaster-startup-checker[36339]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:34 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:17:34 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:17:36 node-9.domain.tld kubelet[4781]: I0402 11:17:36.815325    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:17:36 node-9.domain.tld kubelet[4781]: E0402 11:17:36.815931    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:17:38 node-9.domain.tld kubelet[4781]: I0402 11:17:38.815415    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:17:38 node-9.domain.tld kubelet[4781]: E0402 11:17:38.816484    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:17:40 node-9.domain.tld kubelet[4781]: I0402 11:17:40.815316    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220
Apr 02 11:17:40 node-9.domain.tld kubelet[4781]: E0402 11:17:40.816202    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 1m20s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:17:44 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:17:44 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 33.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:17:44 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:17:44 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:45 node-9.domain.tld coaster-startup-checker[36794]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]: Start to check OSD by-path after node start up.
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]: Node-9 is ready in EOS.
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]: Traceback (most recent call last):
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]:     sys.exit(main())
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:46 node-9.domain.tld coaster-startup-checker[36794]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:46 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:17:46 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:17:51 node-9.domain.tld kubelet[4781]: I0402 11:17:51.815711    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:17:51 node-9.domain.tld kubelet[4781]: E0402 11:17:51.816360    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent [-] Collect node info failed with exception: 'NoneType' object has no attribute 'split'
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent Traceback (most recent call last):
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 130, in collect_data
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/agent.py", line 161, in _collect_data
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:52 node-9.domain.tld coaster-agent[4602]: 2024-04-02 11:17:52.011 4602 ERROR coaster_agent.agent
Apr 02 11:17:53 node-9.domain.tld kubelet[4781]: I0402 11:17:53.815610    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220
Apr 02 11:17:53 node-9.domain.tld kubelet[4781]: I0402 11:17:53.815859    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:17:53 node-9.domain.tld kubelet[4781]: E0402 11:17:53.816466    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 1m20s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:17:53 node-9.domain.tld kubelet[4781]: E0402 11:17:53.816946    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 2m40s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:17:56 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:17:56 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 34.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:17:56 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:17:56 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:17:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:57 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:17:58 node-9.domain.tld coaster-startup-checker[37673]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]: Start to check OSD by-path after node start up.
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]: Node-9 is ready in EOS.
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]: Traceback (most recent call last):
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]:     sys.exit(main())
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:17:59 node-9.domain.tld coaster-startup-checker[37673]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:17:59 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:17:59 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
Apr 02 11:18:05 node-9.domain.tld kubelet[4781]: I0402 11:18:05.815431    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:18:05 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:05.819943330+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for container &ContainerMetadata{Name:qgpu-manager,Attempt:6,}"
Apr 02 11:18:05 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:05.881107248+08:00" level=info msg="CreateContainer within sandbox \"f9e4e4aa2123f0ee135978cfe693ceffc4f5584a103f52477a5f4d85486703ad\" for &ContainerMetadata{Name:qgpu-manager,Attempt:6,} returns container id \"efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867\""
Apr 02 11:18:05 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:05.881653548+08:00" level=info msg="StartContainer for \"efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867\""
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.159355362+08:00" level=info msg="StartContainer for \"efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867\" returns successfully"
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.343211824+08:00" level=info msg="shim disconnected" id=efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.343305292+08:00" level=warning msg="cleaning up after shim disconnected" id=efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867 namespace=k8s.io
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.343330121+08:00" level=info msg="cleaning up dead shim"
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.368873199+08:00" level=warning msg="cleanup warnings time=\"2024-04-02T11:18:06+08:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=38055\n"
Apr 02 11:18:06 node-9.domain.tld kubelet[4781]: I0402 11:18:06.743906    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942
Apr 02 11:18:06 node-9.domain.tld kubelet[4781]: I0402 11:18:06.744520    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.745155836+08:00" level=info msg="RemoveContainer for \"065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942\""
Apr 02 11:18:06 node-9.domain.tld kubelet[4781]: E0402 11:18:06.745568    4781 pod_workers.go:191] Error syncing pod d56ec515-f683-49ae-af58-15c5ec1b769b ("qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"), skipping: failed to "StartContainer" for "qgpu-manager" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-manager pod=qgpu-manager-slc77_imp-qgpu(d56ec515-f683-49ae-af58-15c5ec1b769b)"
Apr 02 11:18:06 node-9.domain.tld containerd[28543]: time="2024-04-02T11:18:06.754795245+08:00" level=info msg="RemoveContainer for \"065d713c46fa2ac2ceb8e2ea9145b96599cb3284b152833a3780f49850d83942\" returns successfully"
Apr 02 11:18:06 node-9.domain.tld kubelet[4781]: I0402 11:18:06.815173    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: fc21293b98ec8af7a972db2aabc5028c63e94deac500e32e4e70137d99187c8b
Apr 02 11:18:06 node-9.domain.tld kubelet[4781]: E0402 11:18:06.815692    4781 pod_workers.go:191] Error syncing pod 7081080d-560e-4046-91d7-1d5f2b1f8230 ("qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"), skipping: failed to "StartContainer" for "qgpu-exporter" with CrashLoopBackOff: "back-off 5m0s restarting failed container=qgpu-exporter pod=qgpu-exporter-2dc89_imp-qgpu(7081080d-560e-4046-91d7-1d5f2b1f8230)"
Apr 02 11:18:06 node-9.domain.tld systemd[32192]: run-containerd-io.containerd.runtime.v2.task-k8s.io-efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit UNIT has successfully entered the 'dead' state.
Apr 02 11:18:06 node-9.domain.tld systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867-rootfs.mount: Succeeded.
-- Subject: Unit succeeded
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit run-containerd-io.containerd.runtime.v2.task-k8s.io-efbb720e3f82612ea602c5dd67215e195d0fc59cc55e715a8ee790400d816867-rootfs.mount has successfully entered the 'dead' state.
Apr 02 11:18:08 node-9.domain.tld kubelet[4781]: I0402 11:18:08.815579    4781 scope.go:111] [topologymanager] RemoveContainer - Container ID: 438e6b33105faf7a9d64d649a0819879ef608b59735101706d730117724e5220
Apr 02 11:18:08 node-9.domain.tld kubelet[4781]: E0402 11:18:08.816389    4781 pod_workers.go:191] Error syncing pod 3f04228b-0006-4b96-acca-351a4d101cfa ("qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"), skipping: failed to "StartContainer" for "worker" with CrashLoopBackOff: "back-off 1m20s restarting failed container=worker pod=qgpu-operator-idc-1710754389-node-feature-discovery-workerp88pb_kube-system(3f04228b-0006-4b96-acca-351a4d101cfa)"
Apr 02 11:18:09 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Service RestartSec=10s expired, scheduling restart.
Apr 02 11:18:09 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Scheduled restart job, restart counter is at 35.
-- Subject: Automatic restarting of a unit has been scheduled
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Automatic restarting of the unit coaster-startup-checker.service has been scheduled, as the result for
-- the configured Restart= setting for the unit.
Apr 02 11:18:09 node-9.domain.tld systemd[1]: Stopped Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished shutting down
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished shutting down.
Apr 02 11:18:09 node-9.domain.tld systemd[1]: Started Roller node start up check service.
-- Subject: Unit coaster-startup-checker.service has finished start-up
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit coaster-startup-checker.service has finished starting up.
-- 
-- The start-up result is done.
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x310f0400): originator(PL), code(0x0f), sub_code(0x0400)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm0: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld kernel: mpt3sas_cm1: log_info(0x30030109): originator(IOP), code(0x03), sub_code(0x0109)
Apr 02 11:18:10 node-9.domain.tld coaster-startup-checker[38117]: No handlers could be found for logger "coaster_agent.raidtools"
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]: Start to check OSD by-path after node start up.
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]: Node-9 is ready in EOS.
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]: Traceback (most recent call last):
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]:   File "/usr/bin/coaster-startup-checker", line 8, in <module>
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]:     sys.exit(main())
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/cmd/startup_checker.py", line 116, in main
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 407, in disks
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]:   File "/builddir/venv/lib/python2.7/site-packages/coaster_agent/disk.py", line 371, in _collect_disks
Apr 02 11:18:11 node-9.domain.tld coaster-startup-checker[38117]: AttributeError: 'NoneType' object has no attribute 'split'
Apr 02 11:18:11 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Main process exited, code=exited, status=1/FAILURE
Apr 02 11:18:11 node-9.domain.tld systemd[1]: coaster-startup-checker.service: Failed with result 'exit-code'.
-- Subject: Unit failed
-- Defined-By: systemd
-- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- The unit coaster-startup-checker.service has entered the 'failed' state with result 'exit-code'.
